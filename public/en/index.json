


[{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"Ai","type":"tags"},{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/categories/code/","section":"Categories","summary":"","title":"Code","type":"categories"},{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"Llm","type":"tags"},{"content":"简介 # NN # 神经网络\n可以拟合任意函数 拟合过程黑盒 前后输入完全无关 RNN # 循环神经网络\n一般处理序列信息，前后输入相关 $$O_t= g(V \\cdot S_t) $$ $$S_t = f(U\\cdot X_t + W\\cdot S_{t-1})$$ Transformer网络 # 用于序列建模的深度神经网络结构 采用全局注意力机制 更好捕捉序列中不同位置之间的关系 并行计算优势明显 输入 # 单词Embedding + 位置Embedding (Positional Encoding) 相加得到 Embedding # Embedding 是一种映射，将现实中的 文字、图片等信息转化为计算机能识别的语言\n举例 地图类比 现实地理的embedding\n对比one-hot编码 one-hot编码：过于稀疏，过度占用资源\nEmbedding 的作用\n降维 利用矩阵乘法进行降维 升维 对低维的数据进行升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开了 语义中通过计算获得单词之间的关系，可以推算出更多单词之间的关系\n单词Embedding # 获取方式：Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到 位置Embedding # Transformer 使用全局信息，不能利用单词的顺序信息 Transformer 中使用位置Embedding 保存单词在序列中的相对或绝对位置 位置Embedding 计算公式\n$$PE_{(pos, 2i)} = \\sin (pos/10000^{2i/d})$$ $$PE_{(pos, 2i+1)} = \\cos (pos/10000^{2i/d})$$PE: 位置Embedding\nd: PE 维度\n2i:表示偶数维度\n2i+1 表示奇数维度\npos 单词在句子中的位置\n通过正余弦公式， $$\\sin (A+B) = \\sin A \\cos B + \\cos A\\sin B$$ $$\\cos (A+B) = \\cos A \\cos B - \\sin A\\sin B$$$$PE_{pos}$$可以很快算出 $$PE_{pos+k}$$ 将单词的词 Embedding 和位置 Embedding 相加， 就是 Transformer 的输入 Attention 注意力机制 # 下意识的动作 被称为 不随意线索(不随着意识的线索) 自己主观的动作是随意线索\n注意力机制则显式地考虑随意线索 随意线索称之为查询(query) 类比现实世界中我想要做的动作 每个输入是一个值(value) 和不随意线索(key) 对， 类比现实世界中的环境 通过注意力池化层来有偏向性地选择某些输入 我们可以用在图书馆找书的场景来类比注意力机制中的 query、key 和 value。\nQuery（查询）：\n假设你现在有一个特定的研究主题，比如 “人工智能在医疗领域的应用”。这个研究主题就是 query，代表着你的特定需求和关注点。你带着这个问题来到图书馆，希望找到与之相关的书籍和资料。\nKey（键）：\n图书馆里的每一本书都可以看作一个信息单元。每本书的书名、目录、关键词等就像是 key。这些 key 可以帮助你快速判断这本书是否与你的查询需求相关。例如，一本书的书名是《人工智能医疗创新》，这个书名就是一个 key，它与你的 query 有较高的相关性。\nValue（值）：\n书的具体内容就是 value。当你通过书名等 key 判断一本书可能与你的 query 相关后，你会进一步查看这本书的具体内容，也就是 value。如果这本书的内容详细介绍了人工智能在医疗领域的各种应用案例、技术原理等，那么这些内容就是你所需要的 value。\n在这个场景中，你首先根据 query（研究主题）去扫描图书馆里的各种 key（书名、关键词等），找到可能相关的书籍。然后，通过查看这些书籍的 value（具体内容），来获取与你的查询需求最相关的信息。这就类似于注意力机制中，通过 query 与 key 的匹配，提取出相应的 value，从而聚焦在最相关的信息上。\n","date":"20 August 2024","externalUrl":null,"permalink":"/posts/code/llm-intro/","section":"Posts","summary":"","title":"llm intro","type":"posts"},{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"7 May 2024","externalUrl":null,"permalink":"/tags/flink/","section":"Tags","summary":"","title":"Flink","type":"tags"},{"content":" Flink 只对时间类型的窗口清除，其他类型的窗口不清除 每一个window 都有一个 trigger 和一个function(ProcessWindowFunction, ReduceFunction 或者 AggregateFunction) 组成方式 # Window Assigner 指派元素如何进入 window Trigger Evictor 从window中 剔除某些元素 Window Assigner # 当我们使用window时，比如countWindow，\ntext.flatMap(new WordCount.Tokenizer()) .name(\u0026#34;tokenizer\u0026#34;) .keyBy(value -\u0026gt; value.f0) .countWindow(windowSize, slideSize) .sum(1) .name(\u0026#34;counter\u0026#34;); 实际上就调用了 window， evictor 和 trigger\n其中，调用window方法，就是根据 WindowAssigner 创建一个window\npackage org.apache.flink.streaming.api.datastream; @Public public class KeyedStream\u0026lt;T, KEY\u0026gt; extends DataStream\u0026lt;T\u0026gt; { public WindowedStream\u0026lt;T, KEY, GlobalWindow\u0026gt; countWindow(long size, long slide) { return window(GlobalWindows.create()) .evictor(CountEvictor.of(size)) .trigger(CountTrigger.of(slide)); } @PublicEvolving public \u0026lt;W extends Window\u0026gt; WindowedStream\u0026lt;T, KEY, W\u0026gt; window( WindowAssigner\u0026lt;? super T, W\u0026gt; assigner) { return new WindowedStream\u0026lt;\u0026gt;(this, assigner); } ... ... } ","date":"7 May 2024","externalUrl":null,"permalink":"/posts/code/flink-window/","section":"Posts","summary":"","title":"Flink Window","type":"posts"},{"content":"","date":"9 August 2023","externalUrl":null,"permalink":"/tags/ros/","section":"Tags","summary":"","title":"Ros","type":"tags"},{"content":"Ros2 命令行接口 # 所有 ROS2 命令行 开始都要添加前缀 ros2，后面跟一条指令，一个动词，若干参数\n文档参数\nros2 指令 --help # 或者 ros2 指令 -h 指令 # action # 允许手动发送目标并显示关于action的debug信息\n动词 # info 输出关于action的信息 ros2 action info /fibonacci list 输出 action的名称列表 ros2 action list send_goal 发送action目标 ros2 action send_goal /fibonacci action_tutotials/action/Fibonacci \u0026#34;order\u0026#34; show 输出action的定义 ros2 action show action_tutorials/action/Fibonacci bag # 允许从rosbag 里 播放 topic 或者 录制 topic 到 rosbag\n动词 # info 输出bag的信息 ros2 info \u0026lt;bag-name\u0026gt; play 播放bag ros2 play \u0026lt;bag-name\u0026gt; record 录制bag ros2 record -a component # 各种组件动词\n动词 # list 输出正在运行的容器列表和组件 ros2 component list load 载入组件到 容器节点 ros2 component load /ComponentManager composition composition::Talker standalone 运行组件到它所属的独立容器节点中\ntypes 输出在ament索引中注册组件的列表\nros2 component types unload 从容器节点卸载组件 ros2 component unload /ComponentManager 1 daemon # 守护进程的动词\n动词 # start 如果守护进程没有运行就开启 status 输出守护进程的状态 stop 如果守护进程运行就停止 doctor # 检查ROS 设置和其他潜在问题(如网络，包版本，rmw中间件等)的工具\nros2 doctor 同义词 # wtf\nros2 wtf 参数 # \u0026ndash;report/-r 输出所有检查的报告 ros2 doctor --report \u0026ndash;report-fail/-rf 只输出失败检查的报告 ros2 doctor --report-fail \u0026ndash;include-warning/-iw 包含失败的检查报告 rosw doctor --include-warning ros2 doctor --include-warning --report-fail extension_points # 列出 扩展 points\nextensions # 列出 扩展\ninterface # 不同ROS接口(action/topic/service)相关动词。接口类型可以通过以下选项过滤掉: \u0026ndash;only-actions, \u0026ndash;only-msgs, \u0026ndash;only-srvs\n动词 # list 列举所有可用的接口类型 ros2 interface list package 输出包内可用接口类型的列表 ros2 interface package std msgs packages 输出提供接口的包的列表 ros2 interface packages --only-msgs proto 打印接口的原型(主体) ros2 interface proto example interfaces/srv/AddTwoInts show 输出接口定义 ros2 interface show geometry msgs/msg/Pose launch # 允许在任意一个包里运行一个启动文件，不用 cd 到那个包里\nros2 launch \u0026lt;package\u0026gt; \u0026lt;launch.file\u0026gt; ros launch demo.nodes.cpp add_two_ints_launch.py lifecycle # 生命周期相关动词\n动词 # get 获取一个和多个节点的生命周期状态 list 输出可用的转换的李彪 nodes 输出具有生命周期的节点列表 set 触发生命周期状态转换 msg(弃用) # 展示有关消息的调试信息\n动词 # list 输出消息类型列表 ros2 msg list package 输出给定包的消息列表 ros2 msg package std_msgs packages 输出包含该消息的包 ros2 msg packages show 输出消息定义 ros2 msg show geometry_msgs/msg/Pose multicast # 多播相关的动词\n动词 # receive 接收单个UDP多播包 send 发送单个UDP多播包 node # 动词 # info 输出节点信息 ros2 node info /talker list 输出可用节点列表 ros2 node list param # 允许操作参数\n动词 # delete 删除参数 ros2 param delete /talker /user_sim_time describe 展示已声明参数的描述性信息\ndump 将给定节点的参数以yaml格式转储到终端或文件中\nget 获取参数\nros2 param get /talker /user_sim_time list 输出可用参数的列表 ros2 param list set 设置参数 ros2 param set /talker /user_sim_time false pkg # 创建ros2包或者输出 包相关的信息\n动词 # create 创建新的ros2包 executables 输出指定包的可执行文件列表 ros2 pkg executables demo_nodes_cpp list 输出可用包的列表 ros2 pkg list prefix 输出包的前缀路径 ros2 pkg prefix std_msgs xml 输出包xml清单列表里的信息 ros2 pkg xml -t version run # 在任意包中允许运行可执行文件，而不用cd\nros2 run \u0026lt;package\u0026gt; \u0026lt;executable\u0026gt; ros2 run demo_node_cpp talker security # 安全相关动词\n动词 # create_key 创建key ros2 security create_key demo keys /talker create_permission 创建keystore ros2 security create_permission demo keys /talker policies/sample policy.xml generate_artifacts 创建权限 ros2 security_generate artifacts list_keys 分配key create_keystore 从身份和策略文件中生成key和权限文件 ros2 security create keystore demo keys distribute_key 从ros 图数据生成 XML策略文件 generate_policy 列出key service # 允许手动调用服务，并显示有关服务的调试信息\n动词 # call 调用服务 ros2 service call /add two ints example interfaces/AddTwoInts ”a: 1, b: 2” find 输出 给定类型的服务列表 ros2 service find rcl interfaces/srv/ListParameters list 输出 服务名称列表 ros2 service list type 输出服务类型 ros2 service type /talker/describe parameters srv(弃用) # 服务相关动词\n动词 # list 输出可用服务类型 package 输出包中的可用服务类型 packages 输出包含服务的包 show 输出服务定义 test # 运行ros2启动测试\ntopic # 用于显示有关ROS主题的调试信息的工具，包括发布者、订阅者、发布速率和消息。\n动词 # bw 展示 topic的带宽 ros2 topic bw /chatter delay 从header的时间戳展示topic的延迟 echo 输出给定topic的消息到屏幕 ros2 topic echo /chatter find 查找给定类型的topic类型 ros2 topic find rcl interfaces/msg/Log hz 展示topic的发布率 ros2 topic hz /chatter info 输出给定topic的信息 ros2 topic info /chatter list 输出 活动的topic列表 ros2 topic list pub 发布数据到topic ros2 topic pub /chatter std msgs/msg/String ’data: Hello ROS 2 world’ type 输出topic的类型 ros2 topic type /rosout ","date":"9 August 2023","externalUrl":null,"permalink":"/posts/code/ros2-cli-%E6%B8%85%E5%8D%95/","section":"Posts","summary":"","title":"ROS2 cli 清单","type":"posts"},{"content":"SQL验证完成后，需要将SQL语法树转换成Operation的集合\npublic List\u0026lt;Operation\u0026gt; parse(String statement) { ... return Collections.singletonList( SqlNodeToOperationConversion.convert(planner, catalogManager, parsed.get(0)) .orElseThrow(() -\u0026gt; new TableException(\u0026#34;Unsupported query: \u0026#34; + statement))); } 在这里将已经验证好的SQL 转换成Oper的过程，就会使用到SqlNodeToOperationConversion.convert\nSqlNodeToOperationConversion.convert # public static Optional\u0026lt;Operation\u0026gt; convert( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode sqlNode) { // validate the query final SqlNode validated = flinkPlanner.validate(sqlNode); return convertValidatedSqlNode(flinkPlanner, catalogManager, validated); } convertValidatedSqlNode 就是转换Operation的入口，一共有三个参数\nflinkPlanner: FlinkPlannerImpl 优化器，将SQL语法树生成flink的执行计划 catalogManager: CatalogManager 当前作业的库表等信息的管理 validated: SqlNode 已经检验好的SQL语法树 convert 参数 # FlinkPlannerImpl # 在实际运行中, flinkPlanner 是通过 FlinkPlannerImpl 实例实现的，具体是在 PlannerContext进行实现的\npackage org.apache.flink.table.planner.delegation; public class PlannerContext { ... public FlinkPlannerImpl createFlinkPlanner() { return new FlinkPlannerImpl( createFrameworkConfig(), this::createCatalogReader, typeFactory, cluster); } ... } 一套flink 使用的schema，类型系统以及SQL优化器等组件的framework配置 一个读取flink的catalog的入口实例 生成flink类型的工厂实例 专门为Flink服务的 calcite 关系优化cluster planner 的作用就是用于将 FlinkSQL 转换成Flink 可以识别的DAG作业，运行到Flink 环境中\nCatalogManager # CatalogManager 隐藏的很深，在最初创建的时候他就已经生成了。如果没有特殊要求，catalog 回创建一个名为 \u0026ldquo;default_catalog\u0026rdquo; 的catalog，在此之下，还会创建一个 名为 “default_database”的数据库\n通常情况下，这里的catalog是不需要我们修改的，在flink UI 界面里我们也经常可以看到，所有的表都是放在这个“default_catlog.default_database\u0026quot;下的 流式任务的创建的代码是在 org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl 下的\npackage org.apache.flink.table.api.bridge.scala.internal ... object StreamTableEnvironmentImpl { ... val catalogManager = CatalogManager.newBuilder .classLoader(userClassLoader) .config(tableConfig) .defaultCatalog( settings.getBuiltInCatalogName, new GenericInMemoryCatalog(settings.getBuiltInCatalogName, settings.getBuiltInDatabaseName)) .executionConfig(executionEnvironment.getConfig) .catalogModificationListeners(TableFactoryUtil .findCatalogModificationListenerList(tableConfig.getConfiguration, userClassLoader)) .catalogStoreHolder( CatalogStoreHolder .newBuilder() .catalogStore(catalogStore) .factory(catalogStoreFactory) .config(tableConfig) .classloader(userClassLoader) .build()) .build ... } 这里的所有参数都是默认值\nSQL验证 # 写入的SQL语句 会先进行验证，通过Flink自己写的Planner，设置自己的规则然后借助Calicte 进行验证\n具体验证过程: FlinkSQL - 验证过程\nconvertValidatedSqlNode # 从这里开始SqlNode进入后就要转换成Operation了\npackage org.apache.flink.table.planner.operations; public class SqlNodeToOperationConversion { ... private static Optional\u0026lt;Operation\u0026gt; convertValidatedSqlNode( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode validated) { ... } ... } 该方法接收3个参数：\nflinkPlanner: 就是从上层传进来的执行计划 catalogManager: 用来管理数据的schema validated: 验证后的Sql语句 private static Optional\u0026lt;Operation\u0026gt; convertValidatedSqlNode( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode validated) { beforeConversion(); // delegate conversion to the registered converters first SqlNodeConvertContext context = new SqlNodeConvertContext(flinkPlanner, catalogManager); Optional\u0026lt;Operation\u0026gt; operation = SqlNodeConverters.convertSqlNode(validated, context); if (operation.isPresent()) { return operation; } ... } 首先做一些前置工作\nbeforeConversion： 清除行级修改上下文，在普通的SQL作业中这一步没什么变化 如果SQL之前注册过，那么就返回之前的SQL，这里因为第一次启动，之前也没有任务，所以也不会直接返回 Q\u0026amp;A # 为什么 parse 方法返回的是 Operation的集合呢？ ","date":"15 July 2023","externalUrl":null,"permalink":"/posts/code/flinksql---operation/","section":"Posts","summary":"","title":"FlinkSQL - Operation","type":"posts"},{"content":"简介 # Paimon 是一个流数据湖平台 数据入湖 # 实时集成 数据库整库同步 schema变更同步（表结构变更） 部分列更新 批量覆盖 支持 # 增量快照 产生changelog lookup join batch/OLAP 查询 核心特性 # 统一批处理和流处理 批量写入和读取，流式更新，变更日志生成 数据湖能力 低成本、高可靠性、可扩展的元数据 各种合并引擎 按照要求更新记录 变更日志生成 多种表类型 schema 变更 重命名列 一致性保证 # 使用两阶段提交 每次提交在提交时最多生成2个快照 桶是读写的最小单元 两个writer如果修改的不是同一个桶，则提交是有序的 如果两个writer修改同一个桶，保证快照隔离，不会丢失更改信息，可能会合并两次提交，混合起来 文件布局 # 快照文件(Snapshot Files) # 快照目录存储了所有的快照文件 快照文件是JSON文件 一个快照文件包含 若干使用的schema文件 快照所有更改的manifest lists 清单文件(Manifest Files) # 清单目录存储了清单列表(manifest lists)和清单文件(manifest files) 清单列表保存的是清单文件的文件名 清单文件包含 LSM数据文件和 changelog 文件的变化(例如快照中LSM数据的创建或者是文件的删除) 数据文件(Data Files) # 数据文件会进行分区和分桶 一个分区下面会有多个分桶(目录) 一个分桶里包含LSM数据 以及 changelog 默认使用orc文件，支持parquet,avro LSM Trees # paimon 的数据是由LSM Tree 进行组织的 Sorted Runs # LSM Tree 将文件组织成若干个 Sorted Runs 一个 sorted run 包含一个或多个数据文件，每个数据文件只属于一个sorted run 数据文件中的记录按照主键排序 同一个 sorted run 中，数据文件的主键范围不会重叠 不同的 sorted run 中，数据文件的主键范围是有可能重叠的，因此在查询LSM Tree 的时候，需要合并所有的 sorted runs 中相同主键的记录，按照用户指定的合并引擎 和时间戳 LSM Tree 的新纪录会先写入内存，内存缓冲区满了之后会刷新到磁盘，创建新的sorted run Compaction # 越来越多的记录写入LSM Tree 后，sorted run 就会增多 查询LSM Tree时需要将所有的 sorted run 都合并起来，过多的sorted run 就会导致查询性能下降，或者OOM 将多个sorted run 合并成一个大的 sorted run 的这个过程被称之为 Compaction 过于频繁的 Compaction也会消耗一定的cpu和磁盘io， paimon 使用与rocksdb类似的通用压缩 Compaction策略 可以指定 专用compaction作业(dedicated compaction job) 进行 compaction catalog 文件系统 # paimon catalog 可以持久化元数据 支持两种metastore\n文件系统，file HDFS 等 Hive,使用 hive metastore 存放元数据 CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/table/store/warehouse\u0026#39; ); 上手 # 目前 paimon 提供了两个的jar包\nboudled jar 用于 读写数据，也就是启动flink任务，with 参数等 action jar 用于 各种操作，如手动compaction等， 这个可以在任务之外进行操作 -- if you\u0026#39;re trying out Paimon in a distributed environment, -- the warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;file:/tmp/paimon\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); -- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.word.length\u0026#39; = \u0026#39;1\u0026#39; ); -- paimon requires checkpoint interval in streaming mode SET \u0026#39;execution.checkpointing.interval\u0026#39; = \u0026#39;10 s\u0026#39;; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; 使用paimon后，该目录如下\ndefault.db # default.db 为默认的数据库，就是catalog下的数据库目录\nword_count # word_cound 是在数据库下创建的表目录\nbucket-0 # bucket-x 就是 真正存储数据的地方，数据多了之后还会有 bucket-1 bucket-2 等 是一系列orc 后缀的文件\nschema # schema 也是json 文件，而且会有多个,主要也是为了保存变更的schema\n{ \u0026#34;id\u0026#34; : 0, \u0026#34;fields\u0026#34; : [ { \u0026#34;id\u0026#34; : 0, \u0026#34;name\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;STRING NOT NULL\u0026#34; }, { \u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;cnt\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT\u0026#34; } ], \u0026#34;highestFieldId\u0026#34; : 1, \u0026#34;partitionKeys\u0026#34; : [ ], \u0026#34;primaryKeys\u0026#34; : [ \u0026#34;word\u0026#34; ], \u0026#34;options\u0026#34; : { } } snapshot # 快照目录保存了很多的文件id，通过这些id 可以更快找到 清单文件和schema\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 15, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-f8c8e502-67ed-4b4f-9d57-bd1ff93943e6-28\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-f8c8e502-67ed-4b4f-9d57-bd1ff93943e6-29\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;a196f82d-68d9-416b-a941-3b478cb0ff9b\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 13, \u0026#34;commitKind\u0026#34; : \u0026#34;APPEND\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1692273479392, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 400, \u0026#34;deltaRecordCount\u0026#34; : 16, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } manifest # 清单目录里有很多清单文件，文件不能完全打开，但是可以看到内部是有数据文件的id等信息，应该是数据变更的信息内容\n数据类型 # 支持了基本上所有的Flink 的数据类型，除了 multiset map类型不支持主键 ","date":"15 July 2023","externalUrl":null,"permalink":"/posts/code/paimon/","section":"Posts","summary":"","title":"Paimon","type":"posts"},{"content":"","date":"15 July 2023","externalUrl":null,"permalink":"/tags/paimon/","section":"Tags","summary":"","title":"Paimon","type":"tags"},{"content":"","date":"15 July 2023","externalUrl":null,"permalink":"/tags/sql/","section":"Tags","summary":"","title":"Sql","type":"tags"},{"content":"","date":"15 June 2023","externalUrl":null,"permalink":"/tags/calcite/","section":"Tags","summary":"","title":"Calcite","type":"tags"},{"content":"当 SqlValidator解析完作用域信息之后，紧接着就开始对类型进行验证\n@Override public SqlNode validate(SqlNode topNode) { SqlValidatorScope scope = new EmptyScope(this); scope = new CatalogScope(scope, ImmutableList.of(\u0026#34;CATALOG\u0026#34;)); final SqlNode topNode2 = validateScopedExpression(topNode, scope); final RelDataType type = getValidatedNodeType(topNode2); Util.discard(type); return topNode2; } getValidatedNodeType # 每个SqlNode 都会设定一个类型比如，\nSELECT * FROM table WHERE column1 + column2 = 10 在解析完成后，“*” 会被解开变成user, product, amoutn， 就会被设定成这样一个类型 RecordType(BIGINT user, VARCHAR(2147483647) product, INTEGER amount)\n@Override public RelDataType getValidatedNodeType(SqlNode node) { RelDataType type = getValidatedNodeTypeIfKnown(node); if (type == null) { if (node.getKind() == SqlKind.IDENTIFIER) { throw newValidationError(node, RESOURCE.unknownIdentifier(node.toString())); } throw Util.needToImplement(node); } else { return type; } } @Override public @Nullable RelDataType getValidatedNodeTypeIfKnown(SqlNode node) { final RelDataType type = nodeToTypeMap.get(node); if (type != null) { return type; } final SqlValidatorNamespace ns = getNamespace(node); if (ns != null) { return ns.getType(); } final SqlNode original = originalExprs.get(node); if (original != null \u0026amp;\u0026amp; original != node) { return getValidatedNodeType(original); } if (node instanceof SqlIdentifier) { return getCatalogReader().getNamedType((SqlIdentifier) node); } return null; } 这里的 nodeToTypeMap 是在validateScopedExpression 这一步就确定了的，整个过程是在 验证命名空间的过程汇总就写入了，当然也是递归的过程 type 类型推断 是通过 SqlValidatorScope 实现这个接口实现的\n每个SqlNode 都对应一个 Scope 在这些Scope中定义规则\n/** Derives the type of a node, never null. */ RelDataType deriveTypeImpl(SqlValidatorScope scope, SqlNode operand) { DeriveTypeVisitor v = new DeriveTypeVisitor(scope); final RelDataType type = operand.accept(v); return requireNonNull(scope.nullifyType(operand, type)); } 于是，在 SqlValidatorImpl中可以找到事如何推导出这些类型是如何生成的\nprivate class DeriveTypeVisitor implements SqlVisitor\u0026lt;RelDataType\u0026gt; { private final SqlValidatorScope scope; DeriveTypeVisitor(SqlValidatorScope scope) { this.scope = scope; } @Override public RelDataType visit(SqlLiteral literal) { return literal.createSqlType(typeFactory); } @Override public RelDataType visit(SqlCall call) { final SqlOperator operator = call.getOperator(); return operator.deriveType(SqlValidatorImpl.this, scope, call); } @Override public RelDataType visit(SqlNodeList nodeList) { // Operand is of a type that we can\u0026#39;t derive a type for. If the // operand is of a peculiar type, such as a SqlNodeList, then you // should override the operator\u0026#39;s validateCall() method so that it // doesn\u0026#39;t try to validate that operand as an expression. throw Util.needToImplement(nodeList); } @Override public RelDataType visit(SqlIdentifier id) { // First check for builtin functions which don\u0026#39;t have parentheses, // like \u0026#34;LOCALTIME\u0026#34;. final SqlCall call = makeNullaryCall(id); if (call != null) { return call.getOperator().validateOperands(SqlValidatorImpl.this, scope, call); } RelDataType type = null; if (!(scope instanceof EmptyScope)) { id = scope.fullyQualify(id).identifier; } // Resolve the longest prefix of id that we can int i; for (i = id.names.size() - 1; i \u0026gt; 0; i--) { // REVIEW jvs 9-June-2005: The name resolution rules used // here are supposed to match SQL:2003 Part 2 Section 6.6 // (identifier chain), but we don\u0026#39;t currently have enough // information to get everything right. In particular, // routine parameters are currently looked up via resolve; // we could do a better job if they were looked up via // resolveColumn. final SqlNameMatcher nameMatcher = catalogReader.nameMatcher(); final SqlValidatorScope.ResolvedImpl resolved = new SqlValidatorScope.ResolvedImpl(); scope.resolve(id.names.subList(0, i), nameMatcher, false, resolved); if (resolved.count() == 1) { // There\u0026#39;s a namespace with the name we seek. final SqlValidatorScope.Resolve resolve = resolved.only(); type = resolve.rowType(); for (SqlValidatorScope.Step p : Util.skip(resolve.path.steps())) { type = type.getFieldList().get(p.i).getType(); } break; } } // Give precedence to namespace found, unless there // are no more identifier components. if (type == null || id.names.size() == 1) { // See if there\u0026#39;s a column with the name we seek in // precisely one of the namespaces in this scope. RelDataType colType = scope.resolveColumn(id.names.get(0), id); if (colType != null) { type = colType; } ++i; } if (type == null) { final SqlIdentifier last = id.getComponent(i - 1, i); throw newValidationError(last, RESOURCE.unknownIdentifier(last.toString())); } // Resolve rest of identifier for (; i \u0026lt; id.names.size(); i++) { String name = id.names.get(i); final RelDataTypeField field; if (name.equals(\u0026#34;\u0026#34;)) { // The wildcard \u0026#34;*\u0026#34; is represented as an empty name. It never // resolves to a field. name = \u0026#34;*\u0026#34;; field = null; } else { final SqlNameMatcher nameMatcher = catalogReader.nameMatcher(); field = nameMatcher.field(type, name); } if (field == null) { throw newValidationError(id.getComponent(i), RESOURCE.unknownField(name)); } type = field.getType(); } type = SqlTypeUtil.addCharsetAndCollation(type, getTypeFactory()); return type; } @Override public RelDataType visit(SqlDataTypeSpec dataType) { // Q. How can a data type have a type? // A. When it appears in an expression. (Say as the 2nd arg to the // CAST operator.) validateDataType(dataType); return dataType.deriveType(SqlValidatorImpl.this); } @Override public RelDataType visit(SqlDynamicParam param) { return unknownType; } @Override public RelDataType visit(SqlIntervalQualifier intervalQualifier) { return typeFactory.createSqlIntervalType(intervalQualifier); } } 至此，SqlNode的验证过程完毕，接下来就是将 SqlNode 转换成 Operation的过程了\n","date":"15 June 2023","externalUrl":null,"permalink":"/posts/code/flinksql---%E7%B1%BB%E5%9E%8B%E9%AA%8C%E8%AF%81/","section":"Posts","summary":"","title":"FlinkSQL - 类型验证","type":"posts"},{"content":"","date":"13 June 2023","externalUrl":null,"permalink":"/tags/i2c/","section":"Tags","summary":"","title":"I2c","type":"tags"},{"content":"","date":"13 June 2023","externalUrl":null,"permalink":"/tags/spi/","section":"Tags","summary":"","title":"Spi","type":"tags"},{"content":"","date":"13 June 2023","externalUrl":null,"permalink":"/tags/uart/","section":"Tags","summary":"","title":"Uart","type":"tags"},{"content":" 协议 线数 通信类型 多主 数据率 总线上期间的数量 线缆长度(米) 工作模式 使用距离 $$UART$$ 2 异步 不支持 3Kbps 到 4Mbps 2 1.5 全双工 远距离 $$SPI$$ 3 同步 不支持 1Mbps \u0026lt; 10 \u0026lt;3 全双工 近距离低速 $$I^2C$$ 2 同步 支持 \u0026lt;3.4Mbps \u0026lt; 10 \u0026lt;3 半双工 近距离低速 举例说明：\nI2C的使用场景：连接传感器、存储器、显示器等设备。例如，连接温度传感器、EEPROM存储器、OLED显示器等。\nUART的使用场景：连接串口设备，如调试器、GPS接收器等。例如，连接串口调试器进行程序调试。\nSPI的使用场景：连接存储器、显示器、传感器等设备。例如，连接Flash存储器、LCD显示器、加速度传感器等。\n选择使用这三者的场景取决于具体的应用需求。\n如果需要连接多个设备，可以选择使用I2C或SPI协议； 如果只需要连接单个设备，可以选择使用UART协议。 如果需要高速传输数据，可以选择使用SPI协议。 如果需要低功耗和简单的通信方式，可以选择使用I2C协议。 如果需要长距离传输数据，可以选择使用UART协议。 UART # UART（Universal Asynchronous Receiver/Transmitter）：UART是一种异步串行通信协议，用于在计算机和外部设备之间传输数据。它使用两根线（TX和RX）进行通信，其中TX是发送线，RX是接收线。UART协议只支持单主机和单从机的通信，不能连接多个设备。UART通常用于连接串口设备，如调试器、GPS接收器等。\n流控制 # 流控制的方式分别有软件和硬件两种。\n软件的流控制方式，在UART通信中，只需RxD、TxD、GND三根即可，数据在传输过程中，依靠代码的判断处理，并通过收发双方进行的数据交互完成控制，在现有通信物理信号线基础上，使用控制字符(ASCII表中的0x00~0x0x1F、0x7F)完成控制指令的交互。一般在私有协议下也会定义一些特殊字符设为控制指令。 硬件的流控制方式，即在原有的RxD、TxD、GND三根信号线的基础上，再增加RTS/CTS和DTR/DSR这两组信号线。第一组线是RTS（Request toSend）和CTS（Clear toSend）。当接收方准备好接收数据，它置高RTS线表示它准备好了，如果发送方也就绪，它置高CTS，表示它即将发送数据。第二组线是DTR（DataTerminal Ready）和DSR（Data SetReady）。这些线主要用于Modem通信。使得串口和Modem通信他们的状态。例如：当Modem已经准备好接收来自PC的数据，它置高DTR线，表示和电话线的连接已经建立。读取DSR线置高，PC机开始发送数据。一个简单的规则是DTR/DSR用于表示系统通信就绪，而RTS/CTS用于单个数据包的传输。 优点 # 只使用两根电线，不需要时钟信号 有一个奇偶校验位，只要双方设置后，就可以改变数据包的结构 缺点 # 数据帧的大小限制为最多9位，不支持多个从属或多个主系统 每个UART的波特率必须在10％之内 SPI # SPI（Serial Peripheral Interface）：SPI是一种串行通信协议，用于在芯片之间传输数据。它使用四根线（MOSI、MISO、SCK和SS）进行通信，其中MOSI是主设备输出从设备输入的数据线，MISO是主设备输入从设备输出的数据线，SCK是时钟线，SS是片选线。SPI协议支持多主机和多从机的通信，可以连接多个设备。SPI通常用于连接存储器、显示器、传感器等设备。\nSPI使用的四根信号线 # SCLK: Serial Clock (output from master)：串行时钟，用来同步数据传输，由主机输出； MOSI \\ SIMO: Master Output, Slave Input(output from master)：主机输出从机输入数据线，通常先传输MSB； MISO \\ SOMI: Master Input, Slave Output(output from slave)：主机输入从机输出数据线，通常先传输LSB； SS: Slave Select (active low, output from master)：片选线，低电平有效，由主机输出。 SS\\CS：控制芯片是否被选中的，也就是说只有片选信号为预先规定的使能信号时（一般默认为低电位），对此芯片的操作才有效，这就允许在同一总线上连接多个SPI设备成为可能。也就是说：当有多个从设备的时候，因为每个从设备上都有一个片选引脚接入到主设备机中，当我们的主设备和某个从设备通信时将需要将从设备对应的片选引脚电平拉低。\nSPI的四种操作模式 # SPI的四种操作模式，它们的区别是定义了在时钟脉冲的哪条边沿转换（toggles）输出信号，哪条边沿采样输入信号，还有时钟脉冲的稳定电平值（就是时钟信号无效时是高还低）。\n对于STM32等MCU自带的硬件SPI外设来说，可能没有那么重要，只需要配置一下模式就行了，但是对于使用使用GPIO模拟或者FPGA来实现SPI的时序，这一点是非常非常重要的。\nMaster 设备会根据将要交换的数据来产生相应的时钟脉冲(Clock Pulse)，时钟脉冲组成了时钟信号(Clock Signal) ，每种模式由时钟信号中的时钟极性（clock polarity）CPOL与时钟周期（clock phase）CPHA来定义。\n不同的从设备可能在出厂是就是配置为某种模式，这是不能改变的，但我们的通信双方必须是工作在同一模式下，所以我们可以对我们的主设备的SPI模式进行配置，从而实现主从通讯。\n时钟极性CPOL是用来配置SCLK的电平出于哪种状态时是空闲态或者有效态；时钟相位CPHA是用来配置数据采样是在第几个边沿。\nCPOL=0，表示当SCLK=0时处于空闲态，所以有效状态就是SCLK处于高电平时； CPOL=1，表示当SCLK=1时处于空闲态，所以有效状态就是SCLK处于低电平时； CPHA=0，表示数据采样是在第1个边沿，数据发送在第2个边沿； CPHA=1，表示数据采样是在第2个边沿，数据发送在第1个边沿。 在高电平有效状态时，第一边沿为上升沿，第二边沿为下降沿；在低电平有效状态时，第一边沿为下降沿，第二边沿为上升沿\n具体四种模式如下：\nCPOL = 0，CPHA = 0：时钟高电平时为有效状态，时钟上升沿（第一个边沿）采样。 CPOL = 0，CPHA = 1：时钟高电平时为有效状态，时钟下降沿（第二个边沿）采样。 CPOL = 1，CPHA = 0：时钟低电平时为有效状态，时钟下降沿（第一个边沿）采样。 CPOL = 1，CPHA = 1：时钟低电平时为有效状态，时钟上升沿（第二个边沿）采样。\nI2C # I2C（Inter-Integrated Circuit）：I2C是一种串行通信协议，用于在芯片之间传输数据。它使用两根线（SDA和SCL）进行通信，其中SDA是数据线，SCL是时钟线。I2C协议支持多主机和多从机的通信，可以连接多个设备。I2C通常用于连接传感器、存储器、显示器等设备。 IIC 是多主设备的总线，IIC没有物理的芯片选择信号线，没有仲裁逻辑电路，只使用serial data (SDA)数据线 和 serial clock(SCL)时钟线两条信号线，数据线用来传输数据，时钟线用来同步数据收发。两根信号线都是双向传输的，这两条线都是漏极开路或者集电极开路结构，使用时需要外加上拉电阻，可以挂载多个设备。\n","date":"13 June 2023","externalUrl":null,"permalink":"/posts/code/uart-i2c-spi/","section":"Posts","summary":"","title":"UART, I2C, SPI","type":"posts"},{"content":"当Sql语句转换为SqlNode 之后 就会进入下一个阶段：将SqlNode 转换成 Operations\n@Override public List\u0026lt;Operation\u0026gt; parse(String statement) { CalciteParser parser = calciteParserSupplier.get(); FlinkPlannerImpl planner = validatorSupplier.get(); Optional\u0026lt;Operation\u0026gt; command = EXTENDED_PARSER.parse(statement); if (command.isPresent()) { return Collections.singletonList(command.get()); } // parse the sql query // use parseSqlList here because we need to support statement end with \u0026#39;;\u0026#39; in sql client. SqlNodeList sqlNodeList = parser.parseSqlList(statement); List\u0026lt;SqlNode\u0026gt; parsed = sqlNodeList.getList(); Preconditions.checkArgument(parsed.size() == 1, \u0026#34;only single statement supported\u0026#34;); return Collections.singletonList( SqlNodeToOperationConversion.convert(planner, catalogManager, parsed.get(0)) .orElseThrow(() -\u0026gt; new TableException(\u0026#34;Unsupported query: \u0026#34; + statement))); } 其中 SqlNodeToOperationConversion.convert方法，就算是将 SqlNode转换成 Operation\nSqlNod会先进行验证工作，然后再进行转换\npublic static Optional\u0026lt;Operation\u0026gt; convert( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode sqlNode) { // validate the query final SqlNode validated = flinkPlanner.validate(sqlNode); return convertValidatedSqlNode(flinkPlanner, catalogManager, validated); } 为什么 SqlNode 需要验证 # 目的当然是为了确保SQL语句的正确性和合法性，避免在后续的执行过程中出现错误 举例来说，假设有以下的SQL语句：\nSELECT name, age FROM student WHERE age \u0026gt; 18 AND gender = \u0026#39;male\u0026#39; 在进行转换成Operation的过程中，需要先进行validate，检查SQL语句是否符合语法规则和语义规则。如果SQL语句中存在语法错误或者语义错误，那么validate过程会抛出异常，提示用户需要修改SQL语句 例如，如果SQL语句中存在以下错误：\nSELECT name, age FROM student WHERE age \u0026gt; \u0026#39;two\u0026#39; AND gender = \u0026#39;male\u0026#39; 其中，age的类型是整数，但是在SQL语句中使用了字符串类型的\u0026rsquo;two\u0026rsquo;进行比较，这是一个语义错误。在进行validate的过程中，会检测到这个错误并抛出异常，提示用户需要修改SQL语句中的比较条件 因此，先进行validate的过程可以帮助用户在SQL语句执行之前就发现错误，避免在后续的执行过程中出现问题，提高SQL语句的执行效率和准确性\nvalidate # validate的过程的输入和输出都是SqlNode，除了检查类型之外还会更改SqlNode的节点信息\n通过FlinkPlanner 对SqlNode进行检查\ndef validate(sqlNode: SqlNode): SqlNode = { val validator = getOrCreateSqlValidator() validate(sqlNode, validator) } FlinkCalciteSqlValidator 实现 了Calicte 的SqlValidatorImpl 作为validator，就是用来检查FlinkSql的Sql语法的\n/** Extends Calcite\u0026#39;s {@link SqlValidator} by Flink-specific behavior. */ @Internal public final class FlinkCalciteSqlValidator extends SqlValidatorImpl { // Enables CallContext#getOutputDataType() when validating SQL expressions. private SqlNode sqlNodeForExpectedOutputType; private RelDataType expectedOutputType; public FlinkCalciteSqlValidator( SqlOperatorTable opTab, SqlValidatorCatalogReader catalogReader, RelDataTypeFactory typeFactory, SqlValidator.Config config) { super(opTab, catalogReader, typeFactory, config); } 创建 validator # 第一次写入Sql时，会新建一个validator，后面再创建就会使用一个单例来处理\ndef getOrCreateSqlValidator(): FlinkCalciteSqlValidator = { if (validator == null) { val catalogReader = catalogReaderSupplier.apply(false) validator = createSqlValidator(catalogReader) } validator } 其中，catalogReaderSupplier是一个函数，目的是读取和解析数据库中的元信息，比如数据库，表，列等信息\n函数的输入是一个bool值，用来确定大小写是否敏感；输出是FlinkCalciteCatalogReader\nFlinkCalciteCatalogReader相比于CalciteCatalogReader，包含了Flink特有的信息，例如：Flink的UDF函数信息和Flink的Table信息\nFlinkCalciteSqlValidator # 通过数据元信息，就可以创建 SqlValidator\nprivate def createSqlValidator(catalogReader: CalciteCatalogReader) = { val validator = new FlinkCalciteSqlValidator( operatorTable, catalogReader, typeFactory, SqlValidator.Config.DEFAULT .withIdentifierExpansion(true) .withDefaultNullCollation(FlinkPlannerImpl.defaultNullCollation) .withTypeCoercionEnabled(false) ) // Disable implicit type coercion for now. validator } 创建SqlValidator 需要以下信息\nSqlOperatorTable, 定义了Sql操作符和方法，也可以查找这些操作符和方法 这个SqlOperatorTable是在创建 StreamTableEnvironment的时候就创建了，具体是在\n// org.apache.flink.table.planner.delegation.PlannerContext public FrameworkConfig createFrameworkConfig() { return Frameworks.newConfigBuilder() .defaultSchema(rootSchema.plus()) .parserConfig(getSqlParserConfig()) .costFactory(new FlinkCostFactory()) .typeSystem(typeSystem) .convertletTable(FlinkConvertletTable.INSTANCE) .sqlToRelConverterConfig(getSqlToRelConverterConfig()) .operatorTable(getSqlOperatorTable(getCalciteConfig())) // 这里创建 // set the executor to evaluate constant expressions .executor( new ExpressionReducer( context.getTableConfig(), context.getClassLoader(), false)) .context(context) .traitDefs(traitDefs) .build(); } 一般来说 opTab 里包含的都是 Calcite自带的操作符 比如 not like, is not null 这类\nSqlValidatorCatalogReader, 就是上面所说的FlinkCalciteCatalogReader，保存的SQL的元数据 RelDataTypeFactory, FlinkSQL的类型，连接 Flink 的 LogicalType 和 Calcite 的 RelDataType。 RelDataType 是 RelNode关系代数中的数据类型，使用Calcite解析SQL的时候内部会转换成这种类型 /** * Flink specific type factory that represents the interface between Flink\u0026#39;s [[LogicalType]] and * Calcite\u0026#39;s [[RelDataType]]. */ class FlinkTypeFactory( classLoader: ClassLoader, typeSystem: RelDataTypeSystem = FlinkTypeSystem.INSTANCE) extends JavaTypeFactoryImpl(typeSystem) with ExtendedRelTypeFactory { private val seenTypes = mutable.HashMap[LogicalType, RelDataType]() /** * Create a calcite field type in table schema from [[LogicalType]]. It use PEEK_FIELDS_NO_EXPAND * when type is a nested struct type (Flink [[RowType]]). * * @param t * flink logical type. * @return * calcite [[RelDataType]]. */ def createFieldTypeFromLogicalType(t: LogicalType): RelDataType = { def newRelDataType(): RelDataType = t.getTypeRoot match { case LogicalTypeRoot.NULL =\u0026gt; createSqlType(NULL) case LogicalTypeRoot.BOOLEAN =\u0026gt; createSqlType(BOOLEAN) case LogicalTypeRoot.TINYINT =\u0026gt; createSqlType(TINYINT) case LogicalTypeRoot.SMALLINT =\u0026gt; createSqlType(SMALLINT) case LogicalTypeRoot.INTEGER =\u0026gt; createSqlType(INTEGER) case LogicalTypeRoot.BIGINT =\u0026gt; createSqlType(BIGINT) case LogicalTypeRoot.FLOAT =\u0026gt; createSqlType(FLOAT) case LogicalTypeRoot.DOUBLE =\u0026gt; createSqlType(DOUBLE) case LogicalTypeRoot.VARCHAR =\u0026gt; createSqlType(VARCHAR, t.asInstanceOf[VarCharType].getLength) case LogicalTypeRoot.CHAR =\u0026gt; createSqlType(CHAR, t.asInstanceOf[CharType].getLength) // temporal types case LogicalTypeRoot.DATE =\u0026gt; createSqlType(DATE) case LogicalTypeRoot.TIME_WITHOUT_TIME_ZONE =\u0026gt; createSqlType(TIME) // interval types case LogicalTypeRoot.INTERVAL_YEAR_MONTH =\u0026gt; createSqlIntervalType( new SqlIntervalQualifier(TimeUnit.YEAR, TimeUnit.MONTH, SqlParserPos.ZERO)) case LogicalTypeRoot.INTERVAL_DAY_TIME =\u0026gt; createSqlIntervalType( new SqlIntervalQualifier(TimeUnit.DAY, TimeUnit.SECOND, SqlParserPos.ZERO)) case LogicalTypeRoot.BINARY =\u0026gt; createSqlType(BINARY, t.asInstanceOf[BinaryType].getLength) case LogicalTypeRoot.VARBINARY =\u0026gt; createSqlType(VARBINARY, t.asInstanceOf[VarBinaryType].getLength) case LogicalTypeRoot.DECIMAL =\u0026gt; t match { case decimalType: DecimalType =\u0026gt; createSqlType(DECIMAL, decimalType.getPrecision, decimalType.getScale) case legacyType: LegacyTypeInformationType[_] if legacyType.getTypeInformation == BasicTypeInfo.BIG_DEC_TYPE_INFO =\u0026gt; createSqlType(DECIMAL, 38, 18) } case LogicalTypeRoot.ROW =\u0026gt; val rowType = t.asInstanceOf[RowType] buildStructType( rowType.getFieldNames, rowType.getChildren, // fields are not expanded in \u0026#34;SELECT *\u0026#34; StructKind.PEEK_FIELDS_NO_EXPAND) case LogicalTypeRoot.STRUCTURED_TYPE =\u0026gt; t match { case structuredType: StructuredType =\u0026gt; StructuredRelDataType.create(this, structuredType) case legacyTypeInformationType: LegacyTypeInformationType[_] =\u0026gt; createFieldTypeFromLogicalType( PlannerTypeUtils.removeLegacyTypes(legacyTypeInformationType)) } case LogicalTypeRoot.ARRAY =\u0026gt; val arrayType = t.asInstanceOf[ArrayType] createArrayType(createFieldTypeFromLogicalType(arrayType.getElementType), -1) case LogicalTypeRoot.MAP =\u0026gt; val mapType = t.asInstanceOf[MapType] createMapType( createFieldTypeFromLogicalType(mapType.getKeyType), createFieldTypeFromLogicalType(mapType.getValueType)) case LogicalTypeRoot.MULTISET =\u0026gt; val multisetType = t.asInstanceOf[MultisetType] createMultisetType(createFieldTypeFromLogicalType(multisetType.getElementType), -1) case LogicalTypeRoot.RAW =\u0026gt; t match { case rawType: RawType[_] =\u0026gt; new RawRelDataType(rawType) case genericType: TypeInformationRawType[_] =\u0026gt; new GenericRelDataType(genericType, true, getTypeSystem) case legacyType: LegacyTypeInformationType[_] =\u0026gt; createFieldTypeFromLogicalType(PlannerTypeUtils.removeLegacyTypes(legacyType)) } case LogicalTypeRoot.SYMBOL =\u0026gt; createSqlType(SqlTypeName.SYMBOL) case _ @t =\u0026gt; throw new TableException(s\u0026#34;Type is not supported: $t\u0026#34;) } // Kind in TimestampType do not affect the hashcode and equals, So we can\u0026#39;t put it to seenTypes val relType = t.getTypeRoot match { case LogicalTypeRoot.TIMESTAMP_WITHOUT_TIME_ZONE =\u0026gt; val timestampType = t.asInstanceOf[TimestampType] timestampType.getKind match { case TimestampKind.ROWTIME =\u0026gt; createRowtimeIndicatorType(t.isNullable, false) case TimestampKind.REGULAR =\u0026gt; createSqlType(TIMESTAMP, timestampType.getPrecision) case TimestampKind.PROCTIME =\u0026gt; throw new TableException( s\u0026#34;Processing time indicator only supports\u0026#34; + s\u0026#34; LocalZonedTimestampType, but actual is TimestampType.\u0026#34; + s\u0026#34; This is a bug in planner, please file an issue.\u0026#34;) } case LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE =\u0026gt; val lzTs = t.asInstanceOf[LocalZonedTimestampType] lzTs.getKind match { case TimestampKind.PROCTIME =\u0026gt; createProctimeIndicatorType(t.isNullable) case TimestampKind.ROWTIME =\u0026gt; createRowtimeIndicatorType(t.isNullable, true) case TimestampKind.REGULAR =\u0026gt; createSqlType(TIMESTAMP_WITH_LOCAL_TIME_ZONE, lzTs.getPrecision) } case _ =\u0026gt; seenTypes.get(t) match { case Some(retType: RelDataType) =\u0026gt; retType case None =\u0026gt; val refType = newRelDataType() seenTypes.put(t, refType) refType } } createTypeWithNullability(relType, t.isNullable) } SqlValidator.Config, SqlValidator的配置信息，这里默认使用的是 ImmutableSqlValidator SqlValidator.Config DEFAULT = ImmutableSqlValidator.Config.builder() .withTypeCoercionFactory(TypeCoercions::createTypeCoercion) .build(); 开始验证 # 创建validator 后 根据validate 来验证 sqlNode\ngraph TB start((\u0026#34;开始\u0026#34;)) --\u0026gt; A(\u0026#34;预处理重写SqlNode\u0026#34;) --\u0026gt; B(\u0026#34;如果有扩展节点，先处理扩展验证\u0026#34;) B --\u0026gt; C{\u0026#34;是否是DDL等\u0026#34;} C --Y--\u0026gt; ends(\u0026#34;结束\u0026#34;) C --N--\u0026gt;D(\u0026#34;根据不同的Sql语句进行验证\u0026#34;) --\u0026gt;E(\u0026#34;返回SqlNode\u0026#34;)--\u0026gt;ends 重写 # 为什么要重写？\n重写相当于是在SqlNode进行一些预处理主要作用是检查查询语句是否符合Flink Table的语法规范，并对查询语句进行一些必要的转换和优化，以便更好地支持Flink的执行引擎\nFlink中重写是依靠PreValidateReWriter来实现的\nsqlNode.accept(new PreValidateReWriter(validator, typeFactory)) 如果是简单的SQL其实不需要重写\noverride def visit(call: SqlCall): Unit = { call match { case e: SqlRichExplain =\u0026gt; e.getStatement match { case r: RichSqlInsert =\u0026gt; rewriteInsert(r) case _ =\u0026gt; // do nothing } case r: RichSqlInsert =\u0026gt; rewriteInsert(r) case _ =\u0026gt; // do nothing } } validator验证SqlNode # 如果不是特殊的SQL，应该都会使用 validator来验证SqlNode\ncase _ =\u0026gt; validator.validate(sqlNode) // org.apache.calcite.sql.validate.SqlValidatorImpl; @Override public SqlNode validate(SqlNode topNode) { SqlValidatorScope scope = new EmptyScope(this); scope = new CatalogScope(scope, ImmutableList.of(\u0026#34;CATALOG\u0026#34;)); final SqlNode topNode2 = validateScopedExpression(topNode, scope); final RelDataType type = getValidatedNodeType(topNode2); Util.discard(type); return topNode2; } SqlValidatorScope # SqlValidatorScope是Calcite中的一个接口，用于表示SQL语句中的作用域。它包含了当前作用域中可见的所有表、列、函数等信息。 具体来说，SqlValidatorScope中包含了以下信息：\n当前作用域中可见的所有表，包括别名和表的元数据信息。 当前作用域中可见的所有列，包括别名和列的元数据信息。 当前作用域中可见的所有函数，包括函数的元数据信息和参数信息。 当前作用域中可见的所有变量，包括变量的类型和值。 举例说明，假设有以下SQL语句：\nSELECT a.name, b.salary FROM employee a JOIN salary b ON a.id = b.id WHERE b.salary \u0026gt; 5000; 在这个SQL语句中，作用域可以分为以下几个部分：\nSELECT子句中的作用域，包括a.name和b.salary两个列。 FROM子句中的作用域，包括employee和salary两个表。 JOIN子句中的作用域，包括a和b两个表的别名。 WHERE子句中的作用域，包括b.salary列和5000常量。 在每个作用域中，SqlValidatorScope都会包含相应的表、列、函数等信息，以便进行语法和语义的验证。\nSqlValidatorScope 本身也是一个树形结构，根节点是一个空scope\nvalidateScopedExpression # 验证过程的第一步就是重写SqlNode，将SqlNode中的不确定的地方都进行更新\n然后如果是Select这种简单的SQL，会将查询进行注册\ngraph TB A((\u0026#34;开始\u0026#34;)) --\u0026gt; B(\u0026#34;SqlNode重写\u0026#34;) --\u0026gt;C(\u0026#34;注册查询\u0026#34;) X --\u0026gt; Z((\u0026#34;结束\u0026#34;)) private SqlNode validateScopedExpression(SqlNode topNode, SqlValidatorScope scope) { SqlNode outermostNode = performUnconditionalRewrites(topNode, false); cursorSet.add(outermostNode); top = outermostNode; TRACER.trace(\u0026#34;After unconditional rewrite: {}\u0026#34;, outermostNode); if (outermostNode.isA(SqlKind.TOP_LEVEL)) { registerQuery(scope, null, outermostNode, outermostNode, null, false); } outermostNode.validate(this, scope); if (!outermostNode.isA(SqlKind.TOP_LEVEL)) { // force type derivation so that we can provide it to the // caller later without needing the scope deriveType(scope, outermostNode); } TRACER.trace(\u0026#34;After validation: {}\u0026#34;, outermostNode); return outermostNode; } performUnconditionalRewrites\ngraph TB A((\u0026#34;开始\u0026#34;)) --\u0026gt; B(\u0026#34;如果SqlNode是SqlCall\u0026#34;) --\u0026gt;|是|C(\u0026#34;获取SqlNode所有子SqlNode(算子)\u0026#34;) --\u0026gt; C1(\u0026#34;遍历算子，递归调用得到重写的算子,并更新\u0026#34;) --\u0026gt;|递归| B C1 --\u0026gt; C2(\u0026#34;如果SqlNode的操作符是函数\u0026#34;) --\u0026gt;|是|C3(\u0026#34;查找内部函数并替换\u0026#34;) --\u0026gt; C4(\u0026#34;如果配置重写，则操作符更新\u0026#34;) B --\u0026gt;|否|D{\u0026#34;是否是SqlNodeList\u0026#34;} --\u0026gt;|是|E(\u0026#34;遍历List,递归调用重写，并更新\u0026#34;)--\u0026gt;|递归| B C4 --\u0026gt; D E --\u0026gt;F{\u0026#34;判断SqlNode的Kind类型\u0026#34;} F --\u0026gt;|\u0026#34;VALUES，ORDER_BY等其他关键字\u0026#34;|G(\u0026#34;根据不同的关键字规则重写\u0026#34;) --\u0026gt;H(\u0026#34;返回SqlNode\u0026#34;) --\u0026gt; Z((\u0026#34;结束\u0026#34;)) F --\u0026gt;|\u0026#34;其他(如Select)\u0026#34;|H 这里大部分都是递归调用，能够实际重写的大部分都是函数调用\nif (call.getOperator() instanceof SqlUnresolvedFunction) { assert call instanceof SqlBasicCall; final SqlUnresolvedFunction function = (SqlUnresolvedFunction) call.getOperator(); // This function hasn\u0026#39;t been resolved yet. Perform // a half-hearted resolution now in case it\u0026#39;s a // builtin function requiring special casing. If it\u0026#39;s // not, we\u0026#39;ll handle it later during overload resolution. final List\u0026lt;SqlOperator\u0026gt; overloads = new ArrayList\u0026lt;\u0026gt;(); opTab.lookupOperatorOverloads( function.getNameAsId(), function.getFunctionType(), SqlSyntax.FUNCTION, overloads, catalogReader.nameMatcher()); if (overloads.size() == 1) { ((SqlBasicCall) call).setOperator(overloads.get(0)); } } SqlNode的lookupOperatorOverloads方法主要是用于查找可用的操作符重载。具体来说，它会根据传入的操作符名称和参数类型，查找符合条件的操作符重载方法。\n举个例子，假设有以下的SqlNode：\nSELECT * FROM table WHERE column1 + column2 = 10 在这个SqlNode中，有一个加法操作符“+”，它的左右两边分别是column1和column2这两个列。因此，lookupOperatorOverloads方法会首先根据“+”操作符名称查找可用的操作符重载方法。如果找到了多个重载方法，它会根据参数类型进一步筛选出符合条件的重载方法。\n假设我们定义了以下的操作符重载方法：\npublic static int operator +(int a, int b) {...} public static double operator +(double a, double b) {...} public static string operator +(string a, string b) {...} 在这种情况下，lookupOperatorOverloads方法会根据column1和column2的数据类型来选择合适的重载方法。\n如果column1和column2都是int类型，那么会选择第一个重载方法； 如果column1和column2都是double类型，那么会选择第二个重载方法； 如果column1和column2都是string类型，那么会选择第三个重载方法。 如果找不到符合条件的操作符重载方法，lookupOperatorOverloads方法会抛出异常。\nregisterQuery\ngraph TB A((\u0026#34;开始\u0026#34;)) --\u0026gt; B(\u0026#34;区分不同的nodeKind\u0026#34;) --\u0026gt;|SELECT|C(\u0026#34;创建并注册Select命名空间\u0026#34;) --\u0026gt;D(\u0026#34;创建Select作用域\u0026#34;) D --\u0026gt; E(\u0026#34;注册where子句\u0026#34;) --\u0026gt; F(\u0026#34;注册from子句\u0026#34;) --\u0026gt; G(\u0026#34;创建并注册From Identifier命名空间\u0026#34;) --\u0026gt; H(\u0026#34;设置TableScope\u0026#34;) --\u0026gt; I(\u0026#34;Select的SqlNode设置新的From的Node\u0026#34;) I --\u0026gt; J(\u0026#34;如果有聚合方法,需要分别注册groupByScope, HavingScope\u0026#34;) --\u0026gt; K(\u0026#34;如果有orderby还需要注册OrderByScope\u0026#34;) K --\u0026gt; Z((\u0026#34;结束\u0026#34;)) private void registerQuery( SqlValidatorScope parentScope, @Nullable SqlValidatorScope usingScope, SqlNode node, SqlNode enclosingNode, @Nullable String alias, boolean forceNullable, boolean checkUpdate) { ... } enclosingNode和Node都是AST节点，但是它们的含义和使用方式有所不同。\nenclosingNode表示当前节点所在的最近的语法结构节点，例如SELECT语句中的FROM子句，WHERE子句等。enclosingNode可以通过调用getEnclosingNode方法获取。\nNode表示当前节点本身，例如SELECT语句中的SELECT子句，FROM子句等。Node可以通过调用getNode方法获取。\n举个例子，假设有以下SQL语句：\nSELECT a, b FROM table1 WHERE a \u0026gt; 10 在validate过程中，对于SELECT子句中的a和b，它们的enclosingNode都是SELECT语句本身，而它们的Node分别是ColumnRef节点，表示一个列引用。\n对于WHERE子句中的a \u0026gt; 10，它的enclosingNode是SELECT语句的WHERE子句，而它的Node是一个比较操作符节点，表示一个大于号操作。\n在validate过程中，Calcite会对每个节点进行类型检查、语法检查等操作，以确保SQL语句的正确性。在这个过程中，enclosingNode和Node都会被用到，以确定当前节点的上下文和语义。\nvalidate\n检查验证状态，如果没有验证，则开始验证，然后验证类型，Select子句会调用SelectNamespace的验证，最后根据不同的子句分别验证 验证对表的访问(只有from子句才会真正被验证) 验证快照到表 验证Select和其子句的形态 Relation 还是 Stream Modality指的是数据流的模式，即数据流的类型和结构。在Relational模式下，数据流是以表格的形式呈现，每个数据流都有一个固定的列集合和数据类型。而在Streaming模式下，数据流是以流的形式呈现，数据是按照时间顺序到达的，每个数据流都是一个无限的数据流。\n举例来说，如果我们有一个查询语句：\nSELECT name, age FROM employees WHERE age \u0026gt; 30 在Relational模式下，我们需要知道employees表格的列集合和数据类型，以及age列的数据类型是什么，才能验证这个查询语句是否合法。 而在Streaming模式下，我们需要知道employees表格的列集合和数据类型，以及数据流是按照时间顺序到达的，才能验证这个查询语句是否合法。 验证过程中我们需要根据数据流的模式来验证查询语句是否合法。如果数据流的模式与查询语句不匹配，就会抛出异常。\n例如，在Relational模式下，如果查询语句中引用了不存在的列，就会抛出异常；在Streaming模式下，如果查询语句中使用了聚合函数，就会抛出异常，因为数据流是无限的，无法计算聚合函数的结果。\n举例 # 以\nselect * from tableA where amount \u0026gt; 2 为例，\ngraph BT C(\u0026#34;SelectScope\u0026#34;) --\u0026gt; B(\u0026#34;CatalogScope\u0026#34;) --\u0026gt; A(\u0026#34;EmptyScope\u0026#34;) E(\u0026#34;tableA 命名空间\u0026#34;) --\u0026gt; D(\u0026#34;TableScope\u0026#34;) --\u0026gt; B ","date":"6 June 2023","externalUrl":null,"permalink":"/posts/code/flinksql---%E9%AA%8C%E8%AF%81%E8%BF%87%E7%A8%8B/","section":"Posts","summary":"","title":"FlinkSQL - 验证过程","type":"posts"},{"content":"parseSqlList # Sql语句从ParseImpl中调用CalciteParser的方法parseSqlList，将SQL语句转换成SqlNode\n// org.apache.flink.table.planner.parse.CalciteParser public SqlNodeList parseSqlList(String sql) { try { SqlParser parser = SqlParser.create(sql, config); return parser.parseStmtList(); } catch (SqlParseException e) { if (e.getMessage().contains(\u0026#34;Encountered \\\u0026#34;\u0026lt;EOF\u0026gt;\\\u0026#34;\u0026#34;)) { throw new SqlParserEOFException(e.getMessage(), e); } throw new SqlParserException(\u0026#34;SQL parse failed. \u0026#34; + e.getMessage(), e); } } 其中 parser 的创建 使用到了 calcite 的 SqlParser实例\n生成 SqlParser 需要两个参数\nsql String 类型，就是传入的Sql字符串 config SqlParser.Config类型 SqlParser.Config # Flink 在解析sqlQuery 或者 createTemporaryView 这类操作SQL的处理中，就会创建CalciteParser,在创建CalciteParser的时候会调用\ngetSqlParserConfig() 的方法获取SqlParser.Config\n// org.apache.flink.table.planner.delegation.PlannerContext /** * Returns the SQL parser config for this environment including a custom Calcite configuration. */ private SqlParser.Config getSqlParserConfig() { return JavaScalaConversionUtil.\u0026lt;SqlParser.Config\u0026gt;toJava( getCalciteConfig().getSqlParserConfig()) .orElseGet( // we use Java lex because back ticks are easier than double quotes in // programming and cases are preserved () -\u0026gt; { SqlConformance conformance = getSqlConformance(); return SqlParser.config() .withParserFactory(FlinkSqlParserFactories.create(conformance)) .withConformance(conformance) .withLex(Lex.JAVA) .withIdentifierMaxLength(256); }); } 如果设置了Calicte的一些配置，那么就会在这里被读取出来自定义的CalicteParser， 否则，走orElseGet信息\nSqlParser在这里设置了解析SQL的语法、词法、方言以及限定词的最大长度\nSqlConformance # SqlConformance 是用来设置 SQL的方言(或标准)的 在FlinkSQL中一共有两种方言，\nCalcite 默认方言 Hive 方言 FlinkSqlParserFactories # /** A util method to create SqlParserImplFactory according to SqlConformance. */ public class FlinkSqlParserFactories { private FlinkSqlParserFactories() {} public static SqlParserImplFactory create(SqlConformance conformance) { if (conformance == FlinkSqlConformance.DEFAULT) { return FlinkSqlParserImpl.FACTORY; } else { throw new TableException(\u0026#34;Unsupported SqlConformance: \u0026#34; + conformance); } } } 这里就很有意思，FlinkSqlParser只接受Calcite默认方言\n然后再看FlinkSqlParserImpl\n// package org.apache.flink.sql.parser.impl; /** * SQL parser, generated from Parser.jj by JavaCC. * * \u0026lt;p\u0026gt;The public wrapper for this parser is {@link SqlParser}. */ public class FlinkSqlParserImpl extends SqlAbstractParserImpl implements FlinkSqlParserImplConstants { private static final Logger LOGGER = CalciteTrace.getParserTracer(); 可以看出，FlinkSqlParserImpl 是通过JavaCC生成的\n生成此类的 JavaCC 文件 在\nflink-table/flink-sql-parser/target/generated-sources/javacc/Parser.jj 而这个应该是根据 FreeMarker的模板fmpp来生成的,打开 flink-sql-parser 下面 codegen里面的config.fmpp，我们可以发现一下一段话，答题意思就是说FMPP继承了CalciteParser，然后解析指定的Sql\n# This file is an FMPP (http://fmpp.sourceforge.net/) configuration file to # allow clients to extend Calcite\u0026#39;s SQL parser to support application specific # SQL statements, literals or data types. # # Calcite\u0026#39;s parser grammar file (Parser.jj) is written in javacc # (https://javacc.org/) with Freemarker (http://freemarker.org/) variables # to allow clients to: # 1. have custom parser implementation class and package name. # 2. insert new parser method implementations written in javacc to parse # custom: # a) SQL statements. # b) literals. # c) data types. # 3. add new keywords to support custom SQL constructs added as part of (2). # 4. add import statements needed by inserted custom parser implementations. # # Parser template file (Parser.jj) along with this file are packaged as # part of the calcite-core-\u0026lt;version\u0026gt;.jar under \u0026#34;codegen\u0026#34; directory. 简单看一下这个jj文件，可以发现，就是在定义SQL如何声明，以及如何处理SQL语句的\nSqlLiteral AllOrDistinct() : { } { \u0026lt;DISTINCT\u0026gt; { return SqlSelectKeyword.DISTINCT.symbol(getPos()); } | \u0026lt;ALL\u0026gt; { return SqlSelectKeyword.ALL.symbol(getPos()); } } 在 flink-sql-parser 中还可以看到生成这些FlinkSQL的逻辑，比如这种 Flink的建表语句如何判断的逻辑，都在这里可以找到\n// org.apache.flink.sql.parser.ddl public class SqlCreateTable extends SqlCreate implements ExtendedSqlNode { public static final SqlSpecialOperator OPERATOR = new SqlSpecialOperator(\u0026#34;CREATE TABLE\u0026#34;, SqlKind.CREATE_TABLE); ... writer.keyword(\u0026#34;CREATE\u0026#34;); if (isTemporary()) { writer.keyword(\u0026#34;TEMPORARY\u0026#34;); } writer.keyword(\u0026#34;TABLE\u0026#34;); if (isIfNotExists()) { writer.keyword(\u0026#34;IF NOT EXISTS\u0026#34;); } tableName.unparse(writer, leftPrec, rightPrec); if (columnList.size() \u0026gt; 0 || tableConstraints.size() \u0026gt; 0 || watermark != null) { SqlUnparseUtils.unparseTableSchema( writer, leftPrec, rightPrec, columnList, tableConstraints, watermark); } if (comment != null) { writer.newlineAndIndent(); writer.keyword(\u0026#34;COMMENT\u0026#34;); comment.unparse(writer, leftPrec, rightPrec); } ... 返回jj文件中，我们可以看出一条SQL语句是如何拆分成一步一步拆分成SqlNode的过程\nSqlNodeList SqlStmtList() : { final List\u0026lt;SqlNode\u0026gt; stmtList = new ArrayList\u0026lt;SqlNode\u0026gt;(); SqlNode stmt; } { stmt = SqlStmt() { stmtList.add(stmt); } ( \u0026lt;SEMICOLON\u0026gt; [ stmt = SqlStmt() { stmtList.add(stmt); } ] )* \u0026lt;EOF\u0026gt; { return new SqlNodeList(stmtList, Span.of(stmtList).pos()); } } Lex # Calcite 中的 Lex 是一个词法分析器，用于将输入的 SQL 语句转换为一系列的词法单元（tokens）。这些词法单元可以被后续的语法分析器（parser）用来构建语法树。\n举个例子，假设我们有一个 SQL 查询语句：\nSELECT name, age FROM users WHERE age \u0026gt; 18; Lex 会将这个语句分解为以下词法单元：\nSELECT name , age FROM users WHERE age \u0026gt; 18 ; 这些词法单元可以被 Calcite 中的语法分析器用来构建语法树，进而进行查询优化和执行\nSqlParser # 了解了 SqlParser.Config的构成，SqlParser就更简单了创建过程就是将Config中的的数据拿出来\n//~ Constructors ----------------------------------------------------------- private SqlParser(SqlAbstractParserImpl parser, Config config) { this.parser = parser; parser.setTabSize(1); parser.setQuotedCasing(config.quotedCasing()); parser.setUnquotedCasing(config.unquotedCasing()); parser.setIdentifierMaxLength(config.identifierMaxLength()); parser.setConformance(config.conformance()); parser.switchTo(SqlAbstractParserImpl.LexicalState.forConfig(config)); } 其中parser.switchTo 用于切换解析器的状态 具体来说，switchTo方法接受一个参数，表示要切换到的状态。在解析SQL语句的过程中，不同的状态对应着不同的语法规则和解析方式。通过切换状态，解析器可以根据当前的语法规则和上下文信息，正确地解析SQL语句。 举个例子，假设我们有一个SQL语句：\nSELECT name, age FROM users WHERE age \u0026gt; 18; 在解析这个SQL语句时，解析器需要根据不同的关键字和符号，判断当前的语法状态。比如，在解析SELECT关键字时，解析器需要切换到SELECT状态；在解析FROM关键字时，解析器需要切换到FROM状态；在解析WHERE关键字时，解析器需要切换到WHERE状态。 具体的实现可以参考SqlAbstractParserImpl类的源代码。\n所以一条字符串类型的Sql语句 转换成 SqlNode 的流程应该就是\ngraph LR A(\u0026#34;FlinkSQL\u0026#34;) --\u0026gt; B(\u0026#34;语法分析规则flink-table-parser\u0026#34;) --\u0026gt; C(\u0026#34;fmpp语法分析规则模板\u0026#34;) --\u0026gt; D(\u0026#34;Parser.jj\u0026#34;) --\u0026gt; E(\u0026#34;FlinkSqlParserImpl\u0026#34;) --\u0026gt; F(\u0026#34;SqlParser\u0026#34;) I(\u0026#34;SQL语句\u0026#34;) --\u0026gt; G(\u0026#34;parseSqlList\u0026#34;) --\u0026gt; F --\u0026gt; H(\u0026#34;SqlNode\u0026#34;) ","date":"27 May 2023","externalUrl":null,"permalink":"/posts/code/flinksql---sql%E8%AF%AD%E5%8F%A5%E5%88%B0sqlnode/","section":"Posts","summary":"","title":"FlinkSQL - SQL语句到SqlNode","type":"posts"},{"content":"","date":"27 May 2023","externalUrl":null,"permalink":"/tags/javacc/","section":"Tags","summary":"","title":"Javacc","type":"tags"},{"content":" 简介 # Apache Calcite是一个动态数据管理框架。\n它包含构成典型数据库管理系统的许多部分，但省略了一些关键功能:数据存储、处理数据的算法和存储元数据的存储库。\n我们主要用 Calcite 来解析SQL。很多项目也是直接使用了Calcite来解析SQL，优化SQL等，比如 FlinkSQL\ncalcite的基本架构如下 关系代数 # 关系模型源于数学。关系是由元组构成的集合，可以通过关系的运算来表达查询要求，而关系代数恰恰是关系操作语言的一种传统的表示方式，它是一种抽象的查询语言。\n关系代数的运算对象是关系，运算结果也是关系。与一般的运算一样，运算对象、运算符和运算结果是关系代数的三大要素。\n关系代数的运算可分为两大类：\n传统的集合运算。这类运算完全把关系看成元组的集合。传统的集合运算包括集合的广义笛卡儿积运算、并运算、交运算和差运算。 专门的关系运算。这类运算除了把关系看成元组的集合外，还通过运算表达了查询的要求。专门的关系运算包括选择、投影、连接和除运算。 关系代数中的运算符可以分为四类：传统的集合运算符、专门的关系运算符、比较运算符和逻辑运算符。 关系运算 和 SQL的关系如下\n运算符 SQL关键字 含义 分类 $$\\cap$$ 交 集合运算 $$\\cup$$ 并 集合运算 $$-$$ 差 集合运算 $$\\times$$ from A,B 广义笛卡尔积 集合运算 $$\\sigma$$ where 选择 关系运算 $$\\Pi$$ select distinct 投影 关系运算 $$\\bowtie$$ join 连接 关系运算 $$\\div$$ 除 关系运算 $$\u003e$$ 大于 比较运算 $$\u003c$$ 小于 比较运算 $$=$$ 等于 比较运算 $$\\neq$$ 不等 比较运算 $$\\leqslant$$ 小于等于 比较运算 $$\\geqslant$$ 大于等于 比较运算 $$\\neg$$ 非 逻辑运算 $$\\land$$ 与 逻辑运算 $$\\lor$$ 或 逻辑运算 SQL语句会先翻译成为关系代数后再被执行的 在执行explain 一条SQL的时候 就可以看到翻译后关系代数的命名\n== Abstract Syntax Tree == LogicalProject(user=[$0], product=[$1], amount=[$2]) +- LogicalFilter(condition=[\u0026gt;($2, 2)]) +- LogicalTableScan(table=[[*anonymous_datastream_source$1*]]) Calcite 解析流程 # Parser 将SQL语句(字符串) 解析成 AST(抽象语法树) 语法树的节点在代码中为SqlNode Validate 校验SqlNode节点，替换节点中的部分属性，设置单调性等信息 Convert 将SqlNode 抽象语法树 转换成RelNode，执行逻辑执行计划 Optimize 成本优化 Execute 生成动态代码，并执行物理执行计划 其中，\nSqlNode 是语法树上的节点，其本质是把SQL语句进行拆解 RelNode 是关系节点，代表的事关系代数中的关系操作，RelNode 更倾向数学的概念，就可以进行下一步的优化了 RexNode 虽然RexNode也属于关系节点，但是这里的RexNode更偏向于去表示表达式，比如一个常数，或者是 简单的a+b的运算，亦或是count(*) 这样的聚合；所以一个RelNode中会有很多RexNode节点 SqlNode # SqlNode 是一个抽象类，没有过多的信息，就包括一个位置信息的对象SqlParserPos\nSqlParserPos 是用来表示SQL语句中的位置信息的类。它包含了行号、列号和字符偏移量等信息，可以用于定位SQL语句中的错误或者生成更加详细的错误信息\npublic abstract class SqlNode implements Cloneable { //~ Static fields/initializers --------------------------------------------- public static final @Nullable SqlNode[] EMPTY_ARRAY = new SqlNode[0]; //~ Instance fields -------------------------------------------------------- protected final SqlParserPos pos; } public class SqlParserPos implements Serializable { //~ Static fields/initializers --------------------------------------------- /** * SqlParserPos representing line one, character one. Use this if the node * doesn\u0026#39;t correspond to a position in piece of SQL text. */ public static final SqlParserPos ZERO = new SqlParserPos(0, 0); /** Same as {@link #ZERO} but always quoted. **/ public static final SqlParserPos QUOTED_ZERO = new QuotedParserPos(0, 0, 0, 0); private static final long serialVersionUID = 1L; //~ Instance fields -------------------------------------------------------- private final int lineNumber; private final int columnNumber; private final int endLineNumber; private final int endColumnNumber; } 从SqlSelect的这个类中就可以看出，一个简单的SQL语句在SqlSelect中都能找到\npublic class SqlSelect extends SqlCall { //~ Static fields/initializers --------------------------------------------- // constants representing operand positions public static final int FROM_OPERAND = 2; public static final int WHERE_OPERAND = 3; public static final int HAVING_OPERAND = 5; SqlNodeList keywordList; SqlNodeList selectList; @Nullable SqlNode from; @Nullable SqlNode where; @Nullable SqlNodeList groupBy; @Nullable SqlNode having; SqlNodeList windowDecls; @Nullable SqlNodeList orderBy; @Nullable SqlNode offset; @Nullable SqlNode fetch; @Nullable SqlNodeList hints; } 这里的keywordList是保留字，如果有 distinct 这种就会放在这个keywordList中 不过从debug的过程来看 这个keywordList是个空的数据\nRelNode # RelNode 是个接口 继承了 RelOptNode\nRelOptNode 目前没有看到除了RelNode以外有其他地方使用和实现\nRelNode 接口定义了树结构的父类以及节点数据和类型的方法 真正处理的方法都放在了 RelShuttle 和 RexShuttle中\npublic interface RelNode extends RelOptNode, Cloneable { /** * Accepts a visit from a shuttle. * * @param shuttle Shuttle * @return A copy of this node incorporating changes made by the shuttle to * this node\u0026#39;s children */ RelNode accept(RelShuttle shuttle); /** * Accepts a visit from a shuttle. If the shuttle updates expression, then * a copy of the relation should be created. This new relation might have * a different row-type. * * @param shuttle Shuttle * @return A copy of this node incorporating changes made by the shuttle to * this node\u0026#39;s children */ RelNode accept(RexShuttle shuttle); } 可以具体一点，看一下比较常用的RelNode 的实现是如何定义的：LogicalProject\n/** * Sub-class of Project not targeted at any particular engine or calling convention. */ public final class LogicalProject extends Project { //~ Constructors ----------------------------------------------------------- /** * Creates a LogicalProject. * * \u0026lt;p\u0026gt;Use {@link #create} unless you know what you\u0026#39;re doing. * * @param cluster Cluster this relational expression belongs to * @param traitSet Traits of this relational expression * @param hints Hints of this relational expression * @param input Input relational expression * @param projects List of expressions for the input columns * @param rowType Output row type */ public LogicalProject( RelOptCluster cluster, RelTraitSet traitSet, List\u0026lt;RelHint\u0026gt; hints, RelNode input, List\u0026lt;? extends RexNode\u0026gt; projects, RelDataType rowType) { super(cluster, traitSet, hints, input, projects, rowType); assert traitSet.containsIfApplicable(Convention.NONE); } /** * Creates a LogicalProject by parsing serialized output. */ public LogicalProject(RelInput input) { super(input); } //~ Methods ---------------------------------------------------------------- /** Creates a LogicalProject. */ public static LogicalProject create(final RelNode input, List\u0026lt;RelHint\u0026gt; hints, final List\u0026lt;? extends RexNode\u0026gt; projects, @Nullable List\u0026lt;? extends @Nullable String\u0026gt; fieldNames) { final RelOptCluster cluster = input.getCluster(); final RelDataType rowType = RexUtil.createStructType(cluster.getTypeFactory(), projects, fieldNames, SqlValidatorUtil.F_SUGGESTER); return create(input, hints, projects, rowType); } /** Creates a LogicalProject, specifying row type rather than field names. */ public static LogicalProject create(final RelNode input, List\u0026lt;RelHint\u0026gt; hints, final List\u0026lt;? extends RexNode\u0026gt; projects, RelDataType rowType) { final RelOptCluster cluster = input.getCluster(); final RelMetadataQuery mq = cluster.getMetadataQuery(); final RelTraitSet traitSet = cluster.traitSet().replace(Convention.NONE) .replaceIfs(RelCollationTraitDef.INSTANCE, () -\u0026gt; RelMdCollation.project(mq, input, projects)); return new LogicalProject(cluster, traitSet, hints, input, projects, rowType); } } LocalProject 的变量定义都散落在他的父类中\n/** * Relational expression that computes a set of \u0026#39;select expressions\u0026#39; from its input relational expression. * See Also: * org.apache.calcite.rel.logical.LogicalProject */ public abstract class Project extends SingleRel implements Hintable { //~ Instance fields -------------------------------------------------------- protected final ImmutableList\u0026lt;RexNode\u0026gt; exps; protected final ImmutableList\u0026lt;RelHint\u0026gt; hints; } /** * Abstract base class for relational expressions with a single input. * * \u0026lt;p\u0026gt;It is not required that single-input relational expressions use this * class as a base class. However, default implementations of methods make life * easier. */ public abstract class SingleRel extends AbstractRelNode { //~ Instance fields -------------------------------------------------------- protected RelNode input; } RexNode # RexNode 是 行表达式(Row Expression) 是通过SqlNode 转换过来 尤其是 select 的项 where 的条件 等等 与SqlNode不同的是,RexNode的类型都是确定的，SqlNode的类型是在优化前就已经定好了，所以这个类型可能不能用\nRexNode 是一个抽象类，也是实现了访问者模式\n/** * Row expression. * * \u0026lt;p\u0026gt;Every row-expression has a type. * (Compare with {@link org.apache.calcite.sql.SqlNode}, which is created before * validation, and therefore types may not be available.) * * \u0026lt;p\u0026gt;Some common row-expressions are: {@link RexLiteral} (constant value), * {@link RexVariable} (variable), {@link RexCall} (call to operator with * operands). Expressions are generally created using a {@link RexBuilder} * factory.\u0026lt;/p\u0026gt; * * \u0026lt;p\u0026gt;All sub-classes of RexNode are immutable.\u0026lt;/p\u0026gt; */ public abstract class RexNode { //~ Instance fields -------------------------------------------------------- // Effectively final. Set in each sub-class constructor, and never re-set. protected @MonotonicNonNull String digest; //~ Methods ---------------------------------------------------------------- public abstract RelDataType getType(); /** * Returns whether this expression always returns true. (Such as if this * expression is equal to the literal \u0026lt;code\u0026gt;TRUE\u0026lt;/code\u0026gt;.) */ public boolean isAlwaysTrue() { return false; } /** * Returns whether this expression always returns false. (Such as if this * expression is equal to the literal \u0026lt;code\u0026gt;FALSE\u0026lt;/code\u0026gt;.) */ public boolean isAlwaysFalse() { return false; } public boolean isA(SqlKind kind) { return getKind() == kind; } public boolean isA(Collection\u0026lt;SqlKind\u0026gt; kinds) { return getKind().belongsTo(kinds); } /** * Returns the kind of node this is. * * @return Node kind, never null */ public SqlKind getKind() { return SqlKind.OTHER; } @Override public String toString() { return requireNonNull(digest, \u0026#34;digest\u0026#34;); } /** Returns the number of nodes in this expression. * * \u0026lt;p\u0026gt;Leaf nodes, such as {@link RexInputRef} or {@link RexLiteral}, have * a count of 1. Calls have a count of 1 plus the sum of their operands. * * \u0026lt;p\u0026gt;Node count is a measure of expression complexity that is used by some * planner rules to prevent deeply nested expressions. */ public int nodeCount() { return 1; } /** * Accepts a visitor, dispatching to the right overloaded * {@link RexVisitor#visitInputRef visitXxx} method. * * \u0026lt;p\u0026gt;Also see {@link RexUtil#apply(RexVisitor, java.util.List, RexNode)}, * which applies a visitor to several expressions simultaneously. */ public abstract \u0026lt;R\u0026gt; R accept(RexVisitor\u0026lt;R\u0026gt; visitor); /** * Accepts a visitor with a payload, dispatching to the right overloaded * {@link RexBiVisitor#visitInputRef(RexInputRef, Object)} visitXxx} method. */ public abstract \u0026lt;R, P\u0026gt; R accept(RexBiVisitor\u0026lt;R, P\u0026gt; visitor, P arg); /** {@inheritDoc} * * \u0026lt;p\u0026gt;Every node must implement {@link #equals} based on its content */ @Override public abstract boolean equals(@Nullable Object obj); /** {@inheritDoc} * * \u0026lt;p\u0026gt;Every node must implement {@link #hashCode} consistent with * {@link #equals} */ @Override public abstract int hashCode(); } 还是具体一点看一下 RexCall的定义 RexCall是有操作符的表达式，操作符可以是 一元二元，也可是函数，或者是固定语法\n/** * An expression formed by a call to an operator with zero or more expressions * as operands. * * \u0026lt;p\u0026gt;Operators may be binary, unary, functions, special syntactic constructs * like \u0026lt;code\u0026gt;CASE ... WHEN ... END\u0026lt;/code\u0026gt;, or even internally generated * constructs like implicit type conversions. The syntax of the operator is * really irrelevant, because row-expressions (unlike * {@link org.apache.calcite.sql.SqlNode SQL expressions}) * do not directly represent a piece of source code. * * \u0026lt;p\u0026gt;It\u0026#39;s not often necessary to sub-class this class. The smarts should be in * the operator, rather than the call. Any extra information about the call can * often be encoded as extra arguments. (These don\u0026#39;t need to be hidden, because * no one is going to be generating source code from this tree.)\u0026lt;/p\u0026gt; */ public class RexCall extends RexNode { //~ Instance fields -------------------------------------------------------- public final SqlOperator op; public final ImmutableList\u0026lt;RexNode\u0026gt; operands; public final RelDataType type; public final int nodeCount; /** * Cache of hash code. */ protected int hash = 0; /** * Cache of normalized variables used for #equals and #hashCode. */ private @Nullable Pair\u0026lt;SqlOperator, List\u0026lt;RexNode\u0026gt;\u0026gt; normalized; } op 操作符，加减乘除，大于小于，函数等 operands 操作数，可以是一元的也可以是多元的 type 数据类型，和SQL的是可以对应上的 nodeCount RexNode的个数，包括本身和Operands操作数 案例 # 通过一个简单的例子\n-- table(user,product,amount) select * from tableA where amount \u0026gt; 2 该Sql语句经过后生成\nLogicalProject(user=[$0], product=[$1], amount=[$2]) +- LogicalFilter(condition=[\u0026gt;($2, 2)]) +- LogicalTableScan(table=[[default_catalog, default_database, tableA]]) 就可以看出：\nselect * 对应的是 LocalProject where amount \u0026gt; 2 对应的是 LogicalFilter from tableA 对应的是 LogicalTableScan LogicalProject 有以下变量\n变量 类型 说明 案例中的取值 exprs ImmutableList\u0026lt;RexNode\u0026gt; 表达式，一般就是select 后面跟的表达式，这里的表达式已经转换过了，给不同的表达式进行了命名,user=[$0], product=[$1], amount=[$2] 这里都是RexInputRef [$0, $1, $2] hints ImmutableList\u0026lt;RelHint\u0026gt; 这里是hint 的表达式，是嵌在代码里的一串增强说明 [] input LogicalFilter 关联其他RelNode，是该对象的输入节点 LogicalFilter(condition=[\u0026gt;($2, 2)]) rowType RelRecordType 数据类型 RecordType(BIGINT user, VARCHAR(2147483647) product, INTEGER amount) digest AbstractRelNode.InnerRelDigest 关系信息的摘要，根据此判断两个关系是否相同 - cluster RelOptCluster 默认的Cluster，提供元数据，统计信息，管理查询计划的各种关系运算符，提供优化查询计划的方法等 - id int - - traitSet List\u0026lt;RelTraitSet\u0026gt; 关系的一些特征、特质，比如处理引擎的规范，Flink分布，mini-batch，下ModifyKind， UpdateKind 这些 [Convention, FlinkRelDistribution, MiniBatchIntervalTrait, ModifyKindSetTrait, UpdateKindTrait] 可以看出RelNode 其实更像是一个关系代数，这个关系代数也是有树型关系在里面的\n","date":"11 May 2023","externalUrl":null,"permalink":"/posts/code/calcite-%E7%AE%80%E4%BB%8B/","section":"Posts","summary":"","title":"Calcite 简介","type":"posts"},{"content":" sqlQuery # sql 进入sqlQuery后，首先就是获取Parser 解析sql语句\n// TableEnvironmentImpl.java @Override public Table sqlQuery(String query) { List\u0026lt;Operation\u0026gt; operations = getParser().parse(query); if (operations.size() != 1) { throw new ValidationException( \u0026#34;Unsupported SQL query! sqlQuery() only accepts a single SQL query.\u0026#34;); } Operation operation = operations.get(0); if (operation instanceof QueryOperation \u0026amp;\u0026amp; !(operation instanceof ModifyOperation)) { return createTable((QueryOperation) operation); } else { throw new ValidationException( \u0026#34;Unsupported SQL query! sqlQuery() only accepts a single SQL query of type \u0026#34; + \u0026#34;SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.\u0026#34;); } } 这里会获取StreamPlanner的Parser\n@Override public Parser getParser() { return getPlanner().getParser(); } @VisibleForTesting public Planner getPlanner() { return planner; } StreamTableEnvironment 在创建的过程中会创建 Planner\nfinal Planner planner = PlannerFactoryUtil.createPlanner( executor, tableConfig, userClassLoader, moduleManager, catalogManager, functionCatalog); 其中，\nexecutor 用于执行Planner的对象 tableConfig table和SQL的配置项，比如 checkpoint，watermark等 userClassLoader 用户动态类加载器 默认的使用org.apache.flink.util.FlinkUserCodeClassLoaders来创建的 moduleManager 模块管理器，会将CoreModule的模块加入到管理器中 module 就是 定义的一系列元数据，包括函数、规则、操作符等 Modules define a set of metadata, including functions, user defined types, operators, rules, etc. Metadata from modules are regarded as built-in or system metadata that users can take advantages of.\ncatalogManager 用于处理 catalog，封装catalog和一些临时表对象 catalog 提供了元数据信息，用来管理元数据信息(如table、view、function 和 type等)，提供了一套api，可以使用Table API和SQL来访问 This interface is responsible for reading and writing metadata such as database/table/views/UDFs from a registered catalog. It connects a registered catalog and Flink\u0026rsquo;s Table API. This interface only processes permanent metadata objects. In order to process temporary objects, a catalog can also implement the TemporaryOperationListener interface.\nfunctionCatalog 函数catalog，保存函数的定义 注册的函数就会放在这个对象中，像UDF 等注册的catalog也会放在这里 FLIP-65 Simple function catalog to store FunctionDefinitions in catalogs. Note: This class can be cleaned up a lot once we drop the methods deprecated as part of FLIP-65. In the long-term, the class should be a part of catalog manager similar to DataTypeFactory.\nPlannerFactoryUtil.createPlanner 方法会先找到 PlannerFactory（默认是 DefaultPlannerFactory）然后根据 TableConfig 中的execution.runtime-mode 确认启动的任务是流任务还是批任务，进而创建Planner(SteamPlanner)\n@Override public Planner create(Context context) { final RuntimeExecutionMode runtimeExecutionMode = context.getTableConfig().get(ExecutionOptions.RUNTIME_MODE); switch (runtimeExecutionMode) { case STREAMING: return new StreamPlanner( context.getExecutor(), context.getTableConfig(), context.getModuleManager(), context.getFunctionCatalog(), context.getCatalogManager(), context.getClassLoader()); case BATCH: return new BatchPlanner( context.getExecutor(), context.getTableConfig(), context.getModuleManager(), context.getFunctionCatalog(), context.getCatalogManager(), context.getClassLoader()); default: throw new TableException( String.format( \u0026#34;Unsupported mode \u0026#39;%s\u0026#39; for \u0026#39;%s\u0026#39;. Only an explicit BATCH or \u0026#34; + \u0026#34;STREAMING mode is supported in Table API.\u0026#34;, runtimeExecutionMode, RUNTIME_MODE.key())); } } Planner的Parser, 就是用来解析SQL语句的, Parser 目前分为两种SQL方言 flink 默认的SQL 和 Hive\n@PublicEvolving public enum SqlDialect { /** Flink\u0026#39;s default SQL behavior. */ DEFAULT, /** * SQL dialect that allows some Apache Hive specific grammar. * * \u0026lt;p\u0026gt;Note: We might never support all of the Hive grammar. See the documentation for supported * features. */ HIVE } 默认情况下 我们创建出来的的Parser(ParserImpl) 是使用Calcite进行解析的\n/** * 这里的 context 就是根据planner 的信息创建的 * parser = parserFactory.create(new DefaultCalciteContext(catalogManager, plannerContext)) * */ @Override public Parser create(Context context) { DefaultCalciteContext defaultCalciteContext = (DefaultCalciteContext) context; return new ParserImpl( defaultCalciteContext.getCatalogManager(), defaultCalciteContext.getPlannerContext()::createFlinkPlanner, defaultCalciteContext.getPlannerContext()::createCalciteParser, defaultCalciteContext.getPlannerContext().getRexFactory()); } createFlinkPlanner 的实现是FlinkPlannerImpl，在这里是共享 flink table API 和 Sql 的 plan 的 在parser中主要负责验证SQL 数据类型等操作 createCalciteParser 包装了一下 Calcite的 SqlParser\npublic FlinkPlannerImpl createFlinkPlanner() { return new FlinkPlannerImpl( createFrameworkConfig(), this::createCatalogReader, typeFactory, cluster); } public CalciteParser createCalciteParser() { return new CalciteParser(getSqlParserConfig()); } ParserImpl # 默认的ParserImpl的类定义\nclassDiagram class ParserImpl { - final CatalogManager catalogManager - final Supplier\u0026lt;FlinkPlannerImpl\u0026gt; validatorSupplier - final Supplier\u0026lt;CalciteParser\u0026gt; calciteParserSupplier - final RexFactory rexFactory - static final ExtendedParser EXTENDED_PARSER +parse(String statement) List~Operation~ } public ParserImpl( CatalogManager catalogManager, Supplier\u0026lt;FlinkPlannerImpl\u0026gt; validatorSupplier, Supplier\u0026lt;CalciteParser\u0026gt; calciteParserSupplier, RexFactory rexFactory) { this.catalogManager = catalogManager; this.validatorSupplier = validatorSupplier; this.calciteParserSupplier = calciteParserSupplier; this.rexFactory = rexFactory; } parse 的过程就是调用 CalciteParser 将 Sql 语句转换成SqlNode的过程，Calcite会调用JavaCC来解析SQL语句\n然后经过转换 把 sqlNode转换成Operation 在这里如果有一些特殊的SQL解析 会放到EXTENDED_PARSER 里面进行解析\n@Override public List\u0026lt;Operation\u0026gt; parse(String statement) { CalciteParser parser = calciteParserSupplier.get(); FlinkPlannerImpl planner = validatorSupplier.get(); Optional\u0026lt;Operation\u0026gt; command = EXTENDED_PARSER.parse(statement); if (command.isPresent()) { return Collections.singletonList(command.get()); } // parse the sql query // use parseSqlList here because we need to support statement end with \u0026#39;;\u0026#39; in sql client. SqlNodeList sqlNodeList = parser.parseSqlList(statement); List\u0026lt;SqlNode\u0026gt; parsed = sqlNodeList.getList(); Preconditions.checkArgument(parsed.size() == 1, \u0026#34;only single statement supported\u0026#34;); return Collections.singletonList( SqlNodeToOperationConversion.convert(planner, catalogManager, parsed.get(0)) .orElseThrow(() -\u0026gt; new TableException(\u0026#34;Unsupported query: \u0026#34; + statement))); } 例如， SQL语句\nselect * from tableA where amount \u0026gt; 2 经过 CalciteParser 就会 生成一个 SqlNode\n最后经过SqlNodeToOperationConversion.convert 会转换成 包含逻辑计划的operation\n目前流程为 ","date":"4 May 2023","externalUrl":null,"permalink":"/posts/code/flinksql---sql%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B/","section":"Posts","summary":"","title":"FlinkSQL - SQL解析过程","type":"posts"},{"content":" flink 参数 # 参数 说明 flink 版本 1.17 java 版本 1.8 测试SQL # select * from tableA where amount \u0026gt; 2 运行环境 # final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); final DataStream\u0026lt;Order\u0026gt; orderA = env.fromCollection( Arrays.asList( new Order(1L, \u0026#34;beer\u0026#34;, 3), new Order(1L, \u0026#34;diaper\u0026#34;, 4), new Order(3L, \u0026#34;rubber\u0026#34;, 2))); final Table tableA = tableEnv.fromDataStream(orderA); final Table result = tableEnv.sqlQuery( \u0026#34;select * from \u0026#34; + tableA + \u0026#34; where amount \u0026gt; 2\u0026#34; ); tableEnv.toDataStream(result, Row.class).print(); env.execute(); 运行结果为\n(true,+I[1, beer, 3, 2, pen, 3]) (true,+I[1, beer, 3, 2, rubber, 3]) SQL流程 # 一条SQL语句 通过Calcite 转换成 物理计划，物理计划通过代码生成计划转换成Flink Transformation 从而最终转换成 Flink 的执行图\n从代码来看\ntableEnv.sqlQuery() 将sql 语句转换成了 逻辑计划 -\u0026gt; 物理计划\nenv.execute() 生成StreamGraph 最终执行语句\nsqlQuery # sqlQuery 会把 输入的 SQL 语句转换成Operation\nOperation 就是对于表的所有操作(DML, DDL, DQL, DCL)\nCovers all sort of Table operations such as queries(DQL), modifications(DML), definitions(DDL), or control actions(DCL). This is the output of Planner.getParser() and Parser.parse(String).\n在这里，Operation就是一个PlannerQueryOperation，里面包含了RelNode的信息。Operation 会包装成Table 对象返回\npublic Table sqlQuery(String query) { /** * 这里会解析Sql语句转换成关系代数 */ List\u0026lt;Operation\u0026gt; operations = getParser().parse(query); if (operations.size() != 1) { throw new ValidationException( \u0026#34;Unsupported SQL query! sqlQuery() only accepts a single SQL query.\u0026#34;); } Operation operation = operations.get(0); if (operation instanceof QueryOperation \u0026amp;\u0026amp; !(operation instanceof ModifyOperation)) { /** * 这里将转换的Operation转换成Flink Table API可以识别的Table对象 */ return createTable((QueryOperation) operation); } else { throw new ValidationException( \u0026#34;Unsupported SQL query! sqlQuery() only accepts a single SQL query of type \u0026#34; + \u0026#34;SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.\u0026#34;); } } ","date":"30 April 2023","externalUrl":null,"permalink":"/posts/code/flinksql/","section":"Posts","summary":"","title":"FlinkSQL","type":"posts"},{"content":"","date":"28 April 2023","externalUrl":null,"permalink":"/tags/lsm/","section":"Tags","summary":"","title":"Lsm","type":"tags"},{"content":"LSM 树(Log-Structured-Merge-Tree) # 不算是树，其实是一种存储结构 利用顺序追加写来提高写性能 内存-文件读取方式会降低读性能 MemTable # 放置在内存里 最新的数据 按照Key 组织数据有序(HBase：使用跳表) WAL保证可靠性 Immutable MemTable # MemTable 达到一定大小后转化成Immutable MemTable 写操作由新的MemTable 处理， 在转存过程中不阻塞数据更新操作 SSTable(Sorted String Table) # 有序键值对集合\n在磁盘的数据结构\n为了加快读取SSTable的读取，可以通过建立key索引以及布隆过滤器来加快key的查找 LSM树会将所有的数据插入、修改、删除等操作记录保存在内存中；当此类操作达到一定的数据量后，再批量地顺序写入到磁盘当中\nLSM树的数据更新是日志式的，当一条数据更新会直接append一条更新记录完成的，目的就是为了顺序写，将Immutable MemTable flush到持久化存储，而不用修改之前的SSTable中的key\n不同的SSTable中，可能存在相同Key的记录，最新的记录是准确的（索引/Bloom来优化查找速度）\n为了去除冗余的key需要进行compactcao操作\nCompact策略 # 顺序冗余存储可能带来的问题 # 读放大 读取数据时实际读取的数据量大于真正的数据量 Eg: 先在 MemTable 查看当前Key 是否存在，不存在继续从SSTable中查找 写放大 写入数据时实际写入的数据量大于真正的数据量 Eg: 写入时可能触发Compact操作，导致实际写入的数据量远大于该key的数据量 空间放大 数据实际占用的磁盘空间比数据的真正大小更多 Eg: 对于一个key来说，只有最新的那条记录是有效的，而之前的记录都是可以被清理回收的。 Compact策略 # size-tiered 策略 # 保证每层内部的SSTable的大小相近 同时限制每一层SSTable的数量 每层限制SSTable为N，当每层SSTable达到N后，则触发Compact操作合并这些SSTable，并将合并后的结果写入到下一层成为一个更大的sstable。 当层数达到一定数量时，最底层的单个SSTable的大小会变得非常大。并且size-tiered策略会导致空间放大比较严重。即使对于同一层的SSTable，每个key的记录是可能存在多份的，只有当该层的SSTable执行compact操作才会消除这些key的冗余记录。\nleveled策略 # leveled策略也是采用分层的思想，每一层限制总文件的大小\n将每一层切分成多个大小相近的SSTable 这一层的SSTable是全局有序的，意味着一个key在每一层至多只有1条记录，不存在冗余记录 合并过程 # L1的总大小超过L1本身大小限制： 此时会从L1中选择至少一个文件，然后把它跟L2有交集的部分(非常关键)进行合并。生成的文件会放在L2: 此时L1第二SSTable的key的范围覆盖了L2中前三个SSTable，那么就需要将L1中第二个SSTable与L2中前三个SSTable执行Compact操作。 如果L2合并后的结果仍旧超出L5的阈值大小，需要重复之前的操作 —— 选至少一个文件然后把它合并到下一层 Ps: 多个不相干的合并是可以并发进行的： leveled策略相较于size-tiered策略来说，每层内key是不会重复的，即使是最坏的情况，除开最底层外，其余层都是重复key，按照相邻层大小比例为10来算，冗余占比也很小。因此空间放大问题得到缓解。但是写放大问题会更加突出。举一个最坏场景，如果LevelN层某个SSTable的key的范围跨度非常大，覆盖了LevelN+1层所有key的范围，那么进行Compact时将涉及LevelN+1层的全部数据\n","date":"28 April 2023","externalUrl":null,"permalink":"/posts/code/lsm%E6%A0%91/","section":"Posts","summary":"","title":"LSM树","type":"posts"},{"content":" 介绍 # AggHandlerCodeGenerator 的代码在 flink planner 下，用来生成聚合函数的代码,是scala 代码 类定义 # classDiagram class AggsHandlerCodeGenerator{ +CodeGeneratorContext ctx +RelBuilder relBuilder +Seq~LogicalType~ inputFieldTypes +Boolean copyInputField +RowType inputType +Seq~RexLiteral~ constants -Seq~GeneratedExpression~ constantExprs -String namespaceClassName -Seq~PlannerWindowProperty~ windowProperties -Boolean hasNamespace -RowType accTypeInfo -Int aggBufferSize -Array~DataType~ mergedAccExternalTypes -Int mergedAccOffset -Boolean mergedAccOnHeap -Array~Int~ ignoreAggValues -Boolean isAccumulateNeeded -Boolean isRetractNeeded -Boolean isMergeNeeded +RowType valueType: -Array~AggCodeGen~ aggBufferCodeGens -Array~AggCodeGen~ aggActionCodeGens +withConstants(Seq~RexLiteral~ literals ): AggsHandlerCodeGenerator } package org.apache.flink.table.planner.codegen.agg class AggsHandlerCodeGenerator( ctx: CodeGeneratorContext, // 上下文 relBuilder: RelBuilder, // 用来生成关系表达式 inputFieldTypes: Seq[LogicalType], copyInputField: Boolean // 需要缓存时将此字段设置为true) { private val inputType = RowType.of(inputFieldTypes: _*) /** 常量表达式 */ private var constants: Seq[RexLiteral] = Seq() private var constantExprs: Seq[GeneratedExpression] = Seq() /** 窗口相关参数，窗口聚合才会用到 */ private var namespaceClassName: String = _ private var windowProperties: Seq[PlannerWindowProperty] = Seq() private var hasNamespace: Boolean = false /** 聚合信息 */ private var accTypeInfo: RowType = _ private var aggBufferSize: Int = _ private var mergedAccExternalTypes: Array[DataType] = _ private var mergedAccOffset: Int = 0 private var mergedAccOnHeap: Boolean = false private var ignoreAggValues: Array[Int] = Array() private var isAccumulateNeeded = false private var isRetractNeeded = false private var isMergeNeeded = false var valueType: RowType = _ /** * 生成 [[AggsHandleFunction]] 或者 [[NamespaceAggsHandleFunction]] 会创建 [[aggBufferCodeGens]] and [[aggActionCodeGens]] 两者包含相同的AggCodeGens，aggBufferCodeGens 以列表的扁平形式， aggActionCodeGens是树形结构 在没有distinct 的的情况下，两者相同 */ /** aggBufferCodeGens 用于生成相关累加器(Accumulator)的 方法 */ private var aggBufferCodeGens: Array[AggCodeGen] = _ /** aggActionCodeGens 树形结构，聚合distinct数据的时候，会将相同需要distinct的字段组成树结构 */ private var aggActionCodeGens: Array[AggCodeGen] = _ object aggshandlercodegenerator { /** static terms **/ val acc_term = \u0026#34;acc\u0026#34; val merged_acc_term = \u0026#34;otheracc\u0026#34; val accumulate_input_term = \u0026#34;accinput\u0026#34; val retract_input_term = \u0026#34;retractinput\u0026#34; val distinct_key_term = \u0026#34;distinctkey\u0026#34; val namespace_term = \u0026#34;namespace\u0026#34; val store_term = \u0026#34;store\u0026#34; val collector: string = classname[collector[_]] val collector_term = \u0026#34;out\u0026#34; val member_collector_term = \u0026#34;convertcollector\u0026#34; val convert_collector_type_term = \u0026#34;convertcollector\u0026#34; val key_term = \u0026#34;groupkey\u0026#34; val input_not_null = false } 如果一个SQL的结构如下\ncount(*), count(distinct a), count(distinct a) filter d \u0026gt; 5, sum(a), sum(distinct a) +----------+-----------+-----------+---------+---------+----------------+ | count(*) | count(a\u0026#39;) | count(a\u0026#39;) | sum(a) | sum(a\u0026#39;) | distinct(a) a\u0026#39; | +----------+-----------+-----------+---------+---------+----------------+ 那么 aggBufferCodeGens 会这样保存\n┌ │ └ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ * ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ d ─ ─ i ─ ─ s ─ ─ t ─ ─ i ─ ─ n ─ ─ c ─ ─ t ─ ─ ( ─ ─ a ─ ─ ) ─ ┐ │ ┘ aggActionCodeGens 会这样保存\n┌ │ │ │ │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ * ─ ─ ) ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┬ │ │ │ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ) ─ ┬ │ │ │ │ ┴ ─ ─ ─ ─ ─ d ─ ─ i ─ ─ s ├ ├ └ ─ ─ t ─ ─ ─ ─ ─ i c c s ─ ─ n o o u ─ ─ c u u m ─ ─ t n n ( ─ ─ ( t t a ─ ─ a ( ( ' ─ ─ ) a a ) ─ ─ ' ' ─ ─ a ) ) ─ ─ ' ─ ─ ( ─ ─ f ─ ─ i ─ ─ l ─ ─ t ─ ─ e ─ ─ r ─ ─ ─ ─ d ─ ─ ─ ─ \u0026gt; ─ ─ ─ ─ 5 ─ ─ ) ─ ─ ─ ─ ─ ┐ │ │ │ │ ┘ CodeGeneratorContext # package org.apache.flink.table.planner.codegen /** 生成代码的上下文，维护代码段的状态 */ class CodeGeneratorContext(val tableConfig: TableConfig) { // 保存用于传递生成类的对象列表 val references: mutable.ArrayBuffer[AnyRef] = new mutable.ArrayBuffer[AnyRef]() // 插入有序，只会被添加一次， 成员状态 private val reusableMemberStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 构造状态 private val reusableInitStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中open方法的状态 private val reusableOpenStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中close方法的状态 private val reusableCloseStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 清理 dataview 的状态 private val reusableCleanupStatements = mutable.LinkedHashSet[String]() // 单个记录的状态， 插入有序，因为本地变量需要被分割，所以本地变量无法访问，只能更新成员变量 private val reusablePerRecordStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // (inputTerm, index) -\u0026gt; expr // 只会被添加一次， 初始化拆箱表达式Map val reusableInputUnboxingExprs: mutable.Map[(String, Int), GeneratedExpression] = mutable.Map[(String, Int), GeneratedExpression]() // 插入有序，只会被添加一次，构造函数的状态 private val reusableConstructorStatements: mutable.LinkedHashSet[(String, String)] = mutable.LinkedHashSet[(String, String)]() // 插入有序，只会被添加一次，类声明状态 private val reusableInnerClassDefinitionStatements: mutable.Map[String, String] = mutable.Map[String, String]() // string_constant -\u0026gt; reused_term // 常量 private val reusableStringConstants: mutable.Map[String, String] = mutable.Map[String, String]() // LogicalType -\u0026gt; reused_term // 类型序列化 private val reusableTypeSerializers: mutable.Map[LogicalType, String] = mutable.Map[LogicalType, String]() /** * Flag map that indicates whether the generated code for method is split into several methods. */ private val isCodeSplitMap = mutable.Map[String, Boolean]() // method_name -\u0026gt; local_variable_statements private val reusableLocalVariableStatements = mutable.Map[String, mutable.LinkedHashSet[String]]( (currentMethodNameForLocalVariables, mutable.LinkedHashSet[String]())) ","date":"26 September 2022","externalUrl":null,"permalink":"/posts/code/flinksql---agghandlercodegenerator/","section":"Posts","summary":"","title":"FlinkSQL - AggHandlerCodeGenerator","type":"posts"},{"content":"about me\n","externalUrl":null,"permalink":"/about/","section":"","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/en/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/en/","section":"Blowfish","summary":"","title":"Blowfish","type":"page"},{"content":"","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/en/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]