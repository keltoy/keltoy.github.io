[{"categories":null,"content":"简介 Apache Calcite是一个动态数据管理框架。 它包含构成典型数据库管理系统的许多部分，但省略了一些关键功能:数据存储、处理数据的算法和存储元数据的存储库。 我们主要用 Calcite 来解析SQL。很多项目也是直接使用了Calcite来解析SQL，优化SQL等，比如 FlinkSQL calicte的基本架构如下 ","date":"2023-05-11","objectID":"/calcite-prefix/:1:0","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"关系代数 关系模型源于数学。关系是由元组构成的集合，可以通过关系的运算来表达查询要求，而关系代数恰恰是关系操作语言的一种传统的表示方式，它是一种抽象的查询语言。 关系代数的运算对象是关系，运算结果也是关系。与一般的运算一样，运算对象、运算符和运算结果是关系代数的三大要素。 关系代数的运算可分为两大类： 传统的集合运算。这类运算完全把关系看成元组的集合。传统的集合运算包括集合的广义笛卡儿积运算、并运算、交运算和差运算。 专门的关系运算。这类运算除了把关系看成元组的集合外，还通过运算表达了查询的要求。专门的关系运算包括选择、投影、连接和除运算。 关系代数中的运算符可以分为四类：传统的集合运算符、专门的关系运算符、比较运算符和逻辑运算符。 关系运算 和 SQL的关系如下 运算符 SQL关键字 含义 分类 $$\\cap$$ 交 集合运算 $$\\cup$$ 并 集合运算 $$-$$ 差 集合运算 $$\\times$$ from A,B 广义笛卡尔积 集合运算 $$\\sigma$$ where 选择 关系运算 $$\\Pi$$ select distinct 投影 关系运算 $$\\bowtie$$ join 连接 关系运算 $$\\div$$ 除 关系运算 $$\u003e$$ 大于 比较运算 $$\u003c$$ 小于 比较运算 $$=$$ 等于 比较运算 $$\\neq$$ 不等 比较运算 $$\\leqslant$$ 小于等于 比较运算 $$\\geqslant$$ 大于等于 比较运算 $$\\neg$$ 非 逻辑运算 $$\\land$$ 与 逻辑运算 $$\\lor$$ 或 逻辑运算 SQL语句会先翻译成为关系代数后再被执行的 在执行explain 一条SQL的时候 就可以看到翻译后关系代数的命名 == Abstract Syntax Tree == LogicalProject(user=[$0], product=[$1], amount=[$2]) +- LogicalFilter(condition=[\u003e($2, 2)]) +- LogicalTableScan(table=[[*anonymous_datastream_source$1*]]) ","date":"2023-05-11","objectID":"/calcite-prefix/:2:0","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"Calcite 解析流程 Parser 将SQL语句(字符串) 解析成 AST(抽象语法树) 语法树的节点在代码中为SqlNode Validate 校验SqlNode节点，替换节点中的部分属性，设置单调性等信息 Convert 将SqlNode 抽象语法树 转换成RelNode，执行逻辑执行计划 Optimize 成本优化 Execute 生成动态代码，并执行物理执行计划 其中， SqlNode 是语法树上的节点，其本质是把SQL语句进行拆解 RelNode 是关系节点，代表的事关系代数中的关系操作，RelNode 更倾向数学的概念，就可以进行下一步的优化了 RexNode 虽然RexNode也属于关系节点，但是这里的RexNode更偏向于去表示表达式，比如一个常数，或者是 简单的a+b的运算，亦或是count(*) 这样的聚合；所以一个RelNode中会有很多RexNode节点 ","date":"2023-05-11","objectID":"/calcite-prefix/:3:0","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"SqlNode SqlNode 是一个抽象类，没有过多的信息，就包括一个位置信息的对象SqlParserPos SqlParserPos 是用来表示SQL语句中的位置信息的类。它包含了行号、列号和字符偏移量等信息，可以用于定位SQL语句中的错误或者生成更加详细的错误信息 public abstract class SqlNode implements Cloneable { //~ Static fields/initializers --------------------------------------------- public static final @Nullable SqlNode[] EMPTY_ARRAY = new SqlNode[0]; //~ Instance fields -------------------------------------------------------- protected final SqlParserPos pos; } public class SqlParserPos implements Serializable { //~ Static fields/initializers --------------------------------------------- /** * SqlParserPos representing line one, character one. Use this if the node * doesn't correspond to a position in piece of SQL text. */ public static final SqlParserPos ZERO = new SqlParserPos(0, 0); /** Same as {@link #ZERO} but always quoted. **/ public static final SqlParserPos QUOTED_ZERO = new QuotedParserPos(0, 0, 0, 0); private static final long serialVersionUID = 1L; //~ Instance fields -------------------------------------------------------- private final int lineNumber; private final int columnNumber; private final int endLineNumber; private final int endColumnNumber; } 从SqlSelect的这个类中就可以看出，一个简单的SQL语句在SqlSelect中都能找到 public class SqlSelect extends SqlCall { //~ Static fields/initializers --------------------------------------------- // constants representing operand positions public static final int FROM_OPERAND = 2; public static final int WHERE_OPERAND = 3; public static final int HAVING_OPERAND = 5; SqlNodeList keywordList; SqlNodeList selectList; @Nullable SqlNode from; @Nullable SqlNode where; @Nullable SqlNodeList groupBy; @Nullable SqlNode having; SqlNodeList windowDecls; @Nullable SqlNodeList orderBy; @Nullable SqlNode offset; @Nullable SqlNode fetch; @Nullable SqlNodeList hints; } 这里的keywordList就是像 SELECT FROM 这样的保留字 不过从debug的过程来看 这个keywordList是个空的数据 ","date":"2023-05-11","objectID":"/calcite-prefix/:3:1","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"RelNode RelNode 是个接口 继承了 RelOptNode RelOptNode 目前没有看到除了RelNode以外有其他地方使用和实现 RelNode 接口定义了树结构的父类以及节点数据和类型的方法 真正处理的方法都放在了 RelShuttle 和 RexShuttle中 public interface RelNode extends RelOptNode, Cloneable { /** * Accepts a visit from a shuttle. * * @param shuttle Shuttle * @return A copy of this node incorporating changes made by the shuttle to * this node's children */ RelNode accept(RelShuttle shuttle); /** * Accepts a visit from a shuttle. If the shuttle updates expression, then * a copy of the relation should be created. This new relation might have * a different row-type. * * @param shuttle Shuttle * @return A copy of this node incorporating changes made by the shuttle to * this node's children */ RelNode accept(RexShuttle shuttle); } 可以具体一点，看一下比较常用的RelNode 的实现是如何定义的：LogicalProject /** * Sub-class of Project not targeted at any particular engine or calling convention. */ public final class LogicalProject extends Project { //~ Constructors ----------------------------------------------------------- /** * Creates a LogicalProject. * * \u003cp\u003eUse {@link #create} unless you know what you're doing. * * @param cluster Cluster this relational expression belongs to * @param traitSet Traits of this relational expression * @param hints Hints of this relational expression * @param input Input relational expression * @param projects List of expressions for the input columns * @param rowType Output row type */ public LogicalProject( RelOptCluster cluster, RelTraitSet traitSet, List\u003cRelHint\u003e hints, RelNode input, List\u003c? extends RexNode\u003e projects, RelDataType rowType) { super(cluster, traitSet, hints, input, projects, rowType); assert traitSet.containsIfApplicable(Convention.NONE); } /** * Creates a LogicalProject by parsing serialized output. */ public LogicalProject(RelInput input) { super(input); } //~ Methods ---------------------------------------------------------------- /** Creates a LogicalProject. */ public static LogicalProject create(final RelNode input, List\u003cRelHint\u003e hints, final List\u003c? extends RexNode\u003e projects, @Nullable List\u003c? extends @Nullable String\u003e fieldNames) { final RelOptCluster cluster = input.getCluster(); final RelDataType rowType = RexUtil.createStructType(cluster.getTypeFactory(), projects, fieldNames, SqlValidatorUtil.F_SUGGESTER); return create(input, hints, projects, rowType); } /** Creates a LogicalProject, specifying row type rather than field names. */ public static LogicalProject create(final RelNode input, List\u003cRelHint\u003e hints, final List\u003c? extends RexNode\u003e projects, RelDataType rowType) { final RelOptCluster cluster = input.getCluster(); final RelMetadataQuery mq = cluster.getMetadataQuery(); final RelTraitSet traitSet = cluster.traitSet().replace(Convention.NONE) .replaceIfs(RelCollationTraitDef.INSTANCE, () -\u003e RelMdCollation.project(mq, input, projects)); return new LogicalProject(cluster, traitSet, hints, input, projects, rowType); } } LocalProject 的变量定义都散落在他的父类中 /** * Relational expression that computes a set of 'select expressions' from its input relational expression. * See Also: * org.apache.calcite.rel.logical.LogicalProject */ public abstract class Project extends SingleRel implements Hintable { //~ Instance fields -------------------------------------------------------- protected final ImmutableList\u003cRexNode\u003e exps; protected final ImmutableList\u003cRelHint\u003e hints; } /** * Abstract base class for relational expressions with a single input. * * \u003cp\u003eIt is not required that single-input relational expressions use this * class as a base class. However, default implementations of methods make life * easier. */ public abstract class SingleRel extends AbstractRelNode { //~ Instance fields -------------------------------------------------------- protected RelNode input; } ","date":"2023-05-11","objectID":"/calcite-prefix/:3:2","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"RexNode RexNode 是 行表达式(Row Expression) 是通过SqlNode 转换过来 尤其是 select 的项 where 的条件 等等 与SqlNode不同的是,RexNode的类型都是确定的，SqlNode的类型是在优化前就已经定好了，所以这个类型可能不能用 RexNode 是一个抽象类，也是实现了访问者模式 /** * Row expression. * * \u003cp\u003eEvery row-expression has a type. * (Compare with {@link org.apache.calcite.sql.SqlNode}, which is created before * validation, and therefore types may not be available.) * * \u003cp\u003eSome common row-expressions are: {@link RexLiteral} (constant value), * {@link RexVariable} (variable), {@link RexCall} (call to operator with * operands). Expressions are generally created using a {@link RexBuilder} * factory.\u003c/p\u003e * * \u003cp\u003eAll sub-classes of RexNode are immutable.\u003c/p\u003e */ public abstract class RexNode { //~ Instance fields -------------------------------------------------------- // Effectively final. Set in each sub-class constructor, and never re-set. protected @MonotonicNonNull String digest; //~ Methods ---------------------------------------------------------------- public abstract RelDataType getType(); /** * Returns whether this expression always returns true. (Such as if this * expression is equal to the literal \u003ccode\u003eTRUE\u003c/code\u003e.) */ public boolean isAlwaysTrue() { return false; } /** * Returns whether this expression always returns false. (Such as if this * expression is equal to the literal \u003ccode\u003eFALSE\u003c/code\u003e.) */ public boolean isAlwaysFalse() { return false; } public boolean isA(SqlKind kind) { return getKind() == kind; } public boolean isA(Collection\u003cSqlKind\u003e kinds) { return getKind().belongsTo(kinds); } /** * Returns the kind of node this is. * * @return Node kind, never null */ public SqlKind getKind() { return SqlKind.OTHER; } @Override public String toString() { return requireNonNull(digest, \"digest\"); } /** Returns the number of nodes in this expression. * * \u003cp\u003eLeaf nodes, such as {@link RexInputRef} or {@link RexLiteral}, have * a count of 1. Calls have a count of 1 plus the sum of their operands. * * \u003cp\u003eNode count is a measure of expression complexity that is used by some * planner rules to prevent deeply nested expressions. */ public int nodeCount() { return 1; } /** * Accepts a visitor, dispatching to the right overloaded * {@link RexVisitor#visitInputRef visitXxx} method. * * \u003cp\u003eAlso see {@link RexUtil#apply(RexVisitor, java.util.List, RexNode)}, * which applies a visitor to several expressions simultaneously. */ public abstract \u003cR\u003e R accept(RexVisitor\u003cR\u003e visitor); /** * Accepts a visitor with a payload, dispatching to the right overloaded * {@link RexBiVisitor#visitInputRef(RexInputRef, Object)} visitXxx} method. */ public abstract \u003cR, P\u003e R accept(RexBiVisitor\u003cR, P\u003e visitor, P arg); /** {@inheritDoc} * * \u003cp\u003eEvery node must implement {@link #equals} based on its content */ @Override public abstract boolean equals(@Nullable Object obj); /** {@inheritDoc} * * \u003cp\u003eEvery node must implement {@link #hashCode} consistent with * {@link #equals} */ @Override public abstract int hashCode(); } 还是具体一点看一下 RexCall的定义 RexCall是有操作符的表达式，操作符可以是 一元二元，也可是函数，或者是固定语法 /** * An expression formed by a call to an operator with zero or more expressions * as operands. * * \u003cp\u003eOperators may be binary, unary, functions, special syntactic constructs * like \u003ccode\u003eCASE ... WHEN ... END\u003c/code\u003e, or even internally generated * constructs like implicit type conversions. The syntax of the operator is * really irrelevant, because row-expressions (unlike * {@link org.apache.calcite.sql.SqlNode SQL expressions}) * do not directly represent a piece of source code. * * \u003cp\u003eIt's not often necessary to sub-class this class. The smarts should be in * the operator, rather than the call. Any extra information about the call can * often be encoded as extra arguments. (These don't need to be hidden, because * no one is going to be generating source code from this tree.)\u003c/p\u003e */ public class RexCall extends RexNode { //~ Instance fields -------------------------------------------------------- public final SqlOperator op; public final ImmutableList\u003cRexNode\u003e operands; public","date":"2023-05-11","objectID":"/calcite-prefix/:3:3","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"案例 通过一个简单的例子 -- table(user,product,amount) select * from tableA where amount \u003e 2 该Sql语句经过后生成 LogicalProject(user=[$0], product=[$1], amount=[$2]) +- LogicalFilter(condition=[\u003e($2, 2)]) +- LogicalTableScan(table=[[default_catalog, default_database, tableA]]) 就可以看出： select * 对应的是 LocalProject where amount \u003e 2 对应的是 LogicalFilter from tableA 对应的是 LogicalTableScan LogicalProject 有以下变量 变量 类型 说明 案例中的取值 exprs ImmutableList\u003cRexNode\u003e 表达式，一般就是select 后面跟的表达式，这里的表达式已经转换过了，给不同的表达式进行了命名,user=[$0], product=[$1], amount=[$2] 这里都是RexInputRef [$0, $1, $2] hints ImmutableList\u003cRelHint\u003e 这里是hint 的表达式，是嵌在代码里的一串增强说明 [] input LogicalFilter 关联其他RelNode，是该对象的输入节点 LogicalFilter(condition=[\u003e($2, 2)]) rowType RelRecordType 数据类型 RecordType(BIGINT user, VARCHAR(2147483647) product, INTEGER amount) digest AbstractRelNode.InnerRelDigest 关系信息的摘要，根据此判断两个关系是否相同 - cluster RelOptCluster 默认的Cluster，提供元数据，统计信息，管理查询计划的各种关系运算符，提供优化查询计划的方法等 - id int - - traitSet List\u003cRelTraitSet\u003e 关系的一些特征、特质，比如处理引擎的规范，Flink分布，mini-batch，下ModifyKind， UpdateKind 这些 [Convention, FlinkRelDistribution, MiniBatchIntervalTrait, ModifyKindSetTrait, UpdateKindTrait] 可以看出RelNode 其实更像是一个关系代数，这个关系代数也是有树型关系在里面的 ","date":"2023-05-11","objectID":"/calcite-prefix/:3:4","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"sqlQuery sql 进入sqlQuery后，首先就是获取Parser 解析sql语句 // TableEnvironmentImpl.java @Override public Table sqlQuery(String query) { List\u003cOperation\u003e operations = getParser().parse(query); if (operations.size() != 1) { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query.\"); } Operation operation = operations.get(0); if (operation instanceof QueryOperation \u0026\u0026 !(operation instanceof ModifyOperation)) { return createTable((QueryOperation) operation); } else { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query of type \" + \"SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.\"); } } 这里会获取StreamPlanner的Parser @Override public Parser getParser() { return getPlanner().getParser(); } @VisibleForTesting public Planner getPlanner() { return planner; } StreamTableEnvironment 在创建的过程中会创建 Planner final Planner planner = PlannerFactoryUtil.createPlanner( executor, tableConfig, userClassLoader, moduleManager, catalogManager, functionCatalog); 其中， executor 用于执行Planner的对象 tableConfig table和SQL的配置项，比如 checkpoint，watermark等 userClassLoader 用户动态类加载器 默认的使用org.apache.flink.util.FlinkUserCodeClassLoaders来创建的 moduleManager 模块管理器，会将CoreModule的模块加入到管理器中 module 就是 定义的一系列元数据，包括函数、规则、操作符等 Modules define a set of metadata, including functions, user defined types, operators, rules, etc. Metadata from modules are regarded as built-in or system metadata that users can take advantages of. catalogManager 用于处理 catalog，封装catalog和一些临时表对象 catalog 提供了元数据信息，用来管理元数据信息(如table、view、function 和 type等)，提供了一套api，可以使用Table API和SQL来访问 This interface is responsible for reading and writing metadata such as database/table/views/UDFs from a registered catalog. It connects a registered catalog and Flink’s Table API. This interface only processes permanent metadata objects. In order to process temporary objects, a catalog can also implement the TemporaryOperationListener interface. functionCatalog 函数catalog，保存函数的定义 注册的函数就会放在这个对象中，像UDF 等注册的catalog也会放在这里 FLIP-65 Simple function catalog to store FunctionDefinitions in catalogs. Note: This class can be cleaned up a lot once we drop the methods deprecated as part of FLIP-65. In the long-term, the class should be a part of catalog manager similar to DataTypeFactory. PlannerFactoryUtil.createPlanner 方法会先找到 PlannerFactory（默认是 DefaultPlannerFactory）然后根据 TableConfig 中的execution.runtime-mode 确认启动的任务是流任务还是批任务，进而创建Planner(SteamPlanner) @Override public Planner create(Context context) { final RuntimeExecutionMode runtimeExecutionMode = context.getTableConfig().get(ExecutionOptions.RUNTIME_MODE); switch (runtimeExecutionMode) { case STREAMING: return new StreamPlanner( context.getExecutor(), context.getTableConfig(), context.getModuleManager(), context.getFunctionCatalog(), context.getCatalogManager(), context.getClassLoader()); case BATCH: return new BatchPlanner( context.getExecutor(), context.getTableConfig(), context.getModuleManager(), context.getFunctionCatalog(), context.getCatalogManager(), context.getClassLoader()); default: throw new TableException( String.format( \"Unsupported mode '%s' for '%s'. Only an explicit BATCH or \" + \"STREAMING mode is supported in Table API.\", runtimeExecutionMode, RUNTIME_MODE.key())); } } Planner的Parser, 就是用来解析SQL语句的, Parser 目前分为两种SQL方言 flink 默认的SQL 和 Hive @PublicEvolving public enum SqlDialect { /** Flink's default SQL behavior. */ DEFAULT, /** * SQL dialect that allows some Apache Hive specific grammar. * * \u003cp\u003eNote: We might never support all of the Hive grammar. See the documentation for supported * features. */ HIVE } 默认情况下 我们创建出来的的Parser(ParserImpl) 是使用Calcite进行解析的 /** * 这里的 context 就是根据planner 的信息创建的 * parser = parserFactory.create(new DefaultCalciteContext(catalogManager, plannerContext)) * */ @Override public Parser create(Context context) { DefaultCalciteContext defaultCalciteContext = (DefaultCalciteContext) context; return new ParserImpl( defaultCalciteContext.getCatalogManager(), defaultCalciteContext.getPlannerCon","date":"2023-05-04","objectID":"/flink-sqlquery/:1:0","tags":["flink","sql"],"title":"Flink sqlQuery","uri":"/flink-sqlquery/"},{"categories":null,"content":"ParserImpl 默认的ParserImpl的类定义 public ParserImpl( CatalogManager catalogManager, Supplier\u003cFlinkPlannerImpl\u003e validatorSupplier, Supplier\u003cCalciteParser\u003e calciteParserSupplier, RexFactory rexFactory) { this.catalogManager = catalogManager; this.validatorSupplier = validatorSupplier; this.calciteParserSupplier = calciteParserSupplier; this.rexFactory = rexFactory; } parse 的过程就是调用 CalciteParser 将 Sql 语句转换成SqlNode的过程，Calcite会调用JavaCC来解析SQL语句 然后经过转换 把 sqlNode转换成Operation 在这里如果有一些特殊的SQL解析 会放到EXTENDED_PARSER 里面进行解析 @Override public List\u003cOperation\u003e parse(String statement) { CalciteParser parser = calciteParserSupplier.get(); FlinkPlannerImpl planner = validatorSupplier.get(); Optional\u003cOperation\u003e command = EXTENDED_PARSER.parse(statement); if (command.isPresent()) { return Collections.singletonList(command.get()); } // parse the sql query // use parseSqlList here because we need to support statement end with ';' in sql client. SqlNodeList sqlNodeList = parser.parseSqlList(statement); List\u003cSqlNode\u003e parsed = sqlNodeList.getList(); Preconditions.checkArgument(parsed.size() == 1, \"only single statement supported\"); return Collections.singletonList( SqlNodeToOperationConversion.convert(planner, catalogManager, parsed.get(0)) .orElseThrow(() -\u003e new TableException(\"Unsupported query: \" + statement))); } 例如， SQL语句 select * from tableA where amount \u003e 2 经过 CalciteParser 就会 生成一个 SqlNode 最后经过SqlNodeToOperationConversion.convert 会转换成 包含逻辑计划的operation 目前流程为 ","date":"2023-05-04","objectID":"/flink-sqlquery/:2:0","tags":["flink","sql"],"title":"Flink sqlQuery","uri":"/flink-sqlquery/"},{"categories":null,"content":"flink 参数 参数 说明 flink 版本 1.17 java 版本 1.8 ","date":"2023-04-30","objectID":"/flink-prefix/:1:0","tags":["flink","sql"],"title":"学习 FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"测试SQL select * from tableA where amount \u003e 2 ","date":"2023-04-30","objectID":"/flink-prefix/:2:0","tags":["flink","sql"],"title":"学习 FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"运行环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); final DataStream\u003cOrder\u003e orderA = env.fromCollection( Arrays.asList( new Order(1L, \"beer\", 3), new Order(1L, \"diaper\", 4), new Order(3L, \"rubber\", 2))); final Table tableA = tableEnv.fromDataStream(orderA); final Table result = tableEnv.sqlQuery( \"select * from \" + tableA + \" where amount \u003e 2\" ); tableEnv.toDataStream(result, Row.class).print(); env.execute(); 运行结果为 (true,+I[1, beer, 3, 2, pen, 3]) (true,+I[1, beer, 3, 2, rubber, 3]) ","date":"2023-04-30","objectID":"/flink-prefix/:3:0","tags":["flink","sql"],"title":"学习 FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"SQL流程 一条SQL语句 通过Calcite 转换成 物理计划，物理计划通过代码生成计划转换成Flink Transformation 从而最终转换成 Flink 的执行图 从代码来看 tableEnv.sqlQuery() 将sql 语句转换成了 逻辑计划 -\u003e 物理计划 env.execute() 生成StreamGraph 最终执行语句 ","date":"2023-04-30","objectID":"/flink-prefix/:4:0","tags":["flink","sql"],"title":"学习 FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"sqlQuery sqlQuery 会把 输入的 SQL 语句转换成Operation Operation 就是对于表的所有操作(DML, DDL, DQL, DCL) Covers all sort of Table operations such as queries(DQL), modifications(DML), definitions(DDL), or control actions(DCL). This is the output of Planner.getParser() and Parser.parse(String). 在这里，Operation就是一个PlannerQueryOperation，里面包含了RelNode的信息。Operation 会包装成Table 对象返回 public Table sqlQuery(String query) { /** * 这里会解析Sql语句转换成关系代数 */ List\u003cOperation\u003e operations = getParser().parse(query); if (operations.size() != 1) { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query.\"); } Operation operation = operations.get(0); if (operation instanceof QueryOperation \u0026\u0026 !(operation instanceof ModifyOperation)) { /** * 这里将转换的Operation转换成Flink Table API可以识别的Table对象 */ return createTable((QueryOperation) operation); } else { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query of type \" + \"SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.\"); } } ","date":"2023-04-30","objectID":"/flink-prefix/:4:1","tags":["flink","sql"],"title":"学习 FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"LSM 树(Log-Structured-Merge-Tree) 不算是树，其实是一种存储结构 利用顺序追加写来提高写性能 内存-文件读取方式会降低读性能 ","date":"2023-04-28","objectID":"/lsm/:1:0","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"MemTable 放置在内存里 最新的数据 按照Key 组织数据有序(HBase：使用跳表) WAL保证可靠性 ","date":"2023-04-28","objectID":"/lsm/:1:1","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Immutable MemTable MemTable 达到一定大小后转化成Immutable MemTable 写操作由新的MemTable 处理， 在转存过程中不阻塞数据更新操作 ","date":"2023-04-28","objectID":"/lsm/:1:2","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"SSTable(Sorted String Table) 有序键值对集合 在磁盘的数据结构 为了加快读取SSTable的读取，可以通过建立key索引以及布隆过滤器来加快key的查找 LSM树会将所有的数据插入、修改、删除等操作记录保存在内存中；当此类操作达到一定的数据量后，再批量地顺序写入到磁盘当中 LSM树的数据更新是日志式的，当一条数据更新会直接append一条更新记录完成的，目的就是为了顺序写，将Immutable MemTable flush到持久化存储，而不用修改之前的SSTable中的key 不同的SSTable中，可能存在相同Key的记录，最新的记录是准确的（索引/Bloom来优化查找速度） 为了去除冗余的key需要进行compactcao操作 ","date":"2023-04-28","objectID":"/lsm/:1:3","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Compact策略 ","date":"2023-04-28","objectID":"/lsm/:2:0","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"顺序冗余存储可能带来的问题 读放大 读取数据时实际读取的数据量大于真正的数据量 Eg: 先在 MemTable 查看当前Key 是否存在，不存在继续从SSTable中查找 写放大 写入数据时实际写入的数据量大于真正的数据量 Eg: 写入时可能触发Compact操作，导致实际写入的数据量远大于该key的数据量 空间放大 数据实际占用的磁盘空间比数据的真正大小更多 Eg: 对于一个key来说，只有最新的那条记录是有效的，而之前的记录都是可以被清理回收的。 ","date":"2023-04-28","objectID":"/lsm/:2:1","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Compact策略 size-tiered 策略 保证每层内部的SSTable的大小相近 同时限制每一层SSTable的数量 每层限制SSTable为N，当每层SSTable达到N后，则触发Compact操作合并这些SSTable，并将合并后的结果写入到下一层成为一个更大的sstable。 当层数达到一定数量时，最底层的单个SSTable的大小会变得非常大。并且size-tiered策略会导致空间放大比较严重。即使对于同一层的SSTable，每个key的记录是可能存在多份的，只有当该层的SSTable执行compact操作才会消除这些key的冗余记录。 leveled策略 leveled策略也是采用分层的思想，每一层限制总文件的大小 将每一层切分成多个大小相近的SSTable 这一层的SSTable是全局有序的，意味着一个key在每一层至多只有1条记录，不存在冗余记录 合并过程 L1的总大小超过L1本身大小限制： 此时会从L1中选择至少一个文件，然后把它跟L2有交集的部分(非常关键)进行合并。生成的文件会放在L2: 此时L1第二SSTable的key的范围覆盖了L2中前三个SSTable，那么就需要将L1中第二个SSTable与L2中前三个SSTable执行Compact操作。 如果L2合并后的结果仍旧超出L5的阈值大小，需要重复之前的操作 —— 选至少一个文件然后把它合并到下一层 Ps: 多个不相干的合并是可以并发进行的： leveled策略相较于size-tiered策略来说，每层内key是不会重复的，即使是最坏的情况，除开最底层外，其余层都是重复key，按照相邻层大小比例为10来算，冗余占比也很小。因此空间放大问题得到缓解。但是写放大问题会更加突出。举一个最坏场景，如果LevelN层某个SSTable的key的范围跨度非常大，覆盖了LevelN+1层所有key的范围，那么进行Compact时将涉及LevelN+1层的全部数据 ","date":"2023-04-28","objectID":"/lsm/:2:2","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Torch123 import torch # 行向量 x = torch.arange(12) x tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) # 张量形状 x.shape torch.Size([12]) # 元素总数 x.numel() 12 # 改变顺序 X = x.reshape(3, 4) X tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) # 全零 torch.zeros((2, 3, 4)) tensor([[[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], [[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]]) # 全1 torch.ones((2, 3, 4)) tensor([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], [[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]]) # 随机张量 torch.randn(3, 4) tensor([[ 1.2938, 0.0115, -1.1682, -0.8837], [ 0.5345, -0.3015, -1.2261, 0.0236], [ 1.4058, -1.2990, 0.5318, -0.9359]]) # 创建张量 torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) # 四则运算 x = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y # **运算符是求幂运算 (tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.])) torch.exp(x) tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03]) # 连接两个张量 X = torch.arange(12, dtype=torch.float32).reshape((3,4)) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]), tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.], [ 4., 5., 6., 7., 1., 2., 3., 4.], [ 8., 9., 10., 11., 4., 3., 2., 1.]])) # 张量判断 X == Y tensor([[False, True, False, True], [False, False, False, False], [False, False, False, False]]) X.sum() tensor(66.) a = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) a, b (tensor([[0], [1], [2]]), tensor([[0, 1]])) # 矩阵广播 a + b tensor([[0, 1], [1, 2], [2, 3]]) X,X[-1], X[1:3] (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]), tensor([ 8., 9., 10., 11.]), tensor([[ 4., 5., 6., 7.], [ 8., 9., 10., 11.]])) X[1, 2] = 9 X tensor([[ 0., 1., 2., 3.], [ 4., 5., 9., 7.], [ 8., 9., 10., 11.]]) X[0:2, :] = 12 X tensor([[12., 12., 12., 12.], [12., 12., 12., 12.], [ 8., 9., 10., 11.]]) # 使用id before = id(Y) Y = Y + X id(Y) == before False # 使用 Z[:] 和 X+= 来节省内存 Z = torch.zeros_like(Y) print('id(Z):', id(Z)) Z[:] = X + Y print('id(Z):', id(Z)) id(Z): 5145260816 id(Z): 5145260816 before = id(X) X += Y id(X) == before True # numpy 的相互转换 A = X.numpy() B = torch.tensor(A) type(A), type(B) (numpy.ndarray, torch.Tensor) # 标量 a = torch.tensor([3.5]) a, a.item(), float(a), int(a) (tensor([3.5000]), 3.5, 3.5, 3) ","date":"2023-04-27","objectID":"/touch-prefix/:0:0","tags":["ml"],"title":"Torch 预热","uri":"/touch-prefix/"},{"categories":null,"content":"介绍 AggHandlerCodeGenerator 的代码在 flink planner 下，用来生成聚合函数的代码,是scala 代码 ","date":"2022-09-26","objectID":"/flink-agghandlercodegenerator/:0:0","tags":["flink"],"title":"Flink AggHandlerCodeGenerator","uri":"/flink-agghandlercodegenerator/"},{"categories":null,"content":"类定义 package org.apache.flink.table.planner.codegen.agg class AggsHandlerCodeGenerator( ctx: CodeGeneratorContext, // 上下文 relBuilder: RelBuilder, // 用来生成关系表达式 inputFieldTypes: Seq[LogicalType], copyInputField: Boolean // 需要缓存时将此字段设置为true) { private val inputType = RowType.of(inputFieldTypes: _*) /** 常量表达式 */ private var constants: Seq[RexLiteral] = Seq() private var constantExprs: Seq[GeneratedExpression] = Seq() /** 窗口相关参数，窗口聚合才会用到 */ private var namespaceClassName: String = _ private var windowProperties: Seq[PlannerWindowProperty] = Seq() private var hasNamespace: Boolean = false /** 聚合信息 */ private var accTypeInfo: RowType = _ private var aggBufferSize: Int = _ private var mergedAccExternalTypes: Array[DataType] = _ private var mergedAccOffset: Int = 0 private var mergedAccOnHeap: Boolean = false private var ignoreAggValues: Array[Int] = Array() private var isAccumulateNeeded = false private var isRetractNeeded = false private var isMergeNeeded = false var valueType: RowType = _ /** * 生成 [[AggsHandleFunction]] 或者 [[NamespaceAggsHandleFunction]] 会创建 [[aggBufferCodeGens]] and [[aggActionCodeGens]] 两者包含相同的AggCodeGens，aggBufferCodeGens 以列表的扁平形式， aggActionCodeGens是树形结构 在没有distinct 的的情况下，两者相同 */ /** aggBufferCodeGens 用于生成相关累加器(Accumulator)的 方法 */ private var aggBufferCodeGens: Array[AggCodeGen] = _ /** aggActionCodeGens 树形结构，聚合distinct数据的时候，会将相同需要distinct的字段组成树结构 */ private var aggActionCodeGens: Array[AggCodeGen] = _ object aggshandlercodegenerator { /** static terms **/ val acc_term = \"acc\" val merged_acc_term = \"otheracc\" val accumulate_input_term = \"accinput\" val retract_input_term = \"retractinput\" val distinct_key_term = \"distinctkey\" val namespace_term = \"namespace\" val store_term = \"store\" val collector: string = classname[collector[_]] val collector_term = \"out\" val member_collector_term = \"convertcollector\" val convert_collector_type_term = \"convertcollector\" val key_term = \"groupkey\" val input_not_null = false } 如果一个SQL的结构如下 count(*), count(distinct a), count(distinct a) filter d \u003e 5, sum(a), sum(distinct a) +----------+-----------+-----------+---------+---------+----------------+ | count(*) | count(a') | count(a') | sum(a) | sum(a') | distinct(a) a' | +----------+-----------+-----------+---------+---------+----------------+ 那么 aggBufferCodeGens 会这样保存 ┌ │ └ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ * ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ d ─ ─ i ─ ─ s ─ ─ t ─ ─ i ─ ─ n ─ ─ c ─ ─ t ─ ─ ( ─ ─ a ─ ─ ) ─ ┐ │ ┘ aggActionCodeGens 会这样保存 ┌ │ │ │ │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ * ─ ─ ) ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┬ │ │ │ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ) ─ ┬ │ │ │ │ ┴ ─ ─ ─ ─ ─ d ─ ─ i ─ ─ s ├ ├ └ ─ ─ t ─ ─ ─ ─ ─ i c c s ─ ─ n o o u ─ ─ c u u m ─ ─ t n n ( ─ ─ ( t t a ─ ─ a ( ( ' ─ ─ ) a a ) ─ ─ ' ' ─ ─ a ) ) ─ ─ ' ─ ─ ( ─ ─ f ─ ─ i ─ ─ l ─ ─ t ─ ─ e ─ ─ r ─ ─ ─ ─ d ─ ─ ─ ─ \u003e ─ ─ ─ ─ 5 ─ ─ ) ─ ─ ─ ─ ─ ┐ │ │ │ │ ┘ ","date":"2022-09-26","objectID":"/flink-agghandlercodegenerator/:1:0","tags":["flink"],"title":"Flink AggHandlerCodeGenerator","uri":"/flink-agghandlercodegenerator/"},{"categories":null,"content":"CodeGeneratorContext package org.apache.flink.table.planner.codegen /** 生成代码的上下文，维护代码段的状态 */ class CodeGeneratorContext(val tableConfig: TableConfig) { // 保存用于传递生成类的对象列表 val references: mutable.ArrayBuffer[AnyRef] = new mutable.ArrayBuffer[AnyRef]() // 插入有序，只会被添加一次， 成员状态 private val reusableMemberStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 构造状态 private val reusableInitStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中open方法的状态 private val reusableOpenStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中close方法的状态 private val reusableCloseStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 清理 dataview 的状态 private val reusableCleanupStatements = mutable.LinkedHashSet[String]() // 单个记录的状态， 插入有序，因为本地变量需要被分割，所以本地变量无法访问，只能更新成员变量 private val reusablePerRecordStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // (inputTerm, index) -\u003e expr // 只会被添加一次， 初始化拆箱表达式Map val reusableInputUnboxingExprs: mutable.Map[(String, Int), GeneratedExpression] = mutable.Map[(String, Int), GeneratedExpression]() // 插入有序，只会被添加一次，构造函数的状态 private val reusableConstructorStatements: mutable.LinkedHashSet[(String, String)] = mutable.LinkedHashSet[(String, String)]() // 插入有序，只会被添加一次，类声明状态 private val reusableInnerClassDefinitionStatements: mutable.Map[String, String] = mutable.Map[String, String]() // string_constant -\u003e reused_term // 常量 private val reusableStringConstants: mutable.Map[String, String] = mutable.Map[String, String]() // LogicalType -\u003e reused_term // 类型序列化 private val reusableTypeSerializers: mutable.Map[LogicalType, String] = mutable.Map[LogicalType, String]() /** * Flag map that indicates whether the generated code for method is split into several methods. */ private val isCodeSplitMap = mutable.Map[String, Boolean]() // method_name -\u003e local_variable_statements private val reusableLocalVariableStatements = mutable.Map[String, mutable.LinkedHashSet[String]]( (currentMethodNameForLocalVariables, mutable.LinkedHashSet[String]())) ","date":"2022-09-26","objectID":"/flink-agghandlercodegenerator/:1:1","tags":["flink"],"title":"Flink AggHandlerCodeGenerator","uri":"/flink-agghandlercodegenerator/"},{"categories":null,"content":"Toxi Alisa ","date":"2022-09-02","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]