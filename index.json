[{"categories":null,"content":"LSM 树(Log-Structured-Merge-Tree) 不算是树，其实是一种存储结构 利用顺序追加写来提高写性能 内存-文件读取方式会降低读性能 ","date":"2023-04-28","objectID":"/lsm/:1:0","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"MemTable 放置在内存里 最新的数据 按照Key 组织数据有序(HBase：使用跳表) WAL保证可靠性 ","date":"2023-04-28","objectID":"/lsm/:1:1","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Immutable MemTable MemTable 达到一定大小后转化成Immutable MemTable 写操作由新的MemTable 处理， 在转存过程中不阻塞数据更新操作 ","date":"2023-04-28","objectID":"/lsm/:1:2","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"SSTable(Sorted String Table) 有序键值对集合 在磁盘的数据结构 为了加快读取SSTable的读取，可以通过建立key索引以及布隆过滤器来加快key的查找 LSM树会将所有的数据插入、修改、删除等操作记录保存在内存中；当此类操作达到一定的数据量后，再批量地顺序写入到磁盘当中 LSM树的数据更新是日志式的，当一条数据更新会直接append一条更新记录完成的，目的就是为了顺序写，将Immutable MemTable flush到持久化存储，而不用修改之前的SSTable中的key 不同的SSTable中，可能存在相同Key的记录，最新的记录是准确的（索引/Bloom来优化查找速度） 为了去除冗余的key需要进行compactcao操作 ","date":"2023-04-28","objectID":"/lsm/:1:3","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Compact策略 ","date":"2023-04-28","objectID":"/lsm/:2:0","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"顺序冗余存储可能带来的问题 读放大 读取数据时实际读取的数据量大于真正的数据量 Eg: 先在 MemTable 查看当前Key 是否存在，不存在继续从SSTable中查找 写放大 写入数据时实际写入的数据量大于真正的数据量 Eg: 写入时可能触发Compact操作，导致实际写入的数据量远大于该key的数据量 空间放大 数据实际占用的磁盘空间比数据的真正大小更多 Eg: 对于一个key来说，只有最新的那条记录是有效的，而之前的记录都是可以被清理回收的。 ","date":"2023-04-28","objectID":"/lsm/:2:1","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Compact策略 size-tiered 策略 保证每层内部的SSTable的大小相近 同时限制每一层SSTable的数量 每层限制SSTable为N，当每层SSTable达到N后，则触发Compact操作合并这些SSTable，并将合并后的结果写入到下一层成为一个更大的sstable。 当层数达到一定数量时，最底层的单个SSTable的大小会变得非常大。并且size-tiered策略会导致空间放大比较严重。即使对于同一层的SSTable，每个key的记录是可能存在多份的，只有当该层的SSTable执行compact操作才会消除这些key的冗余记录。 leveled策略 leveled策略也是采用分层的思想，每一层限制总文件的大小 将每一层切分成多个大小相近的SSTable 这一层的SSTable是全局有序的，意味着一个key在每一层至多只有1条记录，不存在冗余记录 合并过程 L1的总大小超过L1本身大小限制： 此时会从L1中选择至少一个文件，然后把它跟L2有交集的部分(非常关键)进行合并。生成的文件会放在L2: 此时L1第二SSTable的key的范围覆盖了L2中前三个SSTable，那么就需要将L1中第二个SSTable与L2中前三个SSTable执行Compact操作。 如果L2合并后的结果仍旧超出L5的阈值大小，需要重复之前的操作 —— 选至少一个文件然后把它合并到下一层 Ps: 多个不相干的合并是可以并发进行的： leveled策略相较于size-tiered策略来说，每层内key是不会重复的，即使是最坏的情况，除开最底层外，其余层都是重复key，按照相邻层大小比例为10来算，冗余占比也很小。因此空间放大问题得到缓解。但是写放大问题会更加突出。举一个最坏场景，如果LevelN层某个SSTable的key的范围跨度非常大，覆盖了LevelN+1层所有key的范围，那么进行Compact时将涉及LevelN+1层的全部数据 ","date":"2023-04-28","objectID":"/lsm/:2:2","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":" import torch # 行向量 x = torch.arange(12) x tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) # 张量形状 x.shape torch.Size([12]) # 元素总数 x.numel() 12 # 改变顺序 X = x.reshape(3, 4) X tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) # 全零 torch.zeros((2, 3, 4)) tensor([[[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], [[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]]) # 全1 torch.ones((2, 3, 4)) tensor([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], [[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]]) # 随机张量 torch.randn(3, 4) tensor([[ 1.2938, 0.0115, -1.1682, -0.8837], [ 0.5345, -0.3015, -1.2261, 0.0236], [ 1.4058, -1.2990, 0.5318, -0.9359]]) # 创建张量 torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) # 四则运算 x = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y # **运算符是求幂运算 (tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.])) torch.exp(x) tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03]) # 连接两个张量 X = torch.arange(12, dtype=torch.float32).reshape((3,4)) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]), tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.], [ 4., 5., 6., 7., 1., 2., 3., 4.], [ 8., 9., 10., 11., 4., 3., 2., 1.]])) # 张量判断 X == Y tensor([[False, True, False, True], [False, False, False, False], [False, False, False, False]]) X.sum() tensor(66.) a = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) a, b (tensor([[0], [1], [2]]), tensor([[0, 1]])) # 矩阵广播 a + b tensor([[0, 1], [1, 2], [2, 3]]) X,X[-1], X[1:3] (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]), tensor([ 8., 9., 10., 11.]), tensor([[ 4., 5., 6., 7.], [ 8., 9., 10., 11.]])) X[1, 2] = 9 X tensor([[ 0., 1., 2., 3.], [ 4., 5., 9., 7.], [ 8., 9., 10., 11.]]) X[0:2, :] = 12 X tensor([[12., 12., 12., 12.], [12., 12., 12., 12.], [ 8., 9., 10., 11.]]) # 使用id before = id(Y) Y = Y + X id(Y) == before False # 使用 Z[:] 和 X+= 来节省内存 Z = torch.zeros_like(Y) print('id(Z):', id(Z)) Z[:] = X + Y print('id(Z):', id(Z)) id(Z): 5145260816 id(Z): 5145260816 before = id(X) X += Y id(X) == before True # numpy 的相互转换 A = X.numpy() B = torch.tensor(A) type(A), type(B) (numpy.ndarray, torch.Tensor) # 标量 a = torch.tensor([3.5]) a, a.item(), float(a), int(a) (tensor([3.5000]), 3.5, 3.5, 3) ","date":"2023-04-27","objectID":"/deep-learning-prefix/:0:0","tags":["ml"],"title":"Torch 预热","uri":"/deep-learning-prefix/"},{"categories":null,"content":"介绍 AggHandlerCodeGenerator 的代码在 flink planner 下，用来生成聚合函数的代码,是scala 代码 ","date":"2022-09-26","objectID":"/flink-agghandlercodegenerator/:0:0","tags":["flink"],"title":"Flink AggHandlerCodeGenerator","uri":"/flink-agghandlercodegenerator/"},{"categories":null,"content":"类定义 classDiagram class AggsHandlerCodeGenerator{ +CodeGeneratorContext ctx +RelBuilder relBuilder +Seq~LogicalType~ inputFieldTypes +Boolean copyInputField +RowType inputType +Seq~RexLiteral~ constants -Seq~GeneratedExpression~ constantExprs -String namespaceClassName -Seq~PlannerWindowProperty~ windowProperties -Boolean hasNamespace -RowType accTypeInfo -Int aggBufferSize -Array~DataType~ mergedAccExternalTypes -Int mergedAccOffset -Boolean mergedAccOnHeap -Array~Int~ ignoreAggValues -Boolean isAccumulateNeeded -Boolean isRetractNeeded -Boolean isMergeNeeded +RowType valueType: -Array~AggCodeGen~ aggBufferCodeGens -Array~AggCodeGen~ aggActionCodeGens +AggsHandlerCodeGenerator withConstants(Seq~RexLiteral~ literals ): } package org.apache.flink.table.planner.codegen.agg class AggsHandlerCodeGenerator( ctx: CodeGeneratorContext, // 上下文 relBuilder: RelBuilder, // 用来生成关系表达式 inputFieldTypes: Seq[LogicalType], copyInputField: Boolean // 需要缓存时将此字段设置为true) { private val inputType = RowType.of(inputFieldTypes: _*) /** 常量表达式 */ private var constants: Seq[RexLiteral] = Seq() private var constantExprs: Seq[GeneratedExpression] = Seq() /** 窗口相关参数，窗口聚合才会用到 */ private var namespaceClassName: String = _ private var windowProperties: Seq[PlannerWindowProperty] = Seq() private var hasNamespace: Boolean = false /** 聚合信息 */ private var accTypeInfo: RowType = _ private var aggBufferSize: Int = _ private var mergedAccExternalTypes: Array[DataType] = _ private var mergedAccOffset: Int = 0 private var mergedAccOnHeap: Boolean = false private var ignoreAggValues: Array[Int] = Array() private var isAccumulateNeeded = false private var isRetractNeeded = false private var isMergeNeeded = false var valueType: RowType = _ /** * 生成 [[AggsHandleFunction]] 或者 [[NamespaceAggsHandleFunction]] 会创建 [[aggBufferCodeGens]] and [[aggActionCodeGens]] 两者包含相同的AggCodeGens，aggBufferCodeGens 以列表的扁平形式， aggActionCodeGens是树形结构 在没有distinct 的的情况下，两者相同 */ /** aggBufferCodeGens 用于生成相关累加器(Accumulator)的 方法 */ private var aggBufferCodeGens: Array[AggCodeGen] = _ /** aggActionCodeGens 树形结构，聚合distinct数据的时候，会将相同需要distinct的字段组成树结构 */ private var aggActionCodeGens: Array[AggCodeGen] = _ object aggshandlercodegenerator { /** static terms **/ val acc_term = \"acc\" val merged_acc_term = \"otheracc\" val accumulate_input_term = \"accinput\" val retract_input_term = \"retractinput\" val distinct_key_term = \"distinctkey\" val namespace_term = \"namespace\" val store_term = \"store\" val collector: string = classname[collector[_]] val collector_term = \"out\" val member_collector_term = \"convertcollector\" val convert_collector_type_term = \"convertcollector\" val key_term = \"groupkey\" val input_not_null = false } 如果一个SQL的结构如下 count(*),count(distincta),count(distincta)filterd\u003e5,sum(a),sum(distincta)+----------+-----------+-----------+---------+---------+----------------+ |count(*)|count(a') | count(a')|sum(a)|sum(a') | distinct(a) a'|+----------+-----------+-----------+---------+---------+----------------+ 那么 aggBufferCodeGens 会这样保存 ┌────────┬──────────┬──────────┬───────┬────────┬────────────┐ │count(*)│ count(a')│ count(a')│ sum(a)│ sum(a')│ distinct(a)│ └────────┴──────────┴──────────┴───────┴────────┴────────────┘ aggActionCodeGens 会这样保存 ┌─────────────────────────────┬───────┬────────────────────────────────┐ │ count(*) │ sum(a)│ distinct(a) a' │ │ │ │ ├─count(a') │ │ │ │ ├─count(a') (filter d \u003e 5) │ │ │ │ └─sum(a') │ └─────────────────────────────┴───────┴────────────────────────────────┘ ","date":"2022-09-26","objectID":"/flink-agghandlercodegenerator/:1:0","tags":["flink"],"title":"Flink AggHandlerCodeGenerator","uri":"/flink-agghandlercodegenerator/"},{"categories":null,"content":"CodeGeneratorContext package org.apache.flink.table.planner.codegen /** 生成代码的上下文，维护代码段的状态 */ class CodeGeneratorContext(val tableConfig: TableConfig) { // 保存用于传递生成类的对象列表 val references: mutable.ArrayBuffer[AnyRef] = new mutable.ArrayBuffer[AnyRef]() // 插入有序，只会被添加一次， 成员状态 private val reusableMemberStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 构造状态 private val reusableInitStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中open方法的状态 private val reusableOpenStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中close方法的状态 private val reusableCloseStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 清理 dataview 的状态 private val reusableCleanupStatements = mutable.LinkedHashSet[String]() // 单个记录的状态， 插入有序，因为本地变量需要被分割，所以本地变量无法访问，只能更新成员变量 private val reusablePerRecordStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // (inputTerm, index) -\u003e expr // 只会被添加一次， 初始化拆箱表达式Map val reusableInputUnboxingExprs: mutable.Map[(String, Int), GeneratedExpression] = mutable.Map[(String, Int), GeneratedExpression]() // 插入有序，只会被添加一次，构造函数的状态 private val reusableConstructorStatements: mutable.LinkedHashSet[(String, String)] = mutable.LinkedHashSet[(String, String)]() // 插入有序，只会被添加一次，类声明状态 private val reusableInnerClassDefinitionStatements: mutable.Map[String, String] = mutable.Map[String, String]() // string_constant -\u003e reused_term // 常量 private val reusableStringConstants: mutable.Map[String, String] = mutable.Map[String, String]() // LogicalType -\u003e reused_term // 类型序列化 private val reusableTypeSerializers: mutable.Map[LogicalType, String] = mutable.Map[LogicalType, String]() /** * Flag map that indicates whether the generated code for method is split into several methods. */ private val isCodeSplitMap = mutable.Map[String, Boolean]() // method_name -\u003e local_variable_statements private val reusableLocalVariableStatements = mutable.Map[String, mutable.LinkedHashSet[String]]( (currentMethodNameForLocalVariables, mutable.LinkedHashSet[String]())) ","date":"2022-09-26","objectID":"/flink-agghandlercodegenerator/:1:1","tags":["flink"],"title":"Flink AggHandlerCodeGenerator","uri":"/flink-agghandlercodegenerator/"},{"categories":null,"content":"Toxi Alisa ","date":"2022-09-02","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]