[{"categories":null,"content":" 好好享受剩余人生，去探索北京这座城市，身体健康，生活幸福比什么都重要 ","date":"2023-10-07","objectID":"/life-in-beijing/:0:0","tags":["美食","生活"],"title":"吃喝玩乐在北京","uri":"/life-in-beijing/"},{"categories":null,"content":"吃喝篇 名称 详细地址 已经去过 人均价格 菜系风格 连锁 临近地铁站 小楼饭店 北京市通州区南大街12号(华联商场南) 否 85 北京菜 否 通州北关,北运河西,运河商务中心,通运门 曲园酒楼 北京市西城区西四南大街109号 否 98 湘菜 否 西单,北海北,灵境胡同,平安里,车公庄,西四,太平桥,复兴门,阜成门 一楼一饭店 北京市朝阳区工体北路4号机电院70号楼一楼102号(秋果酒店斜对面) 否 143 粤菜 是 东四十条,呼家楼,金台路,东大桥,朝阳门,工人体育场,金台夕照,团结湖,农业展览馆,朝阳公园,中关村,苏州桥,知春里,魏公村,人民大学,知春路,海淀黄庄,苏州街 鸡坤茶室 北京市朝阳区工人体育场北路4号64号楼 否 84 东南亚菜 是 团结湖,金台夕照 八条一号 北京市西城区西四北八条1号 否 98 云南菜 否 平安里 东四民芳 北京市东城区朝阳门内大街79号-2(朝阳门环岛西侧100米北) 否 96 北京菜 是 朝阳门,东四,团结湖,崇文门 费大厨 北京市西城区西单北大街131号西单大悦城F7 否 95 湘菜 是 东直门,东湖渠,西单,青年路,西直门,大望路,公主坟,五棵松 火烧云 北京市朝阳区东三环北路38号院好世界商场负一层B-16号 否 109 云南菜 是 亮马桥,双井 松鹤楼 北京市东城区北三环东路36号2号楼C栋二层01单元 否 261 苏浙菜 否 和平西桥 马凯餐厅 北京市西城区地安门外大街29号什刹海地铁站A2出口北行20米 否 106 湘菜 是 长椿街,什刹海 傅记酱肉 北京市海淀区西三环北路25号 否 72 小吃 否 万寿寺 晋阳饭庄 北京市西城区白广路大街10-4号 否 112 山西菜 是 虎坊桥,珠市口,五棵松,清源路 清水亭 北京市朝阳区建国门外大街1号国贸商城南区F4 否 169 湖北菜 是 大屯路东,国贸 玉华台 北京市西城区裕民路23号 否 106 淮扬菜 是 健德门,望京西 萃华楼 北京市东城区东直门外大街48号银座商厦2层 否 149 烤鸭 是 崇文门,东直门,王府井,公主坟,常营 寂川 北京市朝阳区西大望路甲2号1层101室 否 239 川菜 否 大望路,金台路 新新湘菜馆 北京市东城区东直门街道紫铭大厦 否 79 湘菜 否 东直门,东四十条 丰泽园 北京市东城区新中街甲2号华龙大厦一层 否 203 鲁菜 是 东直门,安立路,王府井,南礼士路 淮扬春 北京市西城区三里河东路10号3层 否 121 淮扬菜 是 木樨地,动物园 京八大楼鲁花轩 北京市朝阳区新东路新源西里东街小区2号楼底商 否 123 北京菜 是 亮马桥 京八大楼京花轩 北京市朝阳区新东路新源西里东街小区2号楼底商 否 311 北京菜 是 大望路 京八大楼楚花轩 北京市大兴区兴丰大街185星城商厦1楼6层 否 63 北京菜 是 黄村西大街 一坐一忘 北京市朝阳区建国路87号北京SKPF6 否 138 云南菜 是 亮马桥,大屯路东,大望路 柳泉居 北京市西城区南大街178号 否 144 鲁菜 否 平安里 三义和酒楼 北京市朝阳区林萃东路1号国奥村-东区 否 108 鲁菜 是 广渠门内,森林公园南门 蟹老宋 北京市丰台区南三环东路8号 否 80 小吃 是 光熙门,刘家窑,通州北苑 滇大池 北京市海淀区清河中街68号清河万象汇F6 否 161 云南菜 是 太阳宫,来广营,龙泽 惠丰堂 北京市海淀区万泉庄路2楼 否 98 鲁菜 是 朝阳门,苏州街,公主坟,魏公村,西北旺 森隆饭庄 北京市东城区崇文门外大街3-11号新世界女子百货F4 否 126 苏浙菜 否 崇文门 奥华餐厅 北京市东城区台基厂大街18-3号 否 58 天津菜 否 崇文门,王府井 三样菜 去过 川菜 是 东直门,海淀五路居,西钓鱼台 三样菜玺川府 朝阳区景祥街瑞赛商务楼B1-F2 否 163 川菜 是 国贸 燕郊烧鸽子 北京市朝阳区东四环中路16号 否 120 东北菜 是 大郊亭,欢乐谷景区,南楼梓庄,六里桥,七里庄 老根山庄 北京市东城区小江胡同3436号 否 93 东北菜 是 珠市口,前门 鼎泰丰 北京市东城区王府井大街138号apmF6 否 155 小笼包 是 东直门,东大桥,呼家楼,国贸,大望路,王府井,团结湖,西单,太平桥,海淀黄庄,人民大学 聚德楼 北京市朝阳区东三环南路52号 否 212 北京菜 否 劲松 功德林 北京市东城区前门东大街甲2号 否 100 素菜 否 崇文门,王府井 三色莲花印度餐厅 北京市光华路22号光华路soho一期2层210 否 138 印度菜 是 金台夕照,木樨地 三色莲花东南亚餐厅 冬至门外大街28号-5爱尔兰酒吧2层 否 106 东南亚菜 是 东大桥，金台夕照,朝阳门 四季民福 北京市东城区东四十条23号 否 165 烤鸭 是 和平里北街,望京,呼家楼,安德里北街,安定门,雍和宫,鼓楼大街 鸿宾楼 北京市朝阳区朝外南街20号联合大厦2层203号 否 138 北京菜 是 朝阳门,车公庄西,车公庄 淮扬府 北京市东城区安定门外大街198号 否 284 淮扬菜 是 安定门,雍和宫,什刹海 淮扬府游园惊梦 北苑路98号院1号楼101内A6006 否 198 淮扬菜 是 大屯路东,北京站,建国门 北京饭店 北京市东城区东长安街33号(近王府井大街) 否 877 北京菜 否 王府井 同和居 朝阳区日坛北路6号日坛公园北门东西两侧1层1F7号地下一层B17号 否 115 鲁菜 是 朝阳公园,什刹海,朝阳门,东大桥,金台夕照,玉渊潭东门,木樨地,角门东 北平楼 北京市昌平区建材城西路87号西三旗桥东北平楼3层 否 118 北京菜 是 惠新西街南口,芍药居,牡丹园,龙泽,立水桥,五棵松,朱辛庄 北京厨房 北京市朝阳区建国路87号北京SKPF6 否 300 粤菜 是 大屯路东,大望路 苏帮袁 北京市东城区东直门南大街1号北京来福士中心F5 否 165 淮扬菜 是 东直门,将台,青年路,大望路,常营,后沙峪,西红门 喜四方 北京市朝阳区朝阳路40号楼 否 68 小吃 否 十里堡 翠园 北京市东城区王府井大街269号王府中环F4 否 346 粤菜 是 王府井,国贸 铸钟褡裢火烧 北京市西城区鼓楼西大街小区鼓楼西大街55号 否 46 小吃 是 什刹海,万寿寺 普拉那 北京市朝阳区亮马桥路52号燕莎友谊商城F1 否 322 德国菜 是 亮马桥 京东一绝 北京市通州区云景东路339号7号1层1-116号(云景里南区南门对面) 否 35 小吃 否 梨园 烤肉宛 北京市西城区南礼士路三条小区南礼士路58号 否 119 北京菜 是 北新桥,雍和宫,什刹海,南礼士路,苏州街, 新川面馆 北京市西城区新街口南大街14号 否 32 小吃 是 新街口,积水潭,平安里,潘家园 局气 北京市西城区北大街74号 否 128 北京菜 是 东大桥,朝阳门,呼家楼,惠新西街北口,安贞门 满恒记 北京市西城区平安里西大街14号 否 104 涮肉 否 平安里 延吉冷面 北京市西城区西四北大街181号 否 53 朝鲜菜 是 西四,平安里 砂锅居 北京市西城区西四南大街60号 否 106 北京菜 是 西四,灵境胡同,六里桥 南门涮肉 北京市西城区南官房胡同1号 否 97 涮肉 是 蒲黄榆,景泰,什刹海,慈寿寺,高碑店 锦芳小吃 北京市东城区鲜鱼口街85号 否 28 小吃 是 前门,磁器口 华威肉饼 北京市朝阳区松榆里37号16号楼 否 68 小吃 是 惠新西街北口,北新桥,雍和宫 门钉李 北京市东城区胜古中路1号蓝宝商务大厦一层北侧 否 55 小吃 否 大钟寺,魏公村 牛排家 北京市朝阳区安定路5号院6号楼中海环宇荟购物中心F4 否 310 牛排 是 建国门,王府井,安贞门,团结湖,丽泽商务区,枣营 小大董 北京市朝阳区曙光西里甲5号院凤凰汇购物中心F3 否 142 烤鸭 是 五棵松,王府井,丽泽商务区,西单,北京西站,青年路,东大桥,团结湖,三元桥 金生隆 北京市西城区六铺炕一巷65号-8 否 159 小吃 是 安德里北街,鼓楼大街 北新桥卤煮 北京市东城区东四北大街141号 否 51 小吃 是 北新桥,六里桥 柴氏风味斋 北京市海淀区三里河路中七区21号楼 否 63 小吃 否 甘家口,白堆子,望京,团结湖 大董 平安里西大街30号远通维景国际大酒店5楼 否 714 烤鸭 是 西钓鱼台,知春里,灯市口,海户屯,大屯路东,青年路,王府井,车公庄,望京南,荣昌东街 玉流馆 北京市朝阳区湖光中街8号 否 124 朝鲜菜 否 望京西 宜宾招待所 去过 川菜 否 宣武门,和平门 耙耳朵 北京市朝阳区广泰东路1号14号楼一层L107、负一层B 否 112 川菜 否 百子湾 泰丰楼 北京市西城区西大街2号 否 171 鲁菜 否 前门 巴州金丝特 北京市海淀区四道口路2号 否 97 新疆菜 否 大钟寺 狼大爷 北京市通州区潞苑北大街宋庄保民健身 否 170 新疆菜 否 东夏园 贾国龙中国堡 北京市西城区复兴门外大街15号北京长安商场F1 否 35 小吃 是 南礼士路,大屯路东,北苑,三元桥,磁器口,霍营,东直门,首经贸,海淀黄庄,富丰桥,公益西桥,五棵松,上地,天坛东门 潇湘阁 北京市海淀区西直门北大街32号枫蓝国际购物中心F2 否 84 湘菜 是 人民大学,国贸,望京,团结湖,大望路,东大桥,中关村,大钟寺,西直门,十里河,北苑,新宫,丰台科技园 君琴花 北京市东城区后街88-1号 否 136 贵州菜 否 中国美术馆,东四 红实家常菜 北京市朝阳区静安东街静安里16号楼 否 70 小吃 否 柳芳 悠航啤酒 东城区东","date":"2023-10-07","objectID":"/life-in-beijing/:1:0","tags":["美食","生活"],"title":"吃喝玩乐在北京","uri":"/life-in-beijing/"},{"categories":null,"content":" 稍不注意，就走过了 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:0:0","tags":["美食","生活","小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":null,"content":"记录 时间 地址 人数 排队等待 花销 2023年10月05日13:10:00 北京市西城区大石桥胡同甲37号 2 10分钟 252 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:1:0","tags":["美食","生活","小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":null,"content":"一句话 哥们几个来吃一顿还是挺舒服的，吃好吃饱 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:2:0","tags":["美食","生活","小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":null,"content":"点餐 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:3:0","tags":["美食","生活","小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":null,"content":"火锅 爆炸锅：牛肉很烂，很香，牛肚、牛板筋等没有异味，都很有嚼劲，豆腐也很香，回味无穷 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:3:1","tags":["美食","生活","小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":null,"content":"配菜 配菜小料：酱料很香，配上牛肉真是一绝；萝卜很脆，但是没有辣味，很好吃；蔬菜感觉有点少，可能是因为肉太多了 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:3:2","tags":["美食","生活","小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":null,"content":"总结 和哥们几个一起来搓一顿还是不错的，物美价廉，平时也可以去，补充一下蛋白质。我们吃的太饱了，中午吃的，导致下午都不咋想吃东西 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:4:0","tags":["美食","生活","小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":null,"content":" 门脸不咋地，不过进去之后别有洞天 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:0:0","tags":["美食","生活","川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":null,"content":"记录 时间 地址 人数 排队等待 花销 2023年08月26日12:51:00 北京市西城区西中胡同28号 2 3个小时+， 排队不过号，可拼桌 275 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:1:0","tags":["美食","生活","川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":null,"content":"一句话 点菜没做好攻略，导致吃起来感觉太腻，不过第一口还是很惊艳的 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:2:0","tags":["美食","生活","川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":null,"content":"点餐 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:0","tags":["美食","生活","川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":null,"content":"凉菜 李庄白肉：调制的辣酱很好吃，还特意打包带回去了，肉因人而异，我觉得腻，对面美丽的女士并不觉得 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:1","tags":["美食","生活","川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":null,"content":"热菜 川香辣子鸡：第一口真的好香，还有点烫嘴，花生米没有那么脆 酒都肥肠：肥肠是酥脆的，没有脏器味道，不过和辣子鸡实在太同类的，导致没有感觉到特别惊艳 粉蒸牛肉：牛肉是嫩的，吃起来很软糯，很久没有吃到这么嫩的牛肉了，也因人而异，对面美丽的女士并不太喜欢 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:2","tags":["美食","生活","川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":null,"content":"主食 宜宾燃面：看不到辣椒，不过依然感觉很辣，不糊嘴，很好吃 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:3","tags":["美食","生活","川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":null,"content":"甜品 醉三江：不甜，要是按照米酒的方式去喝，就没有那种意思 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:4","tags":["美食","生活","川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":null,"content":"总结 这次点餐比较失败，没有蔬菜，吃起来就会感觉很腻，2个人其实没有吃完，下次去的话，会尝尝毛血旺，不过人是真的多… ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:4:0","tags":["美食","生活","川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":null,"content":"Ros2 命令行接口 所有 ROS2 命令行 开始都要添加前缀 ros2，后面跟一条指令，一个动词，若干参数 文档参数 ros2 指令 --help # 或者 ros2 指令 -h ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:1:0","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"指令 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:0","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"action 允许手动发送目标并显示关于action的debug信息 动词 info 输出关于action的信息 ros2 action info /fibonacci list 输出 action的名称列表 ros2 action list send_goal 发送action目标 ros2 action send_goal /fibonacci action_tutotials/action/Fibonacci \"order\" show 输出action的定义 ros2 action show action_tutorials/action/Fibonacci ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:1","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"bag 允许从rosbag 里 播放 topic 或者 录制 topic 到 rosbag 动词 info 输出bag的信息 ros2 info \u003cbag-name\u003e play 播放bag ros2 play \u003cbag-name\u003e record 录制bag ros2 record -a ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:2","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"component 各种组件动词 动词 list 输出正在运行的容器列表和组件 ros2 component list load 载入组件到 容器节点 ros2 component load /ComponentManager composition composition::Talker standalone 运行组件到它所属的独立容器节点中 types 输出在ament索引中注册组件的列表 ros2 component types unload 从容器节点卸载组件 ros2 component unload /ComponentManager 1 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:3","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"daemon 守护进程的动词 动词 start 如果守护进程没有运行就开启 status 输出守护进程的状态 stop 如果守护进程运行就停止 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:4","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"doctor 检查ROS 设置和其他潜在问题(如网络，包版本，rmw中间件等)的工具 ros2 doctor 同义词 wtf ros2 wtf 参数 –report/-r 输出所有检查的报告 ros2 doctor --report –report-fail/-rf 只输出失败检查的报告 ros2 doctor --report-fail –include-warning/-iw 包含失败的检查报告 rosw doctor --include-warning ros2 doctor --include-warning --report-fail ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:5","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"extension_points 列出 扩展 points ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:6","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"extensions 列出 扩展 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:7","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"interface 不同ROS接口(action/topic/service)相关动词。接口类型可以通过以下选项过滤掉: –only-actions, –only-msgs, –only-srvs 动词 list 列举所有可用的接口类型 ros2 interface list package 输出包内可用接口类型的列表 ros2 interface package std msgs packages 输出提供接口的包的列表 ros2 interface packages --only-msgs proto 打印接口的原型(主体) ros2 interface proto example interfaces/srv/AddTwoInts show 输出接口定义 ros2 interface show geometry msgs/msg/Pose ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:8","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"launch 允许在任意一个包里运行一个启动文件，不用 cd 到那个包里 ros2 launch \u003cpackage\u003e \u003claunch.file\u003e ros launch demo.nodes.cpp add_two_ints_launch.py ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:9","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"lifecycle 生命周期相关动词 动词 get 获取一个和多个节点的生命周期状态 list 输出可用的转换的李彪 nodes 输出具有生命周期的节点列表 set 触发生命周期状态转换 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:10","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"msg(弃用) 展示有关消息的调试信息 动词 list 输出消息类型列表 ros2 msg list package 输出给定包的消息列表 ros2 msg package std_msgs packages 输出包含该消息的包 ros2 msg packages show 输出消息定义 ros2 msg show geometry_msgs/msg/Pose ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:11","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"multicast 多播相关的动词 动词 receive 接收单个UDP多播包 send 发送单个UDP多播包 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:12","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"node 动词 info 输出节点信息 ros2 node info /talker list 输出可用节点列表 ros2 node list ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:13","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"param 允许操作参数 动词 delete 删除参数 ros2 param delete /talker /user_sim_time describe 展示已声明参数的描述性信息 dump 将给定节点的参数以yaml格式转储到终端或文件中 get 获取参数 ros2 param get /talker /user_sim_time list 输出可用参数的列表 ros2 param list set 设置参数 ros2 param set /talker /user_sim_time false ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:14","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"pkg 创建ros2包或者输出 包相关的信息 动词 create 创建新的ros2包 executables 输出指定包的可执行文件列表 ros2 pkg executables demo_nodes_cpp list 输出可用包的列表 ros2 pkg list prefix 输出包的前缀路径 ros2 pkg prefix std_msgs xml 输出包xml清单列表里的信息 ros2 pkg xml -t version ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:15","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"run 在任意包中允许运行可执行文件，而不用cd ros2 run \u003cpackage\u003e \u003cexecutable\u003e ros2 run demo_node_cpp talker ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:16","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"security 安全相关动词 动词 create_key 创建key ros2 security create_key demo keys /talker create_permission 创建keystore ros2 security create_permission demo keys /talker policies/sample policy.xml generate_artifacts 创建权限 ros2 security_generate artifacts list_keys 分配key create_keystore 从身份和策略文件中生成key和权限文件 ros2 security create keystore demo keys distribute_key 从ros 图数据生成 XML策略文件 generate_policy 列出key ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:17","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"service 允许手动调用服务，并显示有关服务的调试信息 动词 call 调用服务 ros2 service call /add two ints example interfaces/AddTwoInts ”a: 1, b: 2” find 输出 给定类型的服务列表 ros2 service find rcl interfaces/srv/ListParameters list 输出 服务名称列表 ros2 service list type 输出服务类型 ros2 service type /talker/describe parameters ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:18","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"srv(弃用) 服务相关动词 动词 list 输出可用服务类型 package 输出包中的可用服务类型 packages 输出包含服务的包 show 输出服务定义 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:19","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"test 运行ros2启动测试 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:20","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"topic 用于显示有关ROS主题的调试信息的工具，包括发布者、订阅者、发布速率和消息。 动词 bw 展示 topic的带宽 ros2 topic bw /chatter delay 从header的时间戳展示topic的延迟 echo 输出给定topic的消息到屏幕 ros2 topic echo /chatter find 查找给定类型的topic类型 ros2 topic find rcl interfaces/msg/Log hz 展示topic的发布率 ros2 topic hz /chatter info 输出给定topic的信息 ros2 topic info /chatter list 输出 活动的topic列表 ros2 topic list pub 发布数据到topic ros2 topic pub /chatter std msgs/msg/String ’data: Hello ROS 2 world’ type 输出topic的类型 ros2 topic type /rosout ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:21","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":null,"content":"记录 时间 地址 人数 排队等待 花销 2023年08月06日16:12:00 北京市东城区东直门外新中街甲1号 2 不用排队 280 ","date":"2023-08-06","objectID":"/sanyangcai/:1:0","tags":["美食","生活","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":null,"content":"一句话 萌生了去山城走一遭的想法 ","date":"2023-08-06","objectID":"/sanyangcai/:2:0","tags":["美食","生活","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":null,"content":"点餐 ","date":"2023-08-06","objectID":"/sanyangcai/:3:0","tags":["美食","生活","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":null,"content":"凉菜 素三样：豇豆，藕，苦瓜？其实还不错 蒜蓉白肉：蒜还比较辣，真不错，一点没觉得腻 ","date":"2023-08-06","objectID":"/sanyangcai/:3:1","tags":["美食","生活","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":null,"content":"热菜 辣子鸡：酥脆麻辣，还有花生米，鸡肉有骨头，感觉肉有点少？ 酸菜鱼：嗯，还是好吃的 ","date":"2023-08-06","objectID":"/sanyangcai/:3:2","tags":["美食","生活","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":null,"content":"主食 葱花饼：第一口吃下去，真不错 ","date":"2023-08-06","objectID":"/sanyangcai/:3:3","tags":["美食","生活","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":null,"content":"总结 个人感觉偏重庆那边的风格，够麻够辣，有机会还是要多出去看看。吃完了还是有个疑问，三样菜到底是那三样？ ","date":"2023-08-06","objectID":"/sanyangcai/:4:0","tags":["美食","生活","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":null,"content":"简介 Paimon 是一个流数据湖平台 ","date":"2023-07-15","objectID":"/paimon1/:1:0","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"数据入湖 实时集成 数据库整库同步 schema变更同步（表结构变更） 部分列更新 批量覆盖 ","date":"2023-07-15","objectID":"/paimon1/:1:1","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"支持 增量快照 产生changelog lookup join batch/OLAP 查询 ","date":"2023-07-15","objectID":"/paimon1/:1:2","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"核心特性 统一批处理和流处理 批量写入和读取，流式更新，变更日志生成 数据湖能力 低成本、高可靠性、可扩展的元数据 各种合并引擎 按照要求更新记录 变更日志生成 多种表类型 schema 变更 重命名列 一致性保证 使用两阶段提交 每次提交在提交时最多生成2个快照 桶是读写的最小单元 两个writer如果修改的不是同一个桶，则提交是有序的 如果两个writer修改同一个桶，保证快照隔离，不会丢失更改信息，可能会合并两次提交，混合起来 ","date":"2023-07-15","objectID":"/paimon1/:1:3","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"文件布局 快照文件(Snapshot Files) 快照目录存储了所有的快照文件 快照文件是JSON文件 一个快照文件包含 若干使用的schema文件 快照所有更改的manifest lists 清单文件(Manifest Files) 清单目录存储了清单列表(manifest lists)和清单文件(manifest files) 清单列表保存的是清单文件的文件名 清单文件包含 LSM数据文件和 changelog 文件的变化(例如快照中LSM数据的创建或者是文件的删除) 数据文件(Data Files) 数据文件会进行分区和分桶 一个分区下面会有多个分桶(目录) 一个分桶里包含LSM数据 以及 changelog 默认使用orc文件，支持parquet,avro LSM Trees paimon 的数据是由LSM Tree 进行组织的 Sorted Runs LSM Tree 将文件组织成若干个 Sorted Runs 一个 sorted run 包含一个或多个数据文件，每个数据文件只属于一个sorted run 数据文件中的记录按照主键排序 同一个 sorted run 中，数据文件的主键范围不会重叠 不同的 sorted run 中，数据文件的主键范围是有可能重叠的，因此在查询LSM Tree 的时候，需要合并所有的 sorted runs 中相同主键的记录，按照用户指定的合并引擎 和时间戳 LSM Tree 的新纪录会先写入内存，内存缓冲区满了之后会刷新到磁盘，创建新的sorted run Compaction 越来越多的记录写入LSM Tree 后，sorted run 就会增多 查询LSM Tree时需要将所有的 sorted run 都合并起来，过多的sorted run 就会导致查询性能下降，或者OOM 将多个sorted run 合并成一个大的 sorted run 的这个过程被称之为 Compaction 过于频繁的 Compaction也会消耗一定的cpu和磁盘io， paimon 使用与rocksdb类似的通用压缩 Compaction策略 可以指定 专用compaction作业(dedicated compaction job) 进行 compaction ","date":"2023-07-15","objectID":"/paimon1/:1:4","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"catalog 文件系统 paimon catalog 可以持久化元数据 支持两种metastore 文件系统，file HDFS 等 Hive,使用 hive metastore 存放元数据 CREATE CATALOG my_hive WITH ( 'type' = 'paimon', 'metastore' = 'hive', 'uri' = 'thrift://\u003chive-metastore-host-name\u003e:\u003cport\u003e', -- 'hive-conf-dir' = '...', this is recommended in the kerberos environment 'warehouse' = 'hdfs:///path/to/table/store/warehouse' ); ","date":"2023-07-15","objectID":"/paimon1/:2:0","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"上手 目前 paimon 提供了两个的jar包 boudled jar 用于 读写数据，也就是启动flink任务，with 参数等 action jar 用于 各种操作，如手动compaction等， 这个可以在任务之外进行操作 -- if you're trying out Paimon in a distributed environment, -- the warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( 'type'='paimon', 'warehouse'='file:/tmp/paimon' ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); -- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( 'connector' = 'datagen', 'fields.word.length' = '1' ); -- paimon requires checkpoint interval in streaming mode SET 'execution.checkpointing.interval' = '10 s'; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; 使用paimon后，该目录如下 ","date":"2023-07-15","objectID":"/paimon1/:3:0","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"default.db default.db 为默认的数据库，就是catalog下的数据库目录 ","date":"2023-07-15","objectID":"/paimon1/:3:1","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"word_count word_cound 是在数据库下创建的表目录 ","date":"2023-07-15","objectID":"/paimon1/:3:2","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"bucket-0 bucket-x 就是 真正存储数据的地方，数据多了之后还会有 bucket-1 bucket-2 等 是一系列orc 后缀的文件 ","date":"2023-07-15","objectID":"/paimon1/:3:3","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"schema schema 也是json 文件，而且会有多个,主要也是为了保存变更的schema { \"id\" : 0, \"fields\" : [ { \"id\" : 0, \"name\" : \"word\", \"type\" : \"STRING NOT NULL\" }, { \"id\" : 1, \"name\" : \"cnt\", \"type\" : \"BIGINT\" } ], \"highestFieldId\" : 1, \"partitionKeys\" : [ ], \"primaryKeys\" : [ \"word\" ], \"options\" : { } } ","date":"2023-07-15","objectID":"/paimon1/:3:4","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"snapshot 快照目录保存了很多的文件id，通过这些id 可以更快找到 清单文件和schema { \"version\" : 3, \"id\" : 15, \"schemaId\" : 0, \"baseManifestList\" : \"manifest-list-f8c8e502-67ed-4b4f-9d57-bd1ff93943e6-28\", \"deltaManifestList\" : \"manifest-list-f8c8e502-67ed-4b4f-9d57-bd1ff93943e6-29\", \"changelogManifestList\" : null, \"commitUser\" : \"a196f82d-68d9-416b-a941-3b478cb0ff9b\", \"commitIdentifier\" : 13, \"commitKind\" : \"APPEND\", \"timeMillis\" : 1692273479392, \"logOffsets\" : { }, \"totalRecordCount\" : 400, \"deltaRecordCount\" : 16, \"changelogRecordCount\" : 0, \"watermark\" : -9223372036854775808 } ","date":"2023-07-15","objectID":"/paimon1/:3:5","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"manifest 清单目录里有很多清单文件，文件不能完全打开，但是可以看到内部是有数据文件的id等信息，应该是数据变更的信息内容 ","date":"2023-07-15","objectID":"/paimon1/:3:6","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"数据类型 支持了基本上所有的Flink 的数据类型，除了 multiset map类型不支持主键 ","date":"2023-07-15","objectID":"/paimon1/:4:0","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":null,"content":"当 SqlValidator解析完作用域信息之后，紧接着就开始对类型进行验证 @Override public SqlNode validate(SqlNode topNode) { SqlValidatorScope scope = new EmptyScope(this); scope = new CatalogScope(scope, ImmutableList.of(\"CATALOG\")); final SqlNode topNode2 = validateScopedExpression(topNode, scope); final RelDataType type = getValidatedNodeType(topNode2); Util.discard(type); return topNode2; } ","date":"2023-06-15","objectID":"/flink-validate-type/:0:0","tags":["flink","calcite","sql"],"title":"FlinkSQL验证","uri":"/flink-validate-type/"},{"categories":null,"content":"getValidatedNodeType 每个SqlNode 都会设定一个类型比如， SELECT * FROM table WHERE column1 + column2 = 10 在解析完成后，“*” 会被解开变成user, product, amoutn， 就会被设定成这样一个类型 RecordType(BIGINT user, VARCHAR(2147483647) product, INTEGER amount) @Override public RelDataType getValidatedNodeType(SqlNode node) { RelDataType type = getValidatedNodeTypeIfKnown(node); if (type == null) { if (node.getKind() == SqlKind.IDENTIFIER) { throw newValidationError(node, RESOURCE.unknownIdentifier(node.toString())); } throw Util.needToImplement(node); } else { return type; } } @Override public @Nullable RelDataType getValidatedNodeTypeIfKnown(SqlNode node) { final RelDataType type = nodeToTypeMap.get(node); if (type != null) { return type; } final SqlValidatorNamespace ns = getNamespace(node); if (ns != null) { return ns.getType(); } final SqlNode original = originalExprs.get(node); if (original != null \u0026\u0026 original != node) { return getValidatedNodeType(original); } if (node instanceof SqlIdentifier) { return getCatalogReader().getNamedType((SqlIdentifier) node); } return null; } 这里的 nodeToTypeMap 是在validateScopedExpression 这一步就确定了的，整个过程是在 验证命名空间的过程汇总就写入了，当然也是递归的过程 type 类型推断 是通过 SqlValidatorScope 实现这个接口实现的 每个SqlNode 都对应一个 Scope 在这些Scope中定义规则 /** Derives the type of a node, never null. */ RelDataType deriveTypeImpl(SqlValidatorScope scope, SqlNode operand) { DeriveTypeVisitor v = new DeriveTypeVisitor(scope); final RelDataType type = operand.accept(v); return requireNonNull(scope.nullifyType(operand, type)); } 于是，在 SqlValidatorImpl中可以找到事如何推导出这些类型是如何生成的 private class DeriveTypeVisitor implements SqlVisitor\u003cRelDataType\u003e { private final SqlValidatorScope scope; DeriveTypeVisitor(SqlValidatorScope scope) { this.scope = scope; } @Override public RelDataType visit(SqlLiteral literal) { return literal.createSqlType(typeFactory); } @Override public RelDataType visit(SqlCall call) { final SqlOperator operator = call.getOperator(); return operator.deriveType(SqlValidatorImpl.this, scope, call); } @Override public RelDataType visit(SqlNodeList nodeList) { // Operand is of a type that we can't derive a type for. If the // operand is of a peculiar type, such as a SqlNodeList, then you // should override the operator's validateCall() method so that it // doesn't try to validate that operand as an expression. throw Util.needToImplement(nodeList); } @Override public RelDataType visit(SqlIdentifier id) { // First check for builtin functions which don't have parentheses, // like \"LOCALTIME\". final SqlCall call = makeNullaryCall(id); if (call != null) { return call.getOperator().validateOperands(SqlValidatorImpl.this, scope, call); } RelDataType type = null; if (!(scope instanceof EmptyScope)) { id = scope.fullyQualify(id).identifier; } // Resolve the longest prefix of id that we can int i; for (i = id.names.size() - 1; i \u003e 0; i--) { // REVIEW jvs 9-June-2005: The name resolution rules used // here are supposed to match SQL:2003 Part 2 Section 6.6 // (identifier chain), but we don't currently have enough // information to get everything right. In particular, // routine parameters are currently looked up via resolve; // we could do a better job if they were looked up via // resolveColumn. final SqlNameMatcher nameMatcher = catalogReader.nameMatcher(); final SqlValidatorScope.ResolvedImpl resolved = new SqlValidatorScope.ResolvedImpl(); scope.resolve(id.names.subList(0, i), nameMatcher, false, resolved); if (resolved.count() == 1) { // There's a namespace with the name we seek. final SqlValidatorScope.Resolve resolve = resolved.only(); type = resolve.rowType(); for (SqlValidatorScope.Step p : Util.skip(resolve.path.steps())) { type = type.getFieldList().get(p.i).getType(); } break; } } // Give precedence to namespace found, unless there // are no more identifier components. if (type == null || id.names.size() == 1) { // See if there's a column with the name we seek in // precisely one of the namespaces in this scope. RelDataType colType = scope.resolveColumn(id.names.get(0), id); if (colType !=","date":"2023-06-15","objectID":"/flink-validate-type/:1:0","tags":["flink","calcite","sql"],"title":"FlinkSQL验证","uri":"/flink-validate-type/"},{"categories":null,"content":" 协议 线数 通信类型 多主 数据率 总线上期间的数量 线缆长度(米) 工作模式 使用距离 $$UART$$ 2 异步 不支持 3Kbps 到 4Mbps 2 1.5 全双工 远距离 $$SPI$$ 3 同步 不支持 1Mbps \u003c 10 \u003c3 全双工 近距离低速 $$I^2C$$ 2 同步 支持 \u003c3.4Mbps \u003c 10 \u003c3 半双工 近距离低速 举例说明： I2C的使用场景：连接传感器、存储器、显示器等设备。例如，连接温度传感器、EEPROM存储器、OLED显示器等。 UART的使用场景：连接串口设备，如调试器、GPS接收器等。例如，连接串口调试器进行程序调试。 SPI的使用场景：连接存储器、显示器、传感器等设备。例如，连接Flash存储器、LCD显示器、加速度传感器等。 选择使用这三者的场景取决于具体的应用需求。 如果需要连接多个设备，可以选择使用I2C或SPI协议； 如果只需要连接单个设备，可以选择使用UART协议。 如果需要高速传输数据，可以选择使用SPI协议。 如果需要低功耗和简单的通信方式，可以选择使用I2C协议。 如果需要长距离传输数据，可以选择使用UART协议。 ","date":"2023-06-13","objectID":"/transmitter/:0:0","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":null,"content":"UART UART（Universal Asynchronous Receiver/Transmitter）：UART是一种异步串行通信协议，用于在计算机和外部设备之间传输数据。它使用两根线（TX和RX）进行通信，其中TX是发送线，RX是接收线。UART协议只支持单主机和单从机的通信，不能连接多个设备。UART通常用于连接串口设备，如调试器、GPS接收器等。 ","date":"2023-06-13","objectID":"/transmitter/:1:0","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":null,"content":"流控制 流控制的方式分别有软件和硬件两种。 软件的流控制方式，在UART通信中，只需RxD、TxD、GND三根即可，数据在传输过程中，依靠代码的判断处理，并通过收发双方进行的数据交互完成控制，在现有通信物理信号线基础上，使用控制字符(ASCII表中的0x00~0x0x1F、0x7F)完成控制指令的交互。一般在私有协议下也会定义一些特殊字符设为控制指令。 硬件的流控制方式，即在原有的RxD、TxD、GND三根信号线的基础上，再增加RTS/CTS和DTR/DSR这两组信号线。第一组线是RTS（Request toSend）和CTS（Clear toSend）。当接收方准备好接收数据，它置高RTS线表示它准备好了，如果发送方也就绪，它置高CTS，表示它即将发送数据。第二组线是DTR（DataTerminal Ready）和DSR（Data SetReady）。这些线主要用于Modem通信。使得串口和Modem通信他们的状态。例如：当Modem已经准备好接收来自PC的数据，它置高DTR线，表示和电话线的连接已经建立。读取DSR线置高，PC机开始发送数据。一个简单的规则是DTR/DSR用于表示系统通信就绪，而RTS/CTS用于单个数据包的传输。 ","date":"2023-06-13","objectID":"/transmitter/:1:1","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":null,"content":"优点 只使用两根电线，不需要时钟信号 有一个奇偶校验位，只要双方设置后，就可以改变数据包的结构 ","date":"2023-06-13","objectID":"/transmitter/:1:2","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":null,"content":"缺点 数据帧的大小限制为最多9位，不支持多个从属或多个主系统 每个UART的波特率必须在10％之内 ","date":"2023-06-13","objectID":"/transmitter/:1:3","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":null,"content":"SPI SPI（Serial Peripheral Interface）：SPI是一种串行通信协议，用于在芯片之间传输数据。它使用四根线（MOSI、MISO、SCK和SS）进行通信，其中MOSI是主设备输出从设备输入的数据线，MISO是主设备输入从设备输出的数据线，SCK是时钟线，SS是片选线。SPI协议支持多主机和多从机的通信，可以连接多个设备。SPI通常用于连接存储器、显示器、传感器等设备。 ","date":"2023-06-13","objectID":"/transmitter/:2:0","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":null,"content":"SPI使用的四根信号线 SCLK: Serial Clock (output from master)：串行时钟，用来同步数据传输，由主机输出； MOSI \\ SIMO: Master Output, Slave Input(output from master)：主机输出从机输入数据线，通常先传输MSB； MISO \\ SOMI: Master Input, Slave Output(output from slave)：主机输入从机输出数据线，通常先传输LSB； SS: Slave Select (active low, output from master)：片选线，低电平有效，由主机输出。 SS\\CS：控制芯片是否被选中的，也就是说只有片选信号为预先规定的使能信号时（一般默认为低电位），对此芯片的操作才有效，这就允许在同一总线上连接多个SPI设备成为可能。也就是说：当有多个从设备的时候，因为每个从设备上都有一个片选引脚接入到主设备机中，当我们的主设备和某个从设备通信时将需要将从设备对应的片选引脚电平拉低。 ","date":"2023-06-13","objectID":"/transmitter/:2:1","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":null,"content":"SPI的四种操作模式 SPI的四种操作模式，它们的区别是定义了在时钟脉冲的哪条边沿转换（toggles）输出信号，哪条边沿采样输入信号，还有时钟脉冲的稳定电平值（就是时钟信号无效时是高还低）。 对于STM32等MCU自带的硬件SPI外设来说，可能没有那么重要，只需要配置一下模式就行了，但是对于使用使用GPIO模拟或者FPGA来实现SPI的时序，这一点是非常非常重要的。 Master 设备会根据将要交换的数据来产生相应的时钟脉冲(Clock Pulse)，时钟脉冲组成了时钟信号(Clock Signal) ，每种模式由时钟信号中的时钟极性（clock polarity）CPOL与时钟周期（clock phase）CPHA来定义。 不同的从设备可能在出厂是就是配置为某种模式，这是不能改变的，但我们的通信双方必须是工作在同一模式下，所以我们可以对我们的主设备的SPI模式进行配置，从而实现主从通讯。 时钟极性CPOL是用来配置SCLK的电平出于哪种状态时是空闲态或者有效态；时钟相位CPHA是用来配置数据采样是在第几个边沿。 CPOL=0，表示当SCLK=0时处于空闲态，所以有效状态就是SCLK处于高电平时； CPOL=1，表示当SCLK=1时处于空闲态，所以有效状态就是SCLK处于低电平时； CPHA=0，表示数据采样是在第1个边沿，数据发送在第2个边沿； CPHA=1，表示数据采样是在第2个边沿，数据发送在第1个边沿。 在高电平有效状态时，第一边沿为上升沿，第二边沿为下降沿；在低电平有效状态时，第一边沿为下降沿，第二边沿为上升沿 具体四种模式如下： CPOL = 0，CPHA = 0：时钟高电平时为有效状态，时钟上升沿（第一个边沿）采样。 CPOL = 0，CPHA = 1：时钟高电平时为有效状态，时钟下降沿（第二个边沿）采样。 CPOL = 1，CPHA = 0：时钟低电平时为有效状态，时钟下降沿（第一个边沿）采样。 CPOL = 1，CPHA = 1：时钟低电平时为有效状态，时钟上升沿（第二个边沿）采样。 ","date":"2023-06-13","objectID":"/transmitter/:2:2","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":null,"content":"I2C I2C（Inter-Integrated Circuit）：I2C是一种串行通信协议，用于在芯片之间传输数据。它使用两根线（SDA和SCL）进行通信，其中SDA是数据线，SCL是时钟线。I2C协议支持多主机和多从机的通信，可以连接多个设备。I2C通常用于连接传感器、存储器、显示器等设备。 IIC 是多主设备的总线，IIC没有物理的芯片选择信号线，没有仲裁逻辑电路，只使用serial data (SDA)数据线 和 serial clock(SCL)时钟线两条信号线，数据线用来传输数据，时钟线用来同步数据收发。两根信号线都是双向传输的，这两条线都是漏极开路或者集电极开路结构，使用时需要外加上拉电阻，可以挂载多个设备。 ","date":"2023-06-13","objectID":"/transmitter/:3:0","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":null,"content":"当Sql语句转换为SqlNode 之后 就会进入下一个阶段：将SqlNode 转换成 Operations @Override public List\u003cOperation\u003e parse(String statement) { CalciteParser parser = calciteParserSupplier.get(); FlinkPlannerImpl planner = validatorSupplier.get(); Optional\u003cOperation\u003e command = EXTENDED_PARSER.parse(statement); if (command.isPresent()) { return Collections.singletonList(command.get()); } // parse the sql query // use parseSqlList here because we need to support statement end with ';' in sql client. SqlNodeList sqlNodeList = parser.parseSqlList(statement); List\u003cSqlNode\u003e parsed = sqlNodeList.getList(); Preconditions.checkArgument(parsed.size() == 1, \"only single statement supported\"); return Collections.singletonList( SqlNodeToOperationConversion.convert(planner, catalogManager, parsed.get(0)) .orElseThrow(() -\u003e new TableException(\"Unsupported query: \" + statement))); } 其中 SqlNodeToOperationConversion.convert方法，就算是将 SqlNode转换成 Operation SqlNod会先进行验证工作，然后再进行转换 public static Optional\u003cOperation\u003e convert( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode sqlNode) { // validate the query final SqlNode validated = flinkPlanner.validate(sqlNode); return convertValidatedSqlNode(flinkPlanner, catalogManager, validated); } ","date":"2023-06-06","objectID":"/flink-sql-validate/:0:0","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":null,"content":"为什么 SqlNode 需要验证 目的当然是为了确保SQL语句的正确性和合法性，避免在后续的执行过程中出现错误 举例来说，假设有以下的SQL语句： SELECT name, age FROM student WHERE age \u003e 18 AND gender = 'male' 在进行转换成Operation的过程中，需要先进行validate，检查SQL语句是否符合语法规则和语义规则。如果SQL语句中存在语法错误或者语义错误，那么validate过程会抛出异常，提示用户需要修改SQL语句 例如，如果SQL语句中存在以下错误： SELECT name, age FROM student WHERE age \u003e 'two' AND gender = 'male' 其中，age的类型是整数，但是在SQL语句中使用了字符串类型的’two’进行比较，这是一个语义错误。在进行validate的过程中，会检测到这个错误并抛出异常，提示用户需要修改SQL语句中的比较条件 因此，先进行validate的过程可以帮助用户在SQL语句执行之前就发现错误，避免在后续的执行过程中出现问题，提高SQL语句的执行效率和准确性 ","date":"2023-06-06","objectID":"/flink-sql-validate/:1:0","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":null,"content":"validate validate的过程的输入和输出都是SqlNode，除了检查类型之外还会更改SqlNode的节点信息 通过FlinkPlanner 对SqlNode进行检查 def validate(sqlNode: SqlNode): SqlNode = { val validator = getOrCreateSqlValidator() validate(sqlNode, validator) } FlinkCalciteSqlValidator 实现 了Calicte 的SqlValidatorImpl 作为validator，就是用来检查FlinkSql的Sql语法的 /** Extends Calcite's {@link SqlValidator} by Flink-specific behavior. */ @Internal public final class FlinkCalciteSqlValidator extends SqlValidatorImpl { // Enables CallContext#getOutputDataType() when validating SQL expressions. private SqlNode sqlNodeForExpectedOutputType; private RelDataType expectedOutputType; public FlinkCalciteSqlValidator( SqlOperatorTable opTab, SqlValidatorCatalogReader catalogReader, RelDataTypeFactory typeFactory, SqlValidator.Config config) { super(opTab, catalogReader, typeFactory, config); } ","date":"2023-06-06","objectID":"/flink-sql-validate/:2:0","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":null,"content":"创建 validator 第一次写入Sql时，会新建一个validator，后面再创建就会使用一个单例来处理 def getOrCreateSqlValidator(): FlinkCalciteSqlValidator = { if (validator == null) { val catalogReader = catalogReaderSupplier.apply(false) validator = createSqlValidator(catalogReader) } validator } 其中，catalogReaderSupplier是一个函数，目的是读取和解析数据库中的元信息，比如数据库，表，列等信息 函数的输入是一个bool值，用来确定大小写是否敏感；输出是FlinkCalciteCatalogReader FlinkCalciteCatalogReader相比于CalciteCatalogReader，包含了Flink特有的信息，例如：Flink的UDF函数信息和Flink的Table信息 FlinkCalciteSqlValidator 通过数据元信息，就可以创建 SqlValidator private def createSqlValidator(catalogReader: CalciteCatalogReader) = { val validator = new FlinkCalciteSqlValidator( operatorTable, catalogReader, typeFactory, SqlValidator.Config.DEFAULT .withIdentifierExpansion(true) .withDefaultNullCollation(FlinkPlannerImpl.defaultNullCollation) .withTypeCoercionEnabled(false) ) // Disable implicit type coercion for now. validator } 创建SqlValidator 需要以下信息 SqlOperatorTable, 定义了Sql操作符和方法，也可以查找这些操作符和方法 这个SqlOperatorTable是在创建 StreamTableEnvironment的时候就创建了，具体是在 // org.apache.flink.table.planner.delegation.PlannerContext public FrameworkConfig createFrameworkConfig() { return Frameworks.newConfigBuilder() .defaultSchema(rootSchema.plus()) .parserConfig(getSqlParserConfig()) .costFactory(new FlinkCostFactory()) .typeSystem(typeSystem) .convertletTable(FlinkConvertletTable.INSTANCE) .sqlToRelConverterConfig(getSqlToRelConverterConfig()) .operatorTable(getSqlOperatorTable(getCalciteConfig())) // 这里创建 // set the executor to evaluate constant expressions .executor( new ExpressionReducer( context.getTableConfig(), context.getClassLoader(), false)) .context(context) .traitDefs(traitDefs) .build(); } 一般来说 opTab 里包含的都是 Calcite自带的操作符 比如 not like, is not null 这类 SqlValidatorCatalogReader, 就是上面所说的FlinkCalciteCatalogReader，保存的SQL的元数据 RelDataTypeFactory, FlinkSQL的类型，连接 Flink 的 LogicalType 和 Calcite 的 RelDataType。 RelDataType 是 RelNode关系代数中的数据类型，使用Calcite解析SQL的时候内部会转换成这种类型 /** * Flink specific type factory that represents the interface between Flink's [[LogicalType]] and * Calcite's [[RelDataType]]. */ class FlinkTypeFactory( classLoader: ClassLoader, typeSystem: RelDataTypeSystem = FlinkTypeSystem.INSTANCE) extends JavaTypeFactoryImpl(typeSystem) with ExtendedRelTypeFactory { private val seenTypes = mutable.HashMap[LogicalType, RelDataType]() /** * Create a calcite field type in table schema from [[LogicalType]]. It use PEEK_FIELDS_NO_EXPAND * when type is a nested struct type (Flink [[RowType]]). * * @param t * flink logical type. * @return * calcite [[RelDataType]]. */ def createFieldTypeFromLogicalType(t: LogicalType): RelDataType = { def newRelDataType(): RelDataType = t.getTypeRoot match { case LogicalTypeRoot.NULL =\u003e createSqlType(NULL) case LogicalTypeRoot.BOOLEAN =\u003e createSqlType(BOOLEAN) case LogicalTypeRoot.TINYINT =\u003e createSqlType(TINYINT) case LogicalTypeRoot.SMALLINT =\u003e createSqlType(SMALLINT) case LogicalTypeRoot.INTEGER =\u003e createSqlType(INTEGER) case LogicalTypeRoot.BIGINT =\u003e createSqlType(BIGINT) case LogicalTypeRoot.FLOAT =\u003e createSqlType(FLOAT) case LogicalTypeRoot.DOUBLE =\u003e createSqlType(DOUBLE) case LogicalTypeRoot.VARCHAR =\u003e createSqlType(VARCHAR, t.asInstanceOf[VarCharType].getLength) case LogicalTypeRoot.CHAR =\u003e createSqlType(CHAR, t.asInstanceOf[CharType].getLength) // temporal types case LogicalTypeRoot.DATE =\u003e createSqlType(DATE) case LogicalTypeRoot.TIME_WITHOUT_TIME_ZONE =\u003e createSqlType(TIME) // interval types case LogicalTypeRoot.INTERVAL_YEAR_MONTH =\u003e createSqlIntervalType( new SqlIntervalQualifier(TimeUnit.YEAR, TimeUnit.MONTH, SqlParserPos.ZERO)) case LogicalTypeRoot.INTERVAL_DAY_TIME =\u003e createSqlIntervalType( new SqlIntervalQualifier(TimeUnit.DAY, TimeUnit.SECOND, SqlParserPos.ZERO)) case LogicalTypeRoot.BINARY =\u003e createSqlType(BINARY, t.asInstanceOf[BinaryType].getLength) case LogicalTypeRoot.VARBINARY =\u003e createSqlType(VARBINARY, t.asInstanceOf[VarBinaryType].getLength) case LogicalTypeRoot.DECIMAL =\u003e t match { case decimalType: DecimalType =\u003e createSqlType(DECIMAL, de","date":"2023-06-06","objectID":"/flink-sql-validate/:2:1","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":null,"content":"开始验证 创建validator 后 根据validate 来验证 sqlNode 重写 为什么要重写？ 重写相当于是在SqlNode进行一些预处理主要作用是检查查询语句是否符合Flink Table的语法规范，并对查询语句进行一些必要的转换和优化，以便更好地支持Flink的执行引擎 Flink中重写是依靠PreValidateReWriter来实现的 sqlNode.accept(new PreValidateReWriter(validator, typeFactory)) 如果是简单的SQL其实不需要重写 override def visit(call: SqlCall): Unit = { call match { case e: SqlRichExplain =\u003e e.getStatement match { case r: RichSqlInsert =\u003e rewriteInsert(r) case _ =\u003e // do nothing } case r: RichSqlInsert =\u003e rewriteInsert(r) case _ =\u003e // do nothing } } validator验证SqlNode 如果不是特殊的SQL，应该都会使用 validator来验证SqlNode case _ =\u003e validator.validate(sqlNode) // org.apache.calcite.sql.validate.SqlValidatorImpl; @Override public SqlNode validate(SqlNode topNode) { SqlValidatorScope scope = new EmptyScope(this); scope = new CatalogScope(scope, ImmutableList.of(\"CATALOG\")); final SqlNode topNode2 = validateScopedExpression(topNode, scope); final RelDataType type = getValidatedNodeType(topNode2); Util.discard(type); return topNode2; } SqlValidatorScope SqlValidatorScope是Calcite中的一个接口，用于表示SQL语句中的作用域。它包含了当前作用域中可见的所有表、列、函数等信息。 具体来说，SqlValidatorScope中包含了以下信息： 当前作用域中可见的所有表，包括别名和表的元数据信息。 当前作用域中可见的所有列，包括别名和列的元数据信息。 当前作用域中可见的所有函数，包括函数的元数据信息和参数信息。 当前作用域中可见的所有变量，包括变量的类型和值。 举例说明，假设有以下SQL语句： SELECT a.name, b.salary FROM employee a JOIN salary b ON a.id = b.id WHERE b.salary \u003e 5000; 在这个SQL语句中，作用域可以分为以下几个部分： SELECT子句中的作用域，包括a.name和b.salary两个列。 FROM子句中的作用域，包括employee和salary两个表。 JOIN子句中的作用域，包括a和b两个表的别名。 WHERE子句中的作用域，包括b.salary列和5000常量。 在每个作用域中，SqlValidatorScope都会包含相应的表、列、函数等信息，以便进行语法和语义的验证。 SqlValidatorScope 本身也是一个树形结构，根节点是一个空scope validateScopedExpression 验证过程的第一步就是重写SqlNode，将SqlNode中的不确定的地方都进行更新 然后如果是Select这种简单的SQL，会将查询进行注册 private SqlNode validateScopedExpression(SqlNode topNode, SqlValidatorScope scope) { SqlNode outermostNode = performUnconditionalRewrites(topNode, false); cursorSet.add(outermostNode); top = outermostNode; TRACER.trace(\"After unconditional rewrite: {}\", outermostNode); if (outermostNode.isA(SqlKind.TOP_LEVEL)) { registerQuery(scope, null, outermostNode, outermostNode, null, false); } outermostNode.validate(this, scope); if (!outermostNode.isA(SqlKind.TOP_LEVEL)) { // force type derivation so that we can provide it to the // caller later without needing the scope deriveType(scope, outermostNode); } TRACER.trace(\"After validation: {}\", outermostNode); return outermostNode; } performUnconditionalRewrites 这里大部分都是递归调用，能够实际重写的大部分都是函数调用 if (call.getOperator() instanceof SqlUnresolvedFunction) { assert call instanceof SqlBasicCall; final SqlUnresolvedFunction function = (SqlUnresolvedFunction) call.getOperator(); // This function hasn't been resolved yet. Perform // a half-hearted resolution now in case it's a // builtin function requiring special casing. If it's // not, we'll handle it later during overload resolution. final List\u003cSqlOperator\u003e overloads = new ArrayList\u003c\u003e(); opTab.lookupOperatorOverloads( function.getNameAsId(), function.getFunctionType(), SqlSyntax.FUNCTION, overloads, catalogReader.nameMatcher()); if (overloads.size() == 1) { ((SqlBasicCall) call).setOperator(overloads.get(0)); } } SqlNode的lookupOperatorOverloads方法主要是用于查找可用的操作符重载。具体来说，它会根据传入的操作符名称和参数类型，查找符合条件的操作符重载方法。 举个例子，假设有以下的SqlNode： SELECT * FROM table WHERE column1 + column2 = 10 在这个SqlNode中，有一个加法操作符“+”，它的左右两边分别是column1和column2这两个列。因此，lookupOperatorOverloads方法会首先根据“+”操作符名称查找可用的操作符重载方法。如果找到了多个重载方法，它会根据参数类型进一步筛选出符合条件的重载方法。 假设我们定义了以下的操作符重载方法： public static int operator +(int a, int b) {...} public static double operator +(double a, double b) {...} public static string operator +(string a, string b) {...} 在这种情况下，lookupOperatorOverloads方法会根据column1和column2的数据类型来选择合适的重载方法。 如果column1和column2都是int类型，那么会选择第一个重载方法； 如果column1和column2都是double类型，那么会选择第二个重载方法； 如果column1和column2都是string类型，那么会选择第三个重载方法。 如果找不到符合条件的操作符重载方法，lookupOperatorOverloads方法会抛出异常。 registerQuery private void registerQuery( SqlValidatorScope parentScope, @Nullable SqlValidatorScope usingScope, SqlNode node, SqlNode enclosingNode, @Nullable String alias, boolean forceNullabl","date":"2023-06-06","objectID":"/flink-sql-validate/:2:2","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":null,"content":"举例 以 select * from tableA where amount \u003e 2 为例， ","date":"2023-06-06","objectID":"/flink-sql-validate/:3:0","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":null,"content":"parseSqlList Sql语句从ParseImpl中调用CalciteParser的方法parseSqlList，将SQL语句转换成SqlNode // org.apache.flink.table.planner.parse.CalciteParser public SqlNodeList parseSqlList(String sql) { try { SqlParser parser = SqlParser.create(sql, config); return parser.parseStmtList(); } catch (SqlParseException e) { if (e.getMessage().contains(\"Encountered \\\"\u003cEOF\u003e\\\"\")) { throw new SqlParserEOFException(e.getMessage(), e); } throw new SqlParserException(\"SQL parse failed. \" + e.getMessage(), e); } } 其中 parser 的创建 使用到了 calcite 的 SqlParser实例 生成 SqlParser 需要两个参数 sql String 类型，就是传入的Sql字符串 config SqlParser.Config类型 ","date":"2023-05-27","objectID":"/flink-parsesql/:1:0","tags":["flink","calcite","sql","javacc"],"title":"FlinkSQL - SQL语句到SqlNode","uri":"/flink-parsesql/"},{"categories":null,"content":"SqlParser.Config Flink 在解析sqlQuery 或者 createTemporaryView 这类操作SQL的处理中，就会创建CalciteParser,在创建CalciteParser的时候会调用 getSqlParserConfig() 的方法获取SqlParser.Config // org.apache.flink.table.planner.delegation.PlannerContext /** * Returns the SQL parser config for this environment including a custom Calcite configuration. */ private SqlParser.Config getSqlParserConfig() { return JavaScalaConversionUtil.\u003cSqlParser.Config\u003etoJava( getCalciteConfig().getSqlParserConfig()) .orElseGet( // we use Java lex because back ticks are easier than double quotes in // programming and cases are preserved () -\u003e { SqlConformance conformance = getSqlConformance(); return SqlParser.config() .withParserFactory(FlinkSqlParserFactories.create(conformance)) .withConformance(conformance) .withLex(Lex.JAVA) .withIdentifierMaxLength(256); }); } 如果设置了Calicte的一些配置，那么就会在这里被读取出来自定义的CalicteParser， 否则，走orElseGet信息 SqlParser在这里设置了解析SQL的语法、词法、方言以及限定词的最大长度 SqlConformance SqlConformance 是用来设置 SQL的方言(或标准)的 在FlinkSQL中一共有两种方言， Calcite 默认方言 Hive 方言 FlinkSqlParserFactories /** A util method to create SqlParserImplFactory according to SqlConformance. */ public class FlinkSqlParserFactories { private FlinkSqlParserFactories() {} public static SqlParserImplFactory create(SqlConformance conformance) { if (conformance == FlinkSqlConformance.DEFAULT) { return FlinkSqlParserImpl.FACTORY; } else { throw new TableException(\"Unsupported SqlConformance: \" + conformance); } } } 这里就很有意思，FlinkSqlParser只接受Calcite默认方言 然后再看FlinkSqlParserImpl // package org.apache.flink.sql.parser.impl; /** * SQL parser, generated from Parser.jj by JavaCC. * * \u003cp\u003eThe public wrapper for this parser is {@link SqlParser}. */ public class FlinkSqlParserImpl extends SqlAbstractParserImpl implements FlinkSqlParserImplConstants { private static final Logger LOGGER = CalciteTrace.getParserTracer(); 可以看出，FlinkSqlParserImpl 是通过JavaCC生成的 生成此类的 JavaCC 文件 在 flink-table/flink-sql-parser/target/generated-sources/javacc/Parser.jj 而这个应该是根据 FreeMarker的模板fmpp来生成的,打开 flink-sql-parser 下面 codegen里面的config.fmpp，我们可以发现一下一段话，答题意思就是说FMPP继承了CalciteParser，然后解析指定的Sql # This file is an FMPP (http://fmpp.sourceforge.net/) configuration file to # allow clients to extend Calcite's SQL parser to support application specific # SQL statements, literals or data types. # # Calcite's parser grammar file (Parser.jj) is written in javacc # (https://javacc.org/) with Freemarker (http://freemarker.org/) variables # to allow clients to: # 1. have custom parser implementation class and package name. # 2. insert new parser method implementations written in javacc to parse # custom: # a) SQL statements. # b) literals. # c) data types. # 3. add new keywords to support custom SQL constructs added as part of (2). # 4. add import statements needed by inserted custom parser implementations. # # Parser template file (Parser.jj) along with this file are packaged as # part of the calcite-core-\u003cversion\u003e.jar under \"codegen\" directory. 简单看一下这个jj文件，可以发现，就是在定义SQL如何声明，以及如何处理SQL语句的 SqlLiteral AllOrDistinct() : { } { \u003cDISTINCT\u003e { return SqlSelectKeyword.DISTINCT.symbol(getPos()); } | \u003cALL\u003e { return SqlSelectKeyword.ALL.symbol(getPos()); } } 在 flink-sql-parser 中还可以看到生成这些FlinkSQL的逻辑，比如这种 Flink的建表语句如何判断的逻辑，都在这里可以找到 // org.apache.flink.sql.parser.ddl public class SqlCreateTable extends SqlCreate implements ExtendedSqlNode { public static final SqlSpecialOperator OPERATOR = new SqlSpecialOperator(\"CREATE TABLE\", SqlKind.CREATE_TABLE); ... writer.keyword(\"CREATE\"); if (isTemporary()) { writer.keyword(\"TEMPORARY\"); } writer.keyword(\"TABLE\"); if (isIfNotExists()) { writer.keyword(\"IF NOT EXISTS\"); } tableName.unparse(writer, leftPrec, rightPrec); if (columnList.size() \u003e 0 || tableConstraints.size() \u003e 0 || watermark != null) { SqlUnparseUtils.unparseTableSchema( writer, leftPrec, rightPrec, columnList, tableConstraints, watermark); } if (comment != null) { writer.newlineAndIndent(); writer.keyword(\"COMMENT\"); comment.unparse(writer, leftPrec, rightPrec); } ... 返回jj文件中，我们可以看出一条SQL语句是如何","date":"2023-05-27","objectID":"/flink-parsesql/:1:1","tags":["flink","calcite","sql","javacc"],"title":"FlinkSQL - SQL语句到SqlNode","uri":"/flink-parsesql/"},{"categories":null,"content":"SqlParser 了解了 SqlParser.Config的构成，SqlParser就更简单了创建过程就是将Config中的的数据拿出来 //~ Constructors ----------------------------------------------------------- private SqlParser(SqlAbstractParserImpl parser, Config config) { this.parser = parser; parser.setTabSize(1); parser.setQuotedCasing(config.quotedCasing()); parser.setUnquotedCasing(config.unquotedCasing()); parser.setIdentifierMaxLength(config.identifierMaxLength()); parser.setConformance(config.conformance()); parser.switchTo(SqlAbstractParserImpl.LexicalState.forConfig(config)); } 其中parser.switchTo 用于切换解析器的状态 具体来说，switchTo方法接受一个参数，表示要切换到的状态。在解析SQL语句的过程中，不同的状态对应着不同的语法规则和解析方式。通过切换状态，解析器可以根据当前的语法规则和上下文信息，正确地解析SQL语句。 举个例子，假设我们有一个SQL语句： SELECT name, age FROM users WHERE age \u003e 18; 在解析这个SQL语句时，解析器需要根据不同的关键字和符号，判断当前的语法状态。比如，在解析SELECT关键字时，解析器需要切换到SELECT状态；在解析FROM关键字时，解析器需要切换到FROM状态；在解析WHERE关键字时，解析器需要切换到WHERE状态。 具体的实现可以参考SqlAbstractParserImpl类的源代码。 所以一条字符串类型的Sql语句 转换成 SqlNode 的流程应该就是 ","date":"2023-05-27","objectID":"/flink-parsesql/:1:2","tags":["flink","calcite","sql","javacc"],"title":"FlinkSQL - SQL语句到SqlNode","uri":"/flink-parsesql/"},{"categories":null,"content":"记录 时间 地址 人数 排队等待 花销 2023年05月21日12:45:00 北京市西城区西四北大街24号 2 5分钟 170 ","date":"2023-05-21","objectID":"/xingyuancanting/:1:0","tags":["美食","生活","小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":null,"content":"一句话 点多了，下次尝尝过油肉 ","date":"2023-05-21","objectID":"/xingyuancanting/:2:0","tags":["美食","生活","小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":null,"content":"点餐 ","date":"2023-05-21","objectID":"/xingyuancanting/:3:0","tags":["美食","生活","小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":null,"content":"主食 炖肉面：吸溜…吸溜… 肥肉也挺香的肥而不腻 双拼捞面：一半过油肉，一半虾仁 ","date":"2023-05-21","objectID":"/xingyuancanting/:3:1","tags":["美食","生活","小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":null,"content":"热菜 糖醋里脊：酸甜口，酸酸甜甜 ","date":"2023-05-21","objectID":"/xingyuancanting/:3:2","tags":["美食","生活","小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":null,"content":"小吃 干炸丸子：本人还是比较喜欢吃丸子的，各种丸子都很好吃 豆皮：豆皮配面条，一样的爽口 ","date":"2023-05-21","objectID":"/xingyuancanting/:3:3","tags":["美食","生活","小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":null,"content":"总结 一家家常小店，很实惠的，据说还是一家国营店，人一直很多，我想哪一天如果路过了，还是会再进去的 ","date":"2023-05-21","objectID":"/xingyuancanting/:4:0","tags":["美食","生活","小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":null,"content":"记录 时间 地址 人数 排队等待 花销 2023年05月19日13:30:00 北京市东城区东直门内大街233号 3 不用排队 574 ","date":"2023-05-19","objectID":"/hudafanguan/:1:0","tags":["美食","生活","小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":null,"content":"一句话 看着别人吃还真挺香的~~ ","date":"2023-05-19","objectID":"/hudafanguan/:2:0","tags":["美食","生活","小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":null,"content":"点餐 ","date":"2023-05-19","objectID":"/hudafanguan/:3:0","tags":["美食","生活","小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":null,"content":"热菜 馋嘴蛙仔：很嫩，辣味十足，点了大份，量还是蛮大的，唯一不足的就是太咸了 香辣美容蹄：猪蹄软烂，辣味咸味刚刚好，点的菜里唯一一道没有那么辣 辣炒花蛤：花蛤很干净，有点辣还有一点类似香油的那种淡淡的香味，还是很不错的 ","date":"2023-05-19","objectID":"/hudafanguan/:3:1","tags":["美食","生活","小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":null,"content":"烧烤 烤羊肉串：烤羊肉串还不错，后来又点了一份 烤生蚝：肉很厚，蒜香味没有浸入到里面 ","date":"2023-05-19","objectID":"/hudafanguan/:3:2","tags":["美食","生活","小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":null,"content":"小龙虾 麻辣小龙虾：麻辣味很足，肉比较紧实，虾面往里面拌一下，吃着蛮爽的 ","date":"2023-05-19","objectID":"/hudafanguan/:3:3","tags":["美食","生活","小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":null,"content":"总结 这家店名气很大，来了就是吃小龙虾的 ","date":"2023-05-19","objectID":"/hudafanguan/:4:0","tags":["美食","生活","小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":null,"content":"记录 时间 地址 人数 排队等待 花销 2023年05月14日12:30:00 海淀区清河中街66号院1号楼6层L650B 2 30分钟 340 ","date":"2023-05-14","objectID":"/putiancanting/:1:0","tags":["美食","生活","闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":null,"content":"一句话 下次一个人跑去尝尝肉燕 ","date":"2023-05-14","objectID":"/putiancanting/:2:0","tags":["美食","生活","闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":null,"content":"点餐 ","date":"2023-05-14","objectID":"/putiancanting/:3:0","tags":["美食","生活","闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":null,"content":"凉菜 土笋冻：吃起来嘎吱嘎吱的，QQ弹弹的，啥都别想放嘴里就行啦 ","date":"2023-05-14","objectID":"/putiancanting/:3:1","tags":["美食","生活","闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":null,"content":"热菜 铁板盐焗蛏：肉很肥美，蛏子刚取出来的时候还有汁水 水晶猪蹄冻：浓稠，猪蹄冻的酱汁没吃过，很新奇 家乡焖笋干：笋干就是笋干的味道，让我想起了我不曾去过的鱼米水乡？ 九层塔海蛏煲：蛏子裹得面太厚了，鲜味被油炸和面团盖住了，尝不出肉质感 莆田荔枝肉：荔枝很好吃 马来风光：咸，油很大，菜上面的配料还挺鲜的 ","date":"2023-05-14","objectID":"/putiancanting/:3:2","tags":["美食","生活","闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":null,"content":"甜品 建莲雪耳汤：甜，有点甜 ","date":"2023-05-14","objectID":"/putiancanting/:3:3","tags":["美食","生活","闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":null,"content":"总结 作为一个北方人，对福建菜有一种好奇，吃到的东西都会觉得很新奇，有意思，可以再去尝尝别的菜 ","date":"2023-05-14","objectID":"/putiancanting/:4:0","tags":["美食","生活","闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":null,"content":"简介 Apache Calcite是一个动态数据管理框架。 它包含构成典型数据库管理系统的许多部分，但省略了一些关键功能:数据存储、处理数据的算法和存储元数据的存储库。 我们主要用 Calcite 来解析SQL。很多项目也是直接使用了Calcite来解析SQL，优化SQL等，比如 FlinkSQL calicte的基本架构如下 ","date":"2023-05-11","objectID":"/calcite-prefix/:1:0","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"关系代数 关系模型源于数学。关系是由元组构成的集合，可以通过关系的运算来表达查询要求，而关系代数恰恰是关系操作语言的一种传统的表示方式，它是一种抽象的查询语言。 关系代数的运算对象是关系，运算结果也是关系。与一般的运算一样，运算对象、运算符和运算结果是关系代数的三大要素。 关系代数的运算可分为两大类： 传统的集合运算。这类运算完全把关系看成元组的集合。传统的集合运算包括集合的广义笛卡儿积运算、并运算、交运算和差运算。 专门的关系运算。这类运算除了把关系看成元组的集合外，还通过运算表达了查询的要求。专门的关系运算包括选择、投影、连接和除运算。 关系代数中的运算符可以分为四类：传统的集合运算符、专门的关系运算符、比较运算符和逻辑运算符。 关系运算 和 SQL的关系如下 运算符 SQL关键字 含义 分类 $$\\cap$$ 交 集合运算 $$\\cup$$ 并 集合运算 $$-$$ 差 集合运算 $$\\times$$ from A,B 广义笛卡尔积 集合运算 $$\\sigma$$ where 选择 关系运算 $$\\Pi$$ select distinct 投影 关系运算 $$\\bowtie$$ join 连接 关系运算 $$\\div$$ 除 关系运算 $$\u003e$$ 大于 比较运算 $$\u003c$$ 小于 比较运算 $$=$$ 等于 比较运算 $$\\neq$$ 不等 比较运算 $$\\leqslant$$ 小于等于 比较运算 $$\\geqslant$$ 大于等于 比较运算 $$\\neg$$ 非 逻辑运算 $$\\land$$ 与 逻辑运算 $$\\lor$$ 或 逻辑运算 SQL语句会先翻译成为关系代数后再被执行的 在执行explain 一条SQL的时候 就可以看到翻译后关系代数的命名 == Abstract Syntax Tree == LogicalProject(user=[$0], product=[$1], amount=[$2]) +- LogicalFilter(condition=[\u003e($2, 2)]) +- LogicalTableScan(table=[[*anonymous_datastream_source$1*]]) ","date":"2023-05-11","objectID":"/calcite-prefix/:2:0","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"Calcite 解析流程 Parser 将SQL语句(字符串) 解析成 AST(抽象语法树) 语法树的节点在代码中为SqlNode Validate 校验SqlNode节点，替换节点中的部分属性，设置单调性等信息 Convert 将SqlNode 抽象语法树 转换成RelNode，执行逻辑执行计划 Optimize 成本优化 Execute 生成动态代码，并执行物理执行计划 其中， SqlNode 是语法树上的节点，其本质是把SQL语句进行拆解 RelNode 是关系节点，代表的事关系代数中的关系操作，RelNode 更倾向数学的概念，就可以进行下一步的优化了 RexNode 虽然RexNode也属于关系节点，但是这里的RexNode更偏向于去表示表达式，比如一个常数，或者是 简单的a+b的运算，亦或是count(*) 这样的聚合；所以一个RelNode中会有很多RexNode节点 ","date":"2023-05-11","objectID":"/calcite-prefix/:3:0","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"SqlNode SqlNode 是一个抽象类，没有过多的信息，就包括一个位置信息的对象SqlParserPos SqlParserPos 是用来表示SQL语句中的位置信息的类。它包含了行号、列号和字符偏移量等信息，可以用于定位SQL语句中的错误或者生成更加详细的错误信息 public abstract class SqlNode implements Cloneable { //~ Static fields/initializers --------------------------------------------- public static final @Nullable SqlNode[] EMPTY_ARRAY = new SqlNode[0]; //~ Instance fields -------------------------------------------------------- protected final SqlParserPos pos; } public class SqlParserPos implements Serializable { //~ Static fields/initializers --------------------------------------------- /** * SqlParserPos representing line one, character one. Use this if the node * doesn't correspond to a position in piece of SQL text. */ public static final SqlParserPos ZERO = new SqlParserPos(0, 0); /** Same as {@link #ZERO} but always quoted. **/ public static final SqlParserPos QUOTED_ZERO = new QuotedParserPos(0, 0, 0, 0); private static final long serialVersionUID = 1L; //~ Instance fields -------------------------------------------------------- private final int lineNumber; private final int columnNumber; private final int endLineNumber; private final int endColumnNumber; } 从SqlSelect的这个类中就可以看出，一个简单的SQL语句在SqlSelect中都能找到 public class SqlSelect extends SqlCall { //~ Static fields/initializers --------------------------------------------- // constants representing operand positions public static final int FROM_OPERAND = 2; public static final int WHERE_OPERAND = 3; public static final int HAVING_OPERAND = 5; SqlNodeList keywordList; SqlNodeList selectList; @Nullable SqlNode from; @Nullable SqlNode where; @Nullable SqlNodeList groupBy; @Nullable SqlNode having; SqlNodeList windowDecls; @Nullable SqlNodeList orderBy; @Nullable SqlNode offset; @Nullable SqlNode fetch; @Nullable SqlNodeList hints; } 这里的keywordList是保留字，如果有 distinct 这种就会放在这个keywordList中 不过从debug的过程来看 这个keywordList是个空的数据 ","date":"2023-05-11","objectID":"/calcite-prefix/:3:1","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"RelNode RelNode 是个接口 继承了 RelOptNode RelOptNode 目前没有看到除了RelNode以外有其他地方使用和实现 RelNode 接口定义了树结构的父类以及节点数据和类型的方法 真正处理的方法都放在了 RelShuttle 和 RexShuttle中 public interface RelNode extends RelOptNode, Cloneable { /** * Accepts a visit from a shuttle. * * @param shuttle Shuttle * @return A copy of this node incorporating changes made by the shuttle to * this node's children */ RelNode accept(RelShuttle shuttle); /** * Accepts a visit from a shuttle. If the shuttle updates expression, then * a copy of the relation should be created. This new relation might have * a different row-type. * * @param shuttle Shuttle * @return A copy of this node incorporating changes made by the shuttle to * this node's children */ RelNode accept(RexShuttle shuttle); } 可以具体一点，看一下比较常用的RelNode 的实现是如何定义的：LogicalProject /** * Sub-class of Project not targeted at any particular engine or calling convention. */ public final class LogicalProject extends Project { //~ Constructors ----------------------------------------------------------- /** * Creates a LogicalProject. * * \u003cp\u003eUse {@link #create} unless you know what you're doing. * * @param cluster Cluster this relational expression belongs to * @param traitSet Traits of this relational expression * @param hints Hints of this relational expression * @param input Input relational expression * @param projects List of expressions for the input columns * @param rowType Output row type */ public LogicalProject( RelOptCluster cluster, RelTraitSet traitSet, List\u003cRelHint\u003e hints, RelNode input, List\u003c? extends RexNode\u003e projects, RelDataType rowType) { super(cluster, traitSet, hints, input, projects, rowType); assert traitSet.containsIfApplicable(Convention.NONE); } /** * Creates a LogicalProject by parsing serialized output. */ public LogicalProject(RelInput input) { super(input); } //~ Methods ---------------------------------------------------------------- /** Creates a LogicalProject. */ public static LogicalProject create(final RelNode input, List\u003cRelHint\u003e hints, final List\u003c? extends RexNode\u003e projects, @Nullable List\u003c? extends @Nullable String\u003e fieldNames) { final RelOptCluster cluster = input.getCluster(); final RelDataType rowType = RexUtil.createStructType(cluster.getTypeFactory(), projects, fieldNames, SqlValidatorUtil.F_SUGGESTER); return create(input, hints, projects, rowType); } /** Creates a LogicalProject, specifying row type rather than field names. */ public static LogicalProject create(final RelNode input, List\u003cRelHint\u003e hints, final List\u003c? extends RexNode\u003e projects, RelDataType rowType) { final RelOptCluster cluster = input.getCluster(); final RelMetadataQuery mq = cluster.getMetadataQuery(); final RelTraitSet traitSet = cluster.traitSet().replace(Convention.NONE) .replaceIfs(RelCollationTraitDef.INSTANCE, () -\u003e RelMdCollation.project(mq, input, projects)); return new LogicalProject(cluster, traitSet, hints, input, projects, rowType); } } LocalProject 的变量定义都散落在他的父类中 /** * Relational expression that computes a set of 'select expressions' from its input relational expression. * See Also: * org.apache.calcite.rel.logical.LogicalProject */ public abstract class Project extends SingleRel implements Hintable { //~ Instance fields -------------------------------------------------------- protected final ImmutableList\u003cRexNode\u003e exps; protected final ImmutableList\u003cRelHint\u003e hints; } /** * Abstract base class for relational expressions with a single input. * * \u003cp\u003eIt is not required that single-input relational expressions use this * class as a base class. However, default implementations of methods make life * easier. */ public abstract class SingleRel extends AbstractRelNode { //~ Instance fields -------------------------------------------------------- protected RelNode input; } ","date":"2023-05-11","objectID":"/calcite-prefix/:3:2","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"RexNode RexNode 是 行表达式(Row Expression) 是通过SqlNode 转换过来 尤其是 select 的项 where 的条件 等等 与SqlNode不同的是,RexNode的类型都是确定的，SqlNode的类型是在优化前就已经定好了，所以这个类型可能不能用 RexNode 是一个抽象类，也是实现了访问者模式 /** * Row expression. * * \u003cp\u003eEvery row-expression has a type. * (Compare with {@link org.apache.calcite.sql.SqlNode}, which is created before * validation, and therefore types may not be available.) * * \u003cp\u003eSome common row-expressions are: {@link RexLiteral} (constant value), * {@link RexVariable} (variable), {@link RexCall} (call to operator with * operands). Expressions are generally created using a {@link RexBuilder} * factory.\u003c/p\u003e * * \u003cp\u003eAll sub-classes of RexNode are immutable.\u003c/p\u003e */ public abstract class RexNode { //~ Instance fields -------------------------------------------------------- // Effectively final. Set in each sub-class constructor, and never re-set. protected @MonotonicNonNull String digest; //~ Methods ---------------------------------------------------------------- public abstract RelDataType getType(); /** * Returns whether this expression always returns true. (Such as if this * expression is equal to the literal \u003ccode\u003eTRUE\u003c/code\u003e.) */ public boolean isAlwaysTrue() { return false; } /** * Returns whether this expression always returns false. (Such as if this * expression is equal to the literal \u003ccode\u003eFALSE\u003c/code\u003e.) */ public boolean isAlwaysFalse() { return false; } public boolean isA(SqlKind kind) { return getKind() == kind; } public boolean isA(Collection\u003cSqlKind\u003e kinds) { return getKind().belongsTo(kinds); } /** * Returns the kind of node this is. * * @return Node kind, never null */ public SqlKind getKind() { return SqlKind.OTHER; } @Override public String toString() { return requireNonNull(digest, \"digest\"); } /** Returns the number of nodes in this expression. * * \u003cp\u003eLeaf nodes, such as {@link RexInputRef} or {@link RexLiteral}, have * a count of 1. Calls have a count of 1 plus the sum of their operands. * * \u003cp\u003eNode count is a measure of expression complexity that is used by some * planner rules to prevent deeply nested expressions. */ public int nodeCount() { return 1; } /** * Accepts a visitor, dispatching to the right overloaded * {@link RexVisitor#visitInputRef visitXxx} method. * * \u003cp\u003eAlso see {@link RexUtil#apply(RexVisitor, java.util.List, RexNode)}, * which applies a visitor to several expressions simultaneously. */ public abstract \u003cR\u003e R accept(RexVisitor\u003cR\u003e visitor); /** * Accepts a visitor with a payload, dispatching to the right overloaded * {@link RexBiVisitor#visitInputRef(RexInputRef, Object)} visitXxx} method. */ public abstract \u003cR, P\u003e R accept(RexBiVisitor\u003cR, P\u003e visitor, P arg); /** {@inheritDoc} * * \u003cp\u003eEvery node must implement {@link #equals} based on its content */ @Override public abstract boolean equals(@Nullable Object obj); /** {@inheritDoc} * * \u003cp\u003eEvery node must implement {@link #hashCode} consistent with * {@link #equals} */ @Override public abstract int hashCode(); } 还是具体一点看一下 RexCall的定义 RexCall是有操作符的表达式，操作符可以是 一元二元，也可是函数，或者是固定语法 /** * An expression formed by a call to an operator with zero or more expressions * as operands. * * \u003cp\u003eOperators may be binary, unary, functions, special syntactic constructs * like \u003ccode\u003eCASE ... WHEN ... END\u003c/code\u003e, or even internally generated * constructs like implicit type conversions. The syntax of the operator is * really irrelevant, because row-expressions (unlike * {@link org.apache.calcite.sql.SqlNode SQL expressions}) * do not directly represent a piece of source code. * * \u003cp\u003eIt's not often necessary to sub-class this class. The smarts should be in * the operator, rather than the call. Any extra information about the call can * often be encoded as extra arguments. (These don't need to be hidden, because * no one is going to be generating source code from this tree.)\u003c/p\u003e */ public class RexCall extends RexNode { //~ Instance fields -------------------------------------------------------- public final SqlOperator op; public final ImmutableList\u003cRexNode\u003e operands; public","date":"2023-05-11","objectID":"/calcite-prefix/:3:3","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"案例 通过一个简单的例子 -- table(user,product,amount) select * from tableA where amount \u003e 2 该Sql语句经过后生成 LogicalProject(user=[$0], product=[$1], amount=[$2]) +- LogicalFilter(condition=[\u003e($2, 2)]) +- LogicalTableScan(table=[[default_catalog, default_database, tableA]]) 就可以看出： select * 对应的是 LocalProject where amount \u003e 2 对应的是 LogicalFilter from tableA 对应的是 LogicalTableScan LogicalProject 有以下变量 变量 类型 说明 案例中的取值 exprs ImmutableList\u003cRexNode\u003e 表达式，一般就是select 后面跟的表达式，这里的表达式已经转换过了，给不同的表达式进行了命名,user=[$0], product=[$1], amount=[$2] 这里都是RexInputRef [$0, $1, $2] hints ImmutableList\u003cRelHint\u003e 这里是hint 的表达式，是嵌在代码里的一串增强说明 [] input LogicalFilter 关联其他RelNode，是该对象的输入节点 LogicalFilter(condition=[\u003e($2, 2)]) rowType RelRecordType 数据类型 RecordType(BIGINT user, VARCHAR(2147483647) product, INTEGER amount) digest AbstractRelNode.InnerRelDigest 关系信息的摘要，根据此判断两个关系是否相同 - cluster RelOptCluster 默认的Cluster，提供元数据，统计信息，管理查询计划的各种关系运算符，提供优化查询计划的方法等 - id int - - traitSet List\u003cRelTraitSet\u003e 关系的一些特征、特质，比如处理引擎的规范，Flink分布，mini-batch，下ModifyKind， UpdateKind 这些 [Convention, FlinkRelDistribution, MiniBatchIntervalTrait, ModifyKindSetTrait, UpdateKindTrait] 可以看出RelNode 其实更像是一个关系代数，这个关系代数也是有树型关系在里面的 ","date":"2023-05-11","objectID":"/calcite-prefix/:3:4","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":null,"content":"sqlQuery sql 进入sqlQuery后，首先就是获取Parser 解析sql语句 // TableEnvironmentImpl.java @Override public Table sqlQuery(String query) { List\u003cOperation\u003e operations = getParser().parse(query); if (operations.size() != 1) { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query.\"); } Operation operation = operations.get(0); if (operation instanceof QueryOperation \u0026\u0026 !(operation instanceof ModifyOperation)) { return createTable((QueryOperation) operation); } else { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query of type \" + \"SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.\"); } } 这里会获取StreamPlanner的Parser @Override public Parser getParser() { return getPlanner().getParser(); } @VisibleForTesting public Planner getPlanner() { return planner; } StreamTableEnvironment 在创建的过程中会创建 Planner final Planner planner = PlannerFactoryUtil.createPlanner( executor, tableConfig, userClassLoader, moduleManager, catalogManager, functionCatalog); 其中， executor 用于执行Planner的对象 tableConfig table和SQL的配置项，比如 checkpoint，watermark等 userClassLoader 用户动态类加载器 默认的使用org.apache.flink.util.FlinkUserCodeClassLoaders来创建的 moduleManager 模块管理器，会将CoreModule的模块加入到管理器中 module 就是 定义的一系列元数据，包括函数、规则、操作符等 Modules define a set of metadata, including functions, user defined types, operators, rules, etc. Metadata from modules are regarded as built-in or system metadata that users can take advantages of. catalogManager 用于处理 catalog，封装catalog和一些临时表对象 catalog 提供了元数据信息，用来管理元数据信息(如table、view、function 和 type等)，提供了一套api，可以使用Table API和SQL来访问 This interface is responsible for reading and writing metadata such as database/table/views/UDFs from a registered catalog. It connects a registered catalog and Flink’s Table API. This interface only processes permanent metadata objects. In order to process temporary objects, a catalog can also implement the TemporaryOperationListener interface. functionCatalog 函数catalog，保存函数的定义 注册的函数就会放在这个对象中，像UDF 等注册的catalog也会放在这里 FLIP-65 Simple function catalog to store FunctionDefinitions in catalogs. Note: This class can be cleaned up a lot once we drop the methods deprecated as part of FLIP-65. In the long-term, the class should be a part of catalog manager similar to DataTypeFactory. PlannerFactoryUtil.createPlanner 方法会先找到 PlannerFactory（默认是 DefaultPlannerFactory）然后根据 TableConfig 中的execution.runtime-mode 确认启动的任务是流任务还是批任务，进而创建Planner(SteamPlanner) @Override public Planner create(Context context) { final RuntimeExecutionMode runtimeExecutionMode = context.getTableConfig().get(ExecutionOptions.RUNTIME_MODE); switch (runtimeExecutionMode) { case STREAMING: return new StreamPlanner( context.getExecutor(), context.getTableConfig(), context.getModuleManager(), context.getFunctionCatalog(), context.getCatalogManager(), context.getClassLoader()); case BATCH: return new BatchPlanner( context.getExecutor(), context.getTableConfig(), context.getModuleManager(), context.getFunctionCatalog(), context.getCatalogManager(), context.getClassLoader()); default: throw new TableException( String.format( \"Unsupported mode '%s' for '%s'. Only an explicit BATCH or \" + \"STREAMING mode is supported in Table API.\", runtimeExecutionMode, RUNTIME_MODE.key())); } } Planner的Parser, 就是用来解析SQL语句的, Parser 目前分为两种SQL方言 flink 默认的SQL 和 Hive @PublicEvolving public enum SqlDialect { /** Flink's default SQL behavior. */ DEFAULT, /** * SQL dialect that allows some Apache Hive specific grammar. * * \u003cp\u003eNote: We might never support all of the Hive grammar. See the documentation for supported * features. */ HIVE } 默认情况下 我们创建出来的的Parser(ParserImpl) 是使用Calcite进行解析的 /** * 这里的 context 就是根据planner 的信息创建的 * parser = parserFactory.create(new DefaultCalciteContext(catalogManager, plannerContext)) * */ @Override public Parser create(Context context) { DefaultCalciteContext defaultCalciteContext = (DefaultCalciteContext) context; return new ParserImpl( defaultCalciteContext.getCatalogManager(), defaultCalciteContext.getPlannerCon","date":"2023-05-04","objectID":"/flink-sqlquery/:1:0","tags":["flink","sql"],"title":"FlinkSQL - SQL解析过程","uri":"/flink-sqlquery/"},{"categories":null,"content":"ParserImpl 默认的ParserImpl的类定义 public ParserImpl( CatalogManager catalogManager, Supplier\u003cFlinkPlannerImpl\u003e validatorSupplier, Supplier\u003cCalciteParser\u003e calciteParserSupplier, RexFactory rexFactory) { this.catalogManager = catalogManager; this.validatorSupplier = validatorSupplier; this.calciteParserSupplier = calciteParserSupplier; this.rexFactory = rexFactory; } parse 的过程就是调用 CalciteParser 将 Sql 语句转换成SqlNode的过程，Calcite会调用JavaCC来解析SQL语句 然后经过转换 把 sqlNode转换成Operation 在这里如果有一些特殊的SQL解析 会放到EXTENDED_PARSER 里面进行解析 @Override public List\u003cOperation\u003e parse(String statement) { CalciteParser parser = calciteParserSupplier.get(); FlinkPlannerImpl planner = validatorSupplier.get(); Optional\u003cOperation\u003e command = EXTENDED_PARSER.parse(statement); if (command.isPresent()) { return Collections.singletonList(command.get()); } // parse the sql query // use parseSqlList here because we need to support statement end with ';' in sql client. SqlNodeList sqlNodeList = parser.parseSqlList(statement); List\u003cSqlNode\u003e parsed = sqlNodeList.getList(); Preconditions.checkArgument(parsed.size() == 1, \"only single statement supported\"); return Collections.singletonList( SqlNodeToOperationConversion.convert(planner, catalogManager, parsed.get(0)) .orElseThrow(() -\u003e new TableException(\"Unsupported query: \" + statement))); } 例如， SQL语句 select * from tableA where amount \u003e 2 经过 CalciteParser 就会 生成一个 SqlNode 最后经过SqlNodeToOperationConversion.convert 会转换成 包含逻辑计划的operation 目前流程为 ","date":"2023-05-04","objectID":"/flink-sqlquery/:2:0","tags":["flink","sql"],"title":"FlinkSQL - SQL解析过程","uri":"/flink-sqlquery/"},{"categories":null,"content":"flink 参数 参数 说明 flink 版本 1.17 java 版本 1.8 ","date":"2023-04-30","objectID":"/flink-prefix/:1:0","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"测试SQL select * from tableA where amount \u003e 2 ","date":"2023-04-30","objectID":"/flink-prefix/:2:0","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"运行环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); final DataStream\u003cOrder\u003e orderA = env.fromCollection( Arrays.asList( new Order(1L, \"beer\", 3), new Order(1L, \"diaper\", 4), new Order(3L, \"rubber\", 2))); final Table tableA = tableEnv.fromDataStream(orderA); final Table result = tableEnv.sqlQuery( \"select * from \" + tableA + \" where amount \u003e 2\" ); tableEnv.toDataStream(result, Row.class).print(); env.execute(); 运行结果为 (true,+I[1, beer, 3, 2, pen, 3]) (true,+I[1, beer, 3, 2, rubber, 3]) ","date":"2023-04-30","objectID":"/flink-prefix/:3:0","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"SQL流程 一条SQL语句 通过Calcite 转换成 物理计划，物理计划通过代码生成计划转换成Flink Transformation 从而最终转换成 Flink 的执行图 从代码来看 tableEnv.sqlQuery() 将sql 语句转换成了 逻辑计划 -\u003e 物理计划 env.execute() 生成StreamGraph 最终执行语句 ","date":"2023-04-30","objectID":"/flink-prefix/:4:0","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"sqlQuery sqlQuery 会把 输入的 SQL 语句转换成Operation Operation 就是对于表的所有操作(DML, DDL, DQL, DCL) Covers all sort of Table operations such as queries(DQL), modifications(DML), definitions(DDL), or control actions(DCL). This is the output of Planner.getParser() and Parser.parse(String). 在这里，Operation就是一个PlannerQueryOperation，里面包含了RelNode的信息。Operation 会包装成Table 对象返回 public Table sqlQuery(String query) { /** * 这里会解析Sql语句转换成关系代数 */ List\u003cOperation\u003e operations = getParser().parse(query); if (operations.size() != 1) { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query.\"); } Operation operation = operations.get(0); if (operation instanceof QueryOperation \u0026\u0026 !(operation instanceof ModifyOperation)) { /** * 这里将转换的Operation转换成Flink Table API可以识别的Table对象 */ return createTable((QueryOperation) operation); } else { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query of type \" + \"SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.\"); } } ","date":"2023-04-30","objectID":"/flink-prefix/:4:1","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":null,"content":"LSM 树(Log-Structured-Merge-Tree) 不算是树，其实是一种存储结构 利用顺序追加写来提高写性能 内存-文件读取方式会降低读性能 ","date":"2023-04-28","objectID":"/lsm/:1:0","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"MemTable 放置在内存里 最新的数据 按照Key 组织数据有序(HBase：使用跳表) WAL保证可靠性 ","date":"2023-04-28","objectID":"/lsm/:1:1","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Immutable MemTable MemTable 达到一定大小后转化成Immutable MemTable 写操作由新的MemTable 处理， 在转存过程中不阻塞数据更新操作 ","date":"2023-04-28","objectID":"/lsm/:1:2","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"SSTable(Sorted String Table) 有序键值对集合 在磁盘的数据结构 为了加快读取SSTable的读取，可以通过建立key索引以及布隆过滤器来加快key的查找 LSM树会将所有的数据插入、修改、删除等操作记录保存在内存中；当此类操作达到一定的数据量后，再批量地顺序写入到磁盘当中 LSM树的数据更新是日志式的，当一条数据更新会直接append一条更新记录完成的，目的就是为了顺序写，将Immutable MemTable flush到持久化存储，而不用修改之前的SSTable中的key 不同的SSTable中，可能存在相同Key的记录，最新的记录是准确的（索引/Bloom来优化查找速度） 为了去除冗余的key需要进行compactcao操作 ","date":"2023-04-28","objectID":"/lsm/:1:3","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Compact策略 ","date":"2023-04-28","objectID":"/lsm/:2:0","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"顺序冗余存储可能带来的问题 读放大 读取数据时实际读取的数据量大于真正的数据量 Eg: 先在 MemTable 查看当前Key 是否存在，不存在继续从SSTable中查找 写放大 写入数据时实际写入的数据量大于真正的数据量 Eg: 写入时可能触发Compact操作，导致实际写入的数据量远大于该key的数据量 空间放大 数据实际占用的磁盘空间比数据的真正大小更多 Eg: 对于一个key来说，只有最新的那条记录是有效的，而之前的记录都是可以被清理回收的。 ","date":"2023-04-28","objectID":"/lsm/:2:1","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"Compact策略 size-tiered 策略 保证每层内部的SSTable的大小相近 同时限制每一层SSTable的数量 每层限制SSTable为N，当每层SSTable达到N后，则触发Compact操作合并这些SSTable，并将合并后的结果写入到下一层成为一个更大的sstable。 当层数达到一定数量时，最底层的单个SSTable的大小会变得非常大。并且size-tiered策略会导致空间放大比较严重。即使对于同一层的SSTable，每个key的记录是可能存在多份的，只有当该层的SSTable执行compact操作才会消除这些key的冗余记录。 leveled策略 leveled策略也是采用分层的思想，每一层限制总文件的大小 将每一层切分成多个大小相近的SSTable 这一层的SSTable是全局有序的，意味着一个key在每一层至多只有1条记录，不存在冗余记录 合并过程 L1的总大小超过L1本身大小限制： 此时会从L1中选择至少一个文件，然后把它跟L2有交集的部分(非常关键)进行合并。生成的文件会放在L2: 此时L1第二SSTable的key的范围覆盖了L2中前三个SSTable，那么就需要将L1中第二个SSTable与L2中前三个SSTable执行Compact操作。 如果L2合并后的结果仍旧超出L5的阈值大小，需要重复之前的操作 —— 选至少一个文件然后把它合并到下一层 Ps: 多个不相干的合并是可以并发进行的： leveled策略相较于size-tiered策略来说，每层内key是不会重复的，即使是最坏的情况，除开最底层外，其余层都是重复key，按照相邻层大小比例为10来算，冗余占比也很小。因此空间放大问题得到缓解。但是写放大问题会更加突出。举一个最坏场景，如果LevelN层某个SSTable的key的范围跨度非常大，覆盖了LevelN+1层所有key的范围，那么进行Compact时将涉及LevelN+1层的全部数据 ","date":"2023-04-28","objectID":"/lsm/:2:2","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":null,"content":"介绍 AggHandlerCodeGenerator 的代码在 flink planner 下，用来生成聚合函数的代码,是scala 代码 ","date":"2022-09-26","objectID":"/flink-agghandlercodegenerator/:0:0","tags":["flink"],"title":"Flink AggHandlerCodeGenerator","uri":"/flink-agghandlercodegenerator/"},{"categories":null,"content":"类定义 package org.apache.flink.table.planner.codegen.agg class AggsHandlerCodeGenerator( ctx: CodeGeneratorContext, // 上下文 relBuilder: RelBuilder, // 用来生成关系表达式 inputFieldTypes: Seq[LogicalType], copyInputField: Boolean // 需要缓存时将此字段设置为true) { private val inputType = RowType.of(inputFieldTypes: _*) /** 常量表达式 */ private var constants: Seq[RexLiteral] = Seq() private var constantExprs: Seq[GeneratedExpression] = Seq() /** 窗口相关参数，窗口聚合才会用到 */ private var namespaceClassName: String = _ private var windowProperties: Seq[PlannerWindowProperty] = Seq() private var hasNamespace: Boolean = false /** 聚合信息 */ private var accTypeInfo: RowType = _ private var aggBufferSize: Int = _ private var mergedAccExternalTypes: Array[DataType] = _ private var mergedAccOffset: Int = 0 private var mergedAccOnHeap: Boolean = false private var ignoreAggValues: Array[Int] = Array() private var isAccumulateNeeded = false private var isRetractNeeded = false private var isMergeNeeded = false var valueType: RowType = _ /** * 生成 [[AggsHandleFunction]] 或者 [[NamespaceAggsHandleFunction]] 会创建 [[aggBufferCodeGens]] and [[aggActionCodeGens]] 两者包含相同的AggCodeGens，aggBufferCodeGens 以列表的扁平形式， aggActionCodeGens是树形结构 在没有distinct 的的情况下，两者相同 */ /** aggBufferCodeGens 用于生成相关累加器(Accumulator)的 方法 */ private var aggBufferCodeGens: Array[AggCodeGen] = _ /** aggActionCodeGens 树形结构，聚合distinct数据的时候，会将相同需要distinct的字段组成树结构 */ private var aggActionCodeGens: Array[AggCodeGen] = _ object aggshandlercodegenerator { /** static terms **/ val acc_term = \"acc\" val merged_acc_term = \"otheracc\" val accumulate_input_term = \"accinput\" val retract_input_term = \"retractinput\" val distinct_key_term = \"distinctkey\" val namespace_term = \"namespace\" val store_term = \"store\" val collector: string = classname[collector[_]] val collector_term = \"out\" val member_collector_term = \"convertcollector\" val convert_collector_type_term = \"convertcollector\" val key_term = \"groupkey\" val input_not_null = false } 如果一个SQL的结构如下 count(*), count(distinct a), count(distinct a) filter d \u003e 5, sum(a), sum(distinct a) +----------+-----------+-----------+---------+---------+----------------+ | count(*) | count(a') | count(a') | sum(a) | sum(a') | distinct(a) a' | +----------+-----------+-----------+---------+---------+----------------+ 那么 aggBufferCodeGens 会这样保存 ┌ │ └ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ * ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ d ─ ─ i ─ ─ s ─ ─ t ─ ─ i ─ ─ n ─ ─ c ─ ─ t ─ ─ ( ─ ─ a ─ ─ ) ─ ┐ │ ┘ aggActionCodeGens 会这样保存 ┌ │ │ │ │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ * ─ ─ ) ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┬ │ │ │ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ) ─ ┬ │ │ │ │ ┴ ─ ─ ─ ─ ─ d ─ ─ i ─ ─ s ├ ├ └ ─ ─ t ─ ─ ─ ─ ─ i c c s ─ ─ n o o u ─ ─ c u u m ─ ─ t n n ( ─ ─ ( t t a ─ ─ a ( ( ' ─ ─ ) a a ) ─ ─ ' ' ─ ─ a ) ) ─ ─ ' ─ ─ ( ─ ─ f ─ ─ i ─ ─ l ─ ─ t ─ ─ e ─ ─ r ─ ─ ─ ─ d ─ ─ ─ ─ \u003e ─ ─ ─ ─ 5 ─ ─ ) ─ ─ ─ ─ ─ ┐ │ │ │ │ ┘ ","date":"2022-09-26","objectID":"/flink-agghandlercodegenerator/:1:0","tags":["flink"],"title":"Flink AggHandlerCodeGenerator","uri":"/flink-agghandlercodegenerator/"},{"categories":null,"content":"CodeGeneratorContext package org.apache.flink.table.planner.codegen /** 生成代码的上下文，维护代码段的状态 */ class CodeGeneratorContext(val tableConfig: TableConfig) { // 保存用于传递生成类的对象列表 val references: mutable.ArrayBuffer[AnyRef] = new mutable.ArrayBuffer[AnyRef]() // 插入有序，只会被添加一次， 成员状态 private val reusableMemberStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 构造状态 private val reusableInitStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中open方法的状态 private val reusableOpenStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中close方法的状态 private val reusableCloseStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 清理 dataview 的状态 private val reusableCleanupStatements = mutable.LinkedHashSet[String]() // 单个记录的状态， 插入有序，因为本地变量需要被分割，所以本地变量无法访问，只能更新成员变量 private val reusablePerRecordStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // (inputTerm, index) -\u003e expr // 只会被添加一次， 初始化拆箱表达式Map val reusableInputUnboxingExprs: mutable.Map[(String, Int), GeneratedExpression] = mutable.Map[(String, Int), GeneratedExpression]() // 插入有序，只会被添加一次，构造函数的状态 private val reusableConstructorStatements: mutable.LinkedHashSet[(String, String)] = mutable.LinkedHashSet[(String, String)]() // 插入有序，只会被添加一次，类声明状态 private val reusableInnerClassDefinitionStatements: mutable.Map[String, String] = mutable.Map[String, String]() // string_constant -\u003e reused_term // 常量 private val reusableStringConstants: mutable.Map[String, String] = mutable.Map[String, String]() // LogicalType -\u003e reused_term // 类型序列化 private val reusableTypeSerializers: mutable.Map[LogicalType, String] = mutable.Map[LogicalType, String]() /** * Flag map that indicates whether the generated code for method is split into several methods. */ private val isCodeSplitMap = mutable.Map[String, Boolean]() // method_name -\u003e local_variable_statements private val reusableLocalVariableStatements = mutable.Map[String, mutable.LinkedHashSet[String]]( (currentMethodNameForLocalVariables, mutable.LinkedHashSet[String]())) ","date":"2022-09-26","objectID":"/flink-agghandlercodegenerator/:1:1","tags":["flink"],"title":"Flink AggHandlerCodeGenerator","uri":"/flink-agghandlercodegenerator/"},{"categories":null,"content":"Toxi Alisa ","date":"2022-09-02","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]