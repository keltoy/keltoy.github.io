[{"categories":["Share"],"content":" “love death and robot” ","date":"2024-04-19","objectID":"/ai/:0:0","tags":["ai"],"title":"AI分享会","uri":"/ai/"},{"categories":["Share"],"content":"关于技术 标题 分类 链接 种类 样式 ChatGPT 背后的“功臣”——RLHF 技术详解 链接 众神听令，王者归位！Meta重磅发布最强开源大模型 Llama 3 连接 扩散模型之DiT：纯Transformer架构 链接 Langchain-Chatchat 链接 十分钟读懂Stable Diffusion 链接 ","date":"2024-04-19","objectID":"/ai/:1:0","tags":["ai"],"title":"AI分享会","uri":"/ai/"},{"categories":["Share"],"content":"Just sharing “和别人分享你的知识，那才是永恒之道。” ","date":"2024-04-18","objectID":"/data/:0:0","tags":["flink","paimon"],"title":"数据分享会","uri":"/data/"},{"categories":["Share"],"content":"关于技术 标题 分类 链接 种类 样式 Jmx’s Blog 数据 Jmx’s Blog 博客 集合 大数据卷王 数据 大数据卷王 视频 集合 理解 Paimon changelog producer 数据 理解 Paimon changelog producer 博客 网页 常见 Java 代码缺陷及规避方式 代码 常见 Java 代码缺陷及规避方式 公众号 网页 paimon使用指南 数据 paimon使用指南 文章 网页 ","date":"2024-04-18","objectID":"/data/:1:0","tags":["flink","paimon"],"title":"数据分享会","uri":"/data/"},{"categories":["Code"],"content":" 还是要有部分技术的内容 ","date":"2023-10-26","objectID":"/coding-life/:0:0","tags":["目录"],"title":"Coding Life","uri":"/coding-life/"},{"categories":["Code"],"content":"Calcite Calcite 简介 ","date":"2023-10-26","objectID":"/coding-life/:1:0","tags":["目录"],"title":"Coding Life","uri":"/coding-life/"},{"categories":["Code"],"content":"Flink FlinkSQL - 开始 FlinkSQL - SQL解析过程 FlinkSQL - SQL语句到SqlNode FlinkSQL - 验证过程 FlinkSQL - 类型验证 FlinkSQL - Operation FlinkSQL - AggHandlerCodeGenerator ","date":"2023-10-26","objectID":"/coding-life/:2:0","tags":["目录"],"title":"Coding Life","uri":"/coding-life/"},{"categories":["Code"],"content":"Paimon Paimon ","date":"2023-10-26","objectID":"/coding-life/:3:0","tags":["目录"],"title":"Coding Life","uri":"/coding-life/"},{"categories":["Code"],"content":"Ros UART, I2C, SPI ros2 cheat sheet ","date":"2023-10-26","objectID":"/coding-life/:4:0","tags":["目录"],"title":"Coding Life","uri":"/coding-life/"},{"categories":["Code"],"content":"其他 LSM树 ","date":"2023-10-26","objectID":"/coding-life/:5:0","tags":["目录"],"title":"Coding Life","uri":"/coding-life/"},{"categories":["生活"],"content":" 川鲁粤淮扬，闽浙湘本帮~ 可能不能逛遍吃遍大好河山，但是希望在有生之年好好享受剩余人生~ 身体健康，生活幸福比什么都重要 ","date":"2023-10-07","objectID":"/life-in-beijing/:0:0","tags":["目录"],"title":"吃喝玩乐在北京","uri":"/life-in-beijing/"},{"categories":["生活"],"content":"吃喝篇 名称 菜系风格 连锁 小楼饭店 北京菜 否 曲园酒楼 湘菜 否 一楼一饭店 粤菜 是 鸡坤茶室 东南亚菜 是 八条一号 云南菜 否 东四民芳 北京菜 是 费大厨 湘菜 是 火烧云 云南菜 是 松鹤楼 苏浙菜 否 马凯餐厅 湘菜 是 傅记酱肉 小吃 否 晋阳饭庄 山西菜 是 清水亭 湖北菜 是 玉华台 淮扬菜 是 萃华楼 烤鸭 是 寂川 川菜 否 新新湘菜馆 湘菜 否 丰泽园 鲁菜 是 淮扬春 淮扬菜 是 京八大楼鲁花轩 北京菜 是 京八大楼京花轩 北京菜 是 京八大楼楚花轩 北京菜 是 一坐一忘 云南菜 是 柳泉居 鲁菜 否 三义和酒楼 鲁菜 是 蟹老宋 小吃 是 滇大池 云南菜 是 惠丰堂 鲁菜 是 森隆饭庄 苏浙菜 否 奥华餐厅 天津菜 否 三样菜 川菜 是 三样菜玺川府 川菜 是 燕郊烧鸽子 东北菜 是 老根山庄 东北菜 是 鼎泰丰 小吃 是 聚德楼 北京菜 否 功德林 素菜 否 三色莲花印度餐厅 印度菜 是 三色莲花东南亚餐厅 东南亚菜 是 四季民福 烤鸭 是 鸿宾楼 北京菜 是 淮扬府 淮扬菜 是 淮扬府游园惊梦 淮扬菜 是 北京饭店 北京菜 否 同和居 鲁菜 是 北平楼 北京菜 是 北京厨房 粤菜 是 苏帮袁 淮扬菜 是 喜四方 小吃 否 翠园 粤菜 是 铸钟褡裢火烧 小吃 是 普拉那 德国菜 是 京东一绝 小吃 否 烤肉宛 北京菜 是 新川面馆 小吃 是 局气 北京菜 是 满恒记 涮肉 否 延吉冷面 朝鲜菜 是 砂锅居 北京菜 是 南门涮肉 涮肉 是 锦芳小吃 小吃 是 华威肉饼 小吃 是 门钉李 小吃 否 牛排家 牛排 是 小大董 烤鸭 是 金生隆 小吃 是 北新桥卤煮 小吃 是 柴氏风味斋 小吃 否 大董 烤鸭 是 玉流馆 朝鲜菜 否 宜宾招待所 川菜 否 耙耳朵 川菜 否 泰丰楼 鲁菜 否 巴州金丝特 新疆菜 否 狼大爷 新疆菜 否 贾国龙中国堡 小吃 是 潇湘阁 湘菜 是 君琴花 贵州菜 否 红实家常菜 小吃 否 悠航啤酒 小吃 是 祥云轩 小吃 否 汗腾格里 新疆菜 否 西域驿品 新疆菜 否 北京克拉美丽酒店 新疆菜 否 新疆饭庄 新疆菜 是 准葛尔新疆菜 新疆菜 否 新疆兵团食府 新疆菜 否 新疆喀什餐厅 新疆菜 否 长安白云大酒店 陕菜 否 汉唐春秋西安小吃 小吃 否 飞天食府 甘肃菜 否 银峰兰州拉面 小吃 否 敦煌食府 甘肃菜 是 微商故里 徽菜 是 湖北味道 湖北菜 否 隆中宾馆餐厅 湖北菜 否 九头鸟食府 湖北菜 否 湘都湘味 湘菜 否 静海轩湘菜王 湘菜 否 洞庭君乡 湘菜 否 湘中缘湖南菜 湘菜 是 潇湘永州会馆 湘菜 否 益阳驻京办联络处餐厅 湘菜 否 湖南大厦首湘缘中餐厅 湘菜 否 北京重庆饭店中餐厅 川菜 否 天府食舫 川菜 否 贡院蜀楼 川菜 否 新川办餐厅 川菜 否 北京蜀都宾馆故宫店 川菜 否 蜀阳酒店 川菜 否 富乐山酒店川菜馆 川菜 否 蜀竹情酒楼北营房店 川菜 否 攀西土菜馆 川菜 否 珠穆朗玛餐厅 西藏菜 否 赣人之家 江西菜 否 合味原餐厅 江西菜 否 南昌饭店 江西菜 否 大厨匠 江西菜 否 贵州大厦贵州厅 贵州菜 否 甲秀楼缘餐厅 贵州菜 否 香者餐厅 粤菜 否 大黔门食府 贵州菜 否 布依人家酸汤鱼 贵州菜 否 醉美遵义 贵州菜 否 南长城食府 贵州菜 否 云腾食府 云南菜 否 商都酒店 豫菜 否 南阳食府 豫菜 否 信阳味道 豫菜 否 晋盛食府 山西菜 否 晋府大酒店 山西菜 否 黄河京都大酒店 山西菜 否 齐鲁苑 鲁菜 否 海泰食府 鲁菜 是 百纳烟台山商务酒店餐厅 鲁菜 否 潍坊菜馆 鲁菜 否 苏畅园 江苏大厦店 淮扬菜 否 苏畅园 江苏饭店 淮扬菜 否 南京大饭店(自助) 淮扬菜 否 汉堂苏北菜 淮扬菜 否 常州宾馆餐厅 淮扬菜 否 淮扬菜品鉴堂 淮扬菜 否 江南春 淮扬菜 否 安徽大厦 徽菜 否 同庆楼 徽菜 是 兰州宾馆餐厅院金晖嘉园店 甘肃菜 否 百草滩羊 宁夏菜 否 银都餐厅 宁夏菜 否 兰香苑青海大厦餐厅 青海菜 否 内蒙驻京办餐厅 内蒙菜 否 金绿源火锅 内蒙菜 否 元大都奶茶餐吧 内蒙菜 否 内蒙古宾馆中餐厅 内蒙菜 否 鄂尔多斯艾力酒店餐厅 内蒙菜 否 天赋河套餐厅(蒙之旅蒙菜馆) 内蒙菜 否 龙港酒楼 东北菜 否 京滨饭店 东北菜 否 雪松宾馆吉人缘餐厅 东北菜 否 长白山国际大酒店 东北菜 否 北国江城餐厅 东北菜 否 辽宁饭店辽食府餐厅 东北菜 否 锦州饭店 东北菜 否 豫香苑餐厅 豫菜 否 新云南皇冠酒店彩云天 云南菜 否 蝴蝶泉宾馆 云南菜 否 一品滇 云南菜 否 昆明大厦餐厅 云南菜 否 那兰酒楼 广西菜 否 海南食府 海南菜 否 山泉粤宴云山茶餐厅 粤菜 否 八闽食府 闽菜 否 福州宾馆 闽菜 否 厦门大厦颐豪酒店颐园餐厅 闽菜 否 兰水心桥经典莆田菜 闽菜 否 一品芗情 闽菜 否 浙江食府 苏浙菜 否 宁波宾馆中餐厅 苏浙菜 否 千岛湖鱼味馆 浙菜 否 莆田餐厅 闽菜 是 杏园餐厅 小吃 否 胡大饭馆 小龙虾 是 蜀风情 川菜 否 悦宾饭馆 北京菜 否 玉华宫宾馆 陕菜 否 帮蛋儿新疆丸子汤 新疆菜 否 羊大爷涮肉 涮肉 是 鸡记餐室 粤菜 否 BOTTEGA意库 意大利菜 是 潮堂 粤菜 是 绍拾叁 苏浙菜 否 天下盐 川菜 否 老吉堂 本帮菜 是 粤小馆 粤菜 是 达达尼尔土耳其餐厅 土耳其菜 否 天目湖之家 苏浙菜 否 塞兰坤新疆美食 新疆菜 否 蒙元涮肉 内蒙菜 否 冶春 淮扬菜 否 三晋宾馆餐厅 山西菜 否 凤凰源餐厅 河北菜 否 淡泊居 18891387567 鲁菜 否 云华阁 山西菜 否 菩提树茶餐厅 豫菜 否 泸州幺妹私房菜 小吃 否 葫芦娃牛板筋胡同私房菜 小吃 是 致久精烧滩羊酒馆 小吃 否 ","date":"2023-10-07","objectID":"/life-in-beijing/:1:0","tags":["目录"],"title":"吃喝玩乐在北京","uri":"/life-in-beijing/"},{"categories":["生活"],"content":"博物馆篇 名称 门票 故宫博物馆 中国国家博物馆 北京自然博物馆 孔庙和国子监博物馆 雍和宫藏传佛教艺术博物馆 中国铁道博物馆(正阳门馆) 北京通信电信博物馆 北京文博交流馆 中国美术馆 毛主席纪念堂 北京古观象台 北京市东南城角角楼文物保管所 文天祥祠 茅盾故居 中国医史博物馆 老舍纪念馆 保利艺术博物馆 北京工艺美术博物馆 北京警察博物馆 北京自来水博物馆 王府井古人类文化遗址博物馆 北京松堂斋民间雕刻博物馆 北京人民艺术剧院戏剧博物馆 北京百工博物馆 中国邮政邮票博物馆 中国法院博物馆 北京新文化运动纪念馆 北京通讯电信博物馆 中国法院博物馆 中国印刷博物馆 北京南海子麋鹿苑博物馆 北京西瓜博物馆 北京崔永平皮影艺术博物馆 中国民兵武器装备陈列馆 北京韩美林艺术馆 法海寺 中国航空博物馆 明十三陵博物馆 北京市昌平区博物馆 十三陵水库展览馆 老甲艺术馆 坦克博物馆 北京御生堂中医药博物馆 居庸关长城博物馆 门头沟区博物馆 中国人民革命军事博物馆 清华大学艺术博物馆 大钟寺古钟博物馆 北京石刻艺术博物馆 中华世纪坛艺术馆 北京航空馆 北京艺术博物馆 北京大学赛克勒考古与艺术博物馆 李大钊烈士陵园 中央民族大学民族博物馆 团城演武厅 中国国家画院美术馆 圆明园展览馆 大觉寺 中国蜜蜂博物馆 曹雪芹纪念馆 香山双清别墅 中国电信博物馆 北京上庄纳兰性德史迹陈列馆 北京市海淀区博物馆 中国人民大学博物馆 中国化工博物馆 北京石刻艺术博物馆 中国人民抗日战争纪念馆 北京汽车博物馆 北京辽金城垣博物馆 中华航天博物馆 卢沟桥历史博物馆 胡同张老北京民间艺术馆 中国农业博物馆 北京奥运博物馆 北京民俗博物馆 中国体育博物馆 中国传媒大学传媒博物馆 炎黄艺术馆 北京中华民族博物院 观复博物馆 中国现代文学馆 北京中医药大学中医药博物馆 北京中国紫檀博物馆 北京服装学院服饰博物馆 北京金台艺术馆 北京晋商博物馆 北京励志堂科举扁额博物馆 北京东韵民族艺术博物馆 中国科学院动物研究所标本展示馆 北京习三鼻烟壶紫砂壶博物馆 西藏文化博物馆 北京鲁迅博物馆 文化和旅游部恭王府博物馆 中国地质博物馆 中国科学技术馆 北京天文馆 首都博物馆 北京古代建筑博物馆 中国古动物馆 宋庆龄故居 民族文化宫博物馆 徐悲鸿纪念馆 郭沫若纪念馆 梅兰芳纪念馆 中国佛教图书文物馆 北京古代钱币博物馆 北京市白塔寺管理处 郭守敬纪念馆(闭馆改陈) 北京红楼文化艺术博物馆 古陶文明博物馆 中国钱币博物馆 恭王府及花园 慈悲庵 北京戏曲博物馆 中国印钞造币博物馆 北京宣南文化博物馆 北京李大钊故居 历代帝王庙 北京空竹博物馆 中国消防博物馆 和苑博物馆 海淀博物馆 昌平区博物馆 中国考古博物馆 国家典籍博物馆 天桥印象博物馆 石景山区博物馆 香山革命纪念馆 北京市和光书院博物馆 北京皇城御窑金砖博物馆 北京燕京八绝文化博物馆 辽金城垣博物馆 紫檀博物馆 木木美术馆 中国工艺美术馆 北京皇城艺术馆 红楼文化艺术博物馆 北京金台艺术博物馆 北京荣唐连环画博物馆 北京英杰硬石艺术博物馆 北京天佑兰亭书法文化博物馆 郭守敬纪念馆 齐白石旧居纪念馆 中华民族博物院 东四胡同博物馆 史家胡同博物馆 国家动物博物馆 中国第四纪冰川遗迹陈列馆 中国妇女儿童博物馆 中国华侨历史博物馆 北京文旺阁木作博物馆 中国景泰蓝艺术博物馆 北京中药炮制技术博物馆 北京国韵百年邮票钱币博物馆 北京市姜杰钢琴手风琴博物馆 中国海关博物馆 北京税务博物馆 首都粮食博物馆 红砖美术馆 国礼博物馆 李四光纪念馆 世界花卉大观园 北京茶叶博物馆 北京时间博物馆 北京二锅头酒博物馆 中国非物质文化遗产馆 中国皇家历史第一档案馆 北京御仙都皇家菜博物馆 北京菜百黄金珠宝博物馆 北京航空航天博物馆 中国人民大学家书博物馆 北京大学赛克勒艺术博物馆 北京服装学院民族服饰博物馆 北京外国语大学世界语言博物馆 曹雪芹故居 ","date":"2023-10-07","objectID":"/life-in-beijing/:2:0","tags":["目录"],"title":"吃喝玩乐在北京","uri":"/life-in-beijing/"},{"categories":["生活"],"content":"公园篇 名称 门票 奥林匹克森林公园 颐和园 天坛公园 北海公园 玉渊潭公园 八达岭国家森林公园 朝阳公园 西山国家森林公园 后海公园 百望山公园 青龙湖公园 ","date":"2023-10-07","objectID":"/life-in-beijing/:3:0","tags":["目录"],"title":"吃喝玩乐在北京","uri":"/life-in-beijing/"},{"categories":["Code"],"content":"简介 ","date":"2024-08-20","objectID":"/llm-intro/:1:0","tags":["llm","ai"],"title":"llm intro","uri":"/llm-intro/"},{"categories":["Code"],"content":"NN 神经网络 可以拟合任意函数 拟合过程黑盒 前后输入完全无关 ","date":"2024-08-20","objectID":"/llm-intro/:1:1","tags":["llm","ai"],"title":"llm intro","uri":"/llm-intro/"},{"categories":["Code"],"content":"RNN 循环神经网络 一般处理序列信息，前后输入相关 $$O_t= g(V \\cdot S_t) $$ $$S_t = f(U\\cdot X_t + W\\cdot S_{t-1})$$ ","date":"2024-08-20","objectID":"/llm-intro/:1:2","tags":["llm","ai"],"title":"llm intro","uri":"/llm-intro/"},{"categories":["Code"],"content":"Transformer网络 用于序列建模的深度神经网络结构 采用全局注意力机制 更好捕捉序列中不同位置之间的关系 并行计算优势明显 ","date":"2024-08-20","objectID":"/llm-intro/:2:0","tags":["llm","ai"],"title":"llm intro","uri":"/llm-intro/"},{"categories":["Code"],"content":"输入 单词Embedding + 位置Embedding (Positional Encoding) 相加得到 Embedding Embedding 是一种映射，将现实中的 文字、图片等信息转化为计算机能识别的语言 举例 地图类比 现实地理的embedding 对比one-hot编码 one-hot编码：过于稀疏，过度占用资源 Embedding 的作用 降维 利用矩阵乘法进行降维 升维 对低维的数据进行升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开了 语义中通过计算获得单词之间的关系，可以推算出更多单词之间的关系 单词Embedding 获取方式：Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到 位置Embedding Transformer 使用全局信息，不能利用单词的顺序信息 Transformer 中使用位置Embedding 保存单词在序列中的相对或绝对位置 位置Embedding 计算公式 $$PE_{(pos, 2i)} = \\sin (pos/10000^{2i/d})$$ $$PE_{(pos, 2i+1)} = \\cos (pos/10000^{2i/d})$$ PE: 位置Embedding d: PE 维度 2i:表示偶数维度 2i+1 表示奇数维度 pos 单词在句子中的位置 通过正余弦公式， $$\\sin (A+B) = \\sin A \\cos B + \\cos A\\sin B$$ $$\\cos (A+B) = \\cos A \\cos B - \\sin A\\sin B$$ $$PE_{pos}$$可以很快算出 $$PE_{pos+k}$$ 将单词的词 Embedding 和位置 Embedding 相加， 就是 Transformer 的输入 Attention 注意力机制 下意识的动作 被称为 不随意线索(不随着意识的线索) 自己主观的动作是随意线索 注意力机制则显式地考虑随意线索 随意线索称之为查询(query) 类比现实世界中我想要做的动作 每个输入是一个值(value) 和不随意线索(key) 对， 类比现实世界中的环境 通过注意力池化层来有偏向性地选择某些输入 我们可以用在图书馆找书的场景来类比注意力机制中的 query、key 和 value。 Query（查询）： 假设你现在有一个特定的研究主题，比如 “人工智能在医疗领域的应用”。这个研究主题就是 query，代表着你的特定需求和关注点。你带着这个问题来到图书馆，希望找到与之相关的书籍和资料。 Key（键）： 图书馆里的每一本书都可以看作一个信息单元。每本书的书名、目录、关键词等就像是 key。这些 key 可以帮助你快速判断这本书是否与你的查询需求相关。例如，一本书的书名是《人工智能医疗创新》，这个书名就是一个 key，它与你的 query 有较高的相关性。 Value（值）： 书的具体内容就是 value。当你通过书名等 key 判断一本书可能与你的 query 相关后，你会进一步查看这本书的具体内容，也就是 value。如果这本书的内容详细介绍了人工智能在医疗领域的各种应用案例、技术原理等，那么这些内容就是你所需要的 value。 在这个场景中，你首先根据 query（研究主题）去扫描图书馆里的各种 key（书名、关键词等），找到可能相关的书籍。然后，通过查看这些书籍的 value（具体内容），来获取与你的查询需求最相关的信息。这就类似于注意力机制中，通过 query 与 key 的匹配，提取出相应的 value，从而聚焦在最相关的信息上。 Self-attention （自注意力机制）","date":"2024-08-20","objectID":"/llm-intro/:2:1","tags":["llm","ai"],"title":"llm intro","uri":"/llm-intro/"},{"categories":["Code"],"content":" Flink 只对时间类型的窗口清除，其他类型的窗口不清除 每一个window 都有一个 trigger 和一个function(ProcessWindowFunction, ReduceFunction 或者 AggregateFunction) ","date":"2024-05-07","objectID":"/flink-window/:0:0","tags":["flink"],"title":"Flink Window","uri":"/flink-window/"},{"categories":["Code"],"content":"组成方式 Window Assigner 指派元素如何进入 window Trigger Evictor 从window中 剔除某些元素 ","date":"2024-05-07","objectID":"/flink-window/:1:0","tags":["flink"],"title":"Flink Window","uri":"/flink-window/"},{"categories":["Code"],"content":"Window Assigner 当我们使用window时，比如countWindow， text.flatMap(new WordCount.Tokenizer()) .name(\"tokenizer\") .keyBy(value -\u003e value.f0) .countWindow(windowSize, slideSize) .sum(1) .name(\"counter\"); 实际上就调用了 window， evictor 和 trigger 其中，调用window方法，就是根据 WindowAssigner 创建一个window package org.apache.flink.streaming.api.datastream; @Public public class KeyedStream\u003cT, KEY\u003e extends DataStream\u003cT\u003e { public WindowedStream\u003cT, KEY, GlobalWindow\u003e countWindow(long size, long slide) { return window(GlobalWindows.create()) .evictor(CountEvictor.of(size)) .trigger(CountTrigger.of(slide)); } @PublicEvolving public \u003cW extends Window\u003e WindowedStream\u003cT, KEY, W\u003e window( WindowAssigner\u003c? super T, W\u003e assigner) { return new WindowedStream\u003c\u003e(this, assigner); } ... ... } ","date":"2024-05-07","objectID":"/flink-window/:2:0","tags":["flink"],"title":"Flink Window","uri":"/flink-window/"},{"categories":["生活"],"content":" 齐鲁苑侧边 emmm，这么看还挺气派的 😋 ","date":"2023-11-26","objectID":"/qiluyuan/:0:0","tags":["鲁菜","驻京办"],"title":"齐鲁苑","uri":"/qiluyuan/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年11月26日05:10:00 北京市海淀区马甸桥南路2号七省大院山东宾馆1楼(马甸桥西北角) 2 不用排队 186 ","date":"2023-11-26","objectID":"/qiluyuan/:1:0","tags":["鲁菜","驻京办"],"title":"齐鲁苑","uri":"/qiluyuan/"},{"categories":["生活"],"content":"一句话 让我想家了，妈妈😭 ","date":"2023-11-26","objectID":"/qiluyuan/:2:0","tags":["鲁菜","驻京办"],"title":"齐鲁苑","uri":"/qiluyuan/"},{"categories":["生活"],"content":"点餐 ","date":"2023-11-26","objectID":"/qiluyuan/:3:0","tags":["鲁菜","驻京办"],"title":"齐鲁苑","uri":"/qiluyuan/"},{"categories":["生活"],"content":"热菜 泰安老卤豆腐 泰安老卤豆腐： 五花肉很香，且多汁，软软烂烂的，不过吃多了还是腻腻的；豆腐很入味，心急吃不了热豆腐，心伤就吃老卤豆腐~ 山东烩酥肉 山东烩酥肉：这个口感是微酸的，喝起来还是挺开胃的，白菜，粉皮，小酥肉，让我想到家里的白菜炖粉条~ ","date":"2023-11-26","objectID":"/qiluyuan/:3:1","tags":["鲁菜","驻京办"],"title":"齐鲁苑","uri":"/qiluyuan/"},{"categories":["生活"],"content":"主食 特色炝锅面 特色炝锅面：一碗炝锅面恰到好处，如果再来一个西红柿，就像极了家里的热腾腾的一碗面，吃的很舒服，暖到了的漂流在外的人的心坎里 潘金莲咸菜和武大郎烧饼 潘金莲咸菜： emmm，潘金莲咸菜，不是跟武大郎烧饼很配么，建议再来个西门庆白粥，白净白净的，跟咸菜也挺配 武大郎烧饼： 烧饼很酥，微甜，夹上咸菜，碳水的乐趣~ 再来个二郎炝锅面吧，不吃咸菜 白粥那一套，跟烧饼是一家 潍坊肉火烧 潍坊肉火烧： 不支持单点😂😂😂😂😂 这个上的比较慢，这个上来后这里已经吃不下了，但还是吃了1个，肉多且有汁水，味道很正，应该是用了章丘大葱；强烈建议出小份的 ","date":"2023-11-26","objectID":"/qiluyuan/:3:2","tags":["鲁菜","驻京办"],"title":"齐鲁苑","uri":"/qiluyuan/"},{"categories":["生活"],"content":"总结 就是很舒服，感觉就像回家了一样，当然家里不会，这也是一种宾至如归吧~ ","date":"2023-11-26","objectID":"/qiluyuan/:4:0","tags":["鲁菜","驻京办"],"title":"齐鲁苑","uri":"/qiluyuan/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-11-26","objectID":"/qiluyuan/:5:0","tags":["鲁菜","驻京办"],"title":"齐鲁苑","uri":"/qiluyuan/"},{"categories":["生活"],"content":"湘中缘-北苑店 店面并不难找,后面跟着早餐俩字不明白啥意思 😂 ","date":"2023-10-29","objectID":"/xiangzhongyuan/:0:0","tags":["湘菜","驻京办"],"title":"湘中缘湖南菜","uri":"/xiangzhongyuan/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年08月05日18:40:00 朝阳区秋实街北苑家园莲葩园7号1层13号 2 不用排队 188 ","date":"2023-10-29","objectID":"/xiangzhongyuan/:1:0","tags":["湘菜","驻京办"],"title":"湘中缘湖南菜","uri":"/xiangzhongyuan/"},{"categories":["生活"],"content":"一句话 再有饿又累的时候，皮蛋擂辣椒真的可以配一锅米饭 ","date":"2023-10-29","objectID":"/xiangzhongyuan/:2:0","tags":["湘菜","驻京办"],"title":"湘中缘湖南菜","uri":"/xiangzhongyuan/"},{"categories":["生活"],"content":"点餐 ","date":"2023-10-29","objectID":"/xiangzhongyuan/:3:0","tags":["湘菜","驻京办"],"title":"湘中缘湖南菜","uri":"/xiangzhongyuan/"},{"categories":["生活"],"content":"凉菜 擂辣椒皮蛋 擂辣椒皮蛋：这个不咋好看的凉菜已经成了我们去湘菜馆子必点的凉菜，自从在某一家吃到这个凉菜后，就变成了湘菜必备 这个辣椒皮蛋中辣椒很辣，个人口感不够咸，不过依旧不能撼动它下饭神菜的称号 ","date":"2023-10-29","objectID":"/xiangzhongyuan/:3:1","tags":["湘菜","驻京办"],"title":"湘中缘湖南菜","uri":"/xiangzhongyuan/"},{"categories":["生活"],"content":"热菜 臭豆腐 臭豆腐：这个臭豆腐不算太臭，豆腐表皮很脆，汤汁不错 辣椒炒土猪肉 辣椒炒土猪肉：辣椒炒肉不得不说湘菜的辣椒是真的辣，不过肉是带肉皮的感觉，吃起来会感觉有点腻 剁椒鱼头 剁椒鱼头：鱼肉很嫩，有点辣，越吃越上头，停下来就很辣，需要不停地吃，不停地吃… 这鱼的鱼刺是真的多，后面配的面配这个鱼头汤还真好吃，停不下来，越辣，越爽，吃的越多 清炒时蔬 清炒时蔬：清炒还是炝炒？口感鲜甜辣，味道不错 ","date":"2023-10-29","objectID":"/xiangzhongyuan/:3:2","tags":["湘菜","驻京办"],"title":"湘中缘湖南菜","uri":"/xiangzhongyuan/"},{"categories":["生活"],"content":"主食 米饭 米饭：米饭很香，看到上来电饭锅就感觉很亲切，感觉回家啦~ ","date":"2023-10-29","objectID":"/xiangzhongyuan/:3:3","tags":["湘菜","驻京办"],"title":"湘中缘湖南菜","uri":"/xiangzhongyuan/"},{"categories":["生活"],"content":"总结 湘菜香辣，有很独特的味道，可能是因为太饿了，会感觉味道都还挺好吃的~ 湘中缘应该还会再去的 ","date":"2023-10-29","objectID":"/xiangzhongyuan/:4:0","tags":["湘菜","驻京办"],"title":"湘中缘湖南菜","uri":"/xiangzhongyuan/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-10-29","objectID":"/xiangzhongyuan/:5:0","tags":["湘菜","驻京办"],"title":"湘中缘湖南菜","uri":"/xiangzhongyuan/"},{"categories":["生活"],"content":" 发现美女一枚~ ","date":"2023-10-22","objectID":"/longgangjiulou/:0:0","tags":["东北菜","驻京办"],"title":"龙港酒楼","uri":"/longgangjiulou/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年10月22日17:50:00 北京市西城区复兴门外北大街5号 2 无需排队 235 ","date":"2023-10-22","objectID":"/longgangjiulou/:1:0","tags":["东北菜","驻京办"],"title":"龙港酒楼","uri":"/longgangjiulou/"},{"categories":["生活"],"content":"一句话 虽然不在东北，还是低估了东北菜的分量 ","date":"2023-10-22","objectID":"/longgangjiulou/:2:0","tags":["东北菜","驻京办"],"title":"龙港酒楼","uri":"/longgangjiulou/"},{"categories":["生活"],"content":"点餐 ","date":"2023-10-22","objectID":"/longgangjiulou/:3:0","tags":["东北菜","驻京办"],"title":"龙港酒楼","uri":"/longgangjiulou/"},{"categories":["生活"],"content":"凉菜 东北大拉皮：五彩斑斓，拉皮晶莹剔透，柔软富有弹性；量大管饱，酸甜可口，配菜黄瓜胡萝卜紫甘蓝都很脆，爽口 ","date":"2023-10-22","objectID":"/longgangjiulou/:3:1","tags":["东北菜","驻京办"],"title":"龙港酒楼","uri":"/longgangjiulou/"},{"categories":["生活"],"content":"热菜 传统锅包肉：外酥里嫩，酸甜口的锅包肉吃起来一点也不腻，好吃，这一盘子2个人全都吃干净了 BB豆：忘了叫啥名字了，就记住最后的BB豆 😂，上来了一锅土豆也是惊呆了我们；口味终于不是酸甜口的，属于咸香口的土豆，太多了，吃到一半有点撑了 大棒骨： 大棒骨：大棒骨味道还可以，和卤肉店买的酱香棒骨还是有一些区别，没有卤肉店那么咸 ","date":"2023-10-22","objectID":"/longgangjiulou/:3:2","tags":["东北菜","驻京办"],"title":"龙港酒楼","uri":"/longgangjiulou/"},{"categories":["生活"],"content":"主食 韭菜盒子：韭菜盒真的可以，就喜欢这口，本来已经吃饱了，结果上来之后还是想吃，吃不够又点了一个 ","date":"2023-10-22","objectID":"/longgangjiulou/:3:3","tags":["东北菜","驻京办"],"title":"龙港酒楼","uri":"/longgangjiulou/"},{"categories":["生活"],"content":"总结 东北菜还是实惠一些，锅包肉酸甜不腻，韭菜盒子非常推荐~ ","date":"2023-10-22","objectID":"/longgangjiulou/:4:0","tags":["东北菜","驻京办"],"title":"龙港酒楼","uri":"/longgangjiulou/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-10-22","objectID":"/longgangjiulou/:5:0","tags":["东北菜","驻京办"],"title":"龙港酒楼","uri":"/longgangjiulou/"},{"categories":["生活"],"content":" 实际上是在宾馆的二层，应该既可以当做宾馆的餐厅，又对外营业，而且还可以当做会厅使用 ","date":"2023-10-14","objectID":"/fuzhoubinguan/:0:0","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年10月14日18:10:00 北京市西城区前半壁街30号 2 不用排队 451 ","date":"2023-10-14","objectID":"/fuzhoubinguan/:1:0","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":"一句话 终于吃到了梦寐以求的佛跳墙啦~ ","date":"2023-10-14","objectID":"/fuzhoubinguan/:2:0","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":"点餐 ","date":"2023-10-14","objectID":"/fuzhoubinguan/:3:0","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":"热菜 清炒广芥：根茎部非常嫩脆爽口，“芥蓝如菌蕈，脆美牙颊响”，我想我现在才能体会得到 正宗荔枝肉：荔枝肉内部包裹着一块荸荠，咬下去一口会散发出清香，整体味道酸甜可口，忍不住想吃第二口 ","date":"2023-10-14","objectID":"/fuzhoubinguan/:3:1","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":"主食 三鲜馅饼：韭菜，猪肉， 虾仁，传统的三鲜馅饼，内部汤汁很浓浊，咬下一口不留神就会溅出来 糟肉光饼：糟肉红润透亮，肥而不腻，有淡淡的酒香，香甜可口，个人感觉光饼的造型很像汉堡 海蛎饼：外部酥脆，内部嫩滑鲜臭 香糟炸海鳗：鳗鱼的感觉和糟肉类似，多了一点鲜臭味道，吃的时候要注意，可能会有鱼刺 ","date":"2023-10-14","objectID":"/fuzhoubinguan/:3:2","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":"例汤 鱼丸肉燕汤：鱼丸很大，有鱼的味道，细细品尝可以感觉到微小的鱼刺的感觉，不过不会被扎到；肉燕的皮比馄饨薄很多，吃起来偏咸；汤很鲜，那种平常在家能吃到的鲜 精品佛跳墙：佛跳墙汤是一种咸鲜的口感，一闻一尝就可以感觉到大海的那种味道，海参吃起来软糯，有一种吃骨髓的感觉；鲍鱼嫩嫩的，不过吃起来有点像蘑菇 ","date":"2023-10-14","objectID":"/fuzhoubinguan/:3:3","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":"甜点 芝麻芋泥：芋泥不是很甜，口感软糯顺滑，和奶茶店的香波芋泥是不一样的，一口接着一口，很快就吃完啦 ","date":"2023-10-14","objectID":"/fuzhoubinguan/:3:4","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":"总结 这家餐馆没个导航还真不太好找，整体菜品很具有南方的特点，我们作为北方人会感觉海鲜有一点海鲜那种臭味，上菜速度比较慢，基本上是上来一个菜我们吃完后，下一个菜才端上来，不过这也可以确定饭店的菜品是厨师做的；整体来说，给我们很多味道上的惊喜 ","date":"2023-10-14","objectID":"/fuzhoubinguan/:4:0","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-10-14","objectID":"/fuzhoubinguan/:5:0","tags":["闽菜","驻京办"],"title":"福州宾馆","uri":"/fuzhoubinguan/"},{"categories":["生活"],"content":" 稍不注意，就走过了 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:0:0","tags":["小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年10月05日13:10:00 北京市西城区大石桥胡同甲37号 2 10分钟 252 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:1:0","tags":["小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":["生活"],"content":"一句话 哥们几个来吃一顿还是挺舒服的，吃好吃饱 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:2:0","tags":["小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":["生活"],"content":"点餐 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:3:0","tags":["小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":["生活"],"content":"火锅 爆炸锅：牛肉很烂，很香，牛肚、牛板筋等没有异味，都很有嚼劲，豆腐也很香，回味无穷 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:3:1","tags":["小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":["生活"],"content":"配菜 配菜小料：酱料很香，配上牛肉真是一绝；萝卜很脆，但是没有辣味，很好吃；蔬菜感觉有点少，可能是因为肉太多了 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:3:2","tags":["小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":["生活"],"content":"总结 和哥们几个一起来搓一顿还是不错的，物美价廉，平时也可以去，补充一下蛋白质。我们吃的太饱了，中午吃的，导致下午都不咋想吃东西 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:4:0","tags":["小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-10-05","objectID":"/huluwaniubanjinghutongsifangcai/:5:0","tags":["小吃","火锅"],"title":"葫芦娃牛板筋胡同私房菜","uri":"/huluwaniubanjinghutongsifangcai/"},{"categories":["生活"],"content":" 门脸不咋地，不过进去之后别有洞天 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:0:0","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年08月26日12:51:00 北京市西城区西中胡同28号 2 3个小时+， 排队不过号，可拼桌 275 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:1:0","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["生活"],"content":"一句话 点菜没做好攻略，导致吃起来感觉太腻，不过第一口还是很惊艳的 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:2:0","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["生活"],"content":"点餐 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:0","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["生活"],"content":"凉菜 李庄白肉：调制的辣酱很好吃，还特意打包带回去了，肉因人而异，我觉得腻，对面美丽的女士并不觉得 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:1","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["生活"],"content":"热菜 川香辣子鸡：第一口真的好香，还有点烫嘴，花生米没有那么脆 酒都肥肠：肥肠是酥脆的，没有脏器味道，不过和辣子鸡实在太同类的，导致没有感觉到特别惊艳 粉蒸牛肉：牛肉是嫩的，吃起来很软糯，很久没有吃到这么嫩的牛肉了，也因人而异，对面美丽的女士并不太喜欢 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:2","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["生活"],"content":"主食 宜宾燃面：看不到辣椒，不过依然感觉很辣，不糊嘴，很好吃 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:3","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["生活"],"content":"甜品 醉三江：不甜，要是按照米酒的方式去喝，就没有那种意思 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:3:4","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["生活"],"content":"总结 这次点餐比较失败，没有蔬菜，吃起来就会感觉很腻，2个人其实没有吃完，下次去的话，会尝尝毛血旺，不过人是真的多… ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:4:0","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-08-26","objectID":"/yibinzhaodaisuo/:5:0","tags":["川菜","驻京办"],"title":"宜宾招待所","uri":"/yibinzhaodaisuo/"},{"categories":["Code"],"content":"Ros2 命令行接口 所有 ROS2 命令行 开始都要添加前缀 ros2，后面跟一条指令，一个动词，若干参数 文档参数 ros2 指令 --help # 或者 ros2 指令 -h ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:1:0","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"指令 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:0","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"action 允许手动发送目标并显示关于action的debug信息 动词 info 输出关于action的信息 ros2 action info /fibonacci list 输出 action的名称列表 ros2 action list send_goal 发送action目标 ros2 action send_goal /fibonacci action_tutotials/action/Fibonacci \"order\" show 输出action的定义 ros2 action show action_tutorials/action/Fibonacci ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:1","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"bag 允许从rosbag 里 播放 topic 或者 录制 topic 到 rosbag 动词 info 输出bag的信息 ros2 info \u003cbag-name\u003e play 播放bag ros2 play \u003cbag-name\u003e record 录制bag ros2 record -a ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:2","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"component 各种组件动词 动词 list 输出正在运行的容器列表和组件 ros2 component list load 载入组件到 容器节点 ros2 component load /ComponentManager composition composition::Talker standalone 运行组件到它所属的独立容器节点中 types 输出在ament索引中注册组件的列表 ros2 component types unload 从容器节点卸载组件 ros2 component unload /ComponentManager 1 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:3","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"daemon 守护进程的动词 动词 start 如果守护进程没有运行就开启 status 输出守护进程的状态 stop 如果守护进程运行就停止 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:4","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"doctor 检查ROS 设置和其他潜在问题(如网络，包版本，rmw中间件等)的工具 ros2 doctor 同义词 wtf ros2 wtf 参数 –report/-r 输出所有检查的报告 ros2 doctor --report –report-fail/-rf 只输出失败检查的报告 ros2 doctor --report-fail –include-warning/-iw 包含失败的检查报告 rosw doctor --include-warning ros2 doctor --include-warning --report-fail ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:5","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"extension_points 列出 扩展 points ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:6","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"extensions 列出 扩展 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:7","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"interface 不同ROS接口(action/topic/service)相关动词。接口类型可以通过以下选项过滤掉: –only-actions, –only-msgs, –only-srvs 动词 list 列举所有可用的接口类型 ros2 interface list package 输出包内可用接口类型的列表 ros2 interface package std msgs packages 输出提供接口的包的列表 ros2 interface packages --only-msgs proto 打印接口的原型(主体) ros2 interface proto example interfaces/srv/AddTwoInts show 输出接口定义 ros2 interface show geometry msgs/msg/Pose ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:8","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"launch 允许在任意一个包里运行一个启动文件，不用 cd 到那个包里 ros2 launch \u003cpackage\u003e \u003claunch.file\u003e ros launch demo.nodes.cpp add_two_ints_launch.py ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:9","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"lifecycle 生命周期相关动词 动词 get 获取一个和多个节点的生命周期状态 list 输出可用的转换的李彪 nodes 输出具有生命周期的节点列表 set 触发生命周期状态转换 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:10","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"msg(弃用) 展示有关消息的调试信息 动词 list 输出消息类型列表 ros2 msg list package 输出给定包的消息列表 ros2 msg package std_msgs packages 输出包含该消息的包 ros2 msg packages show 输出消息定义 ros2 msg show geometry_msgs/msg/Pose ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:11","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"multicast 多播相关的动词 动词 receive 接收单个UDP多播包 send 发送单个UDP多播包 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:12","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"node 动词 info 输出节点信息 ros2 node info /talker list 输出可用节点列表 ros2 node list ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:13","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"param 允许操作参数 动词 delete 删除参数 ros2 param delete /talker /user_sim_time describe 展示已声明参数的描述性信息 dump 将给定节点的参数以yaml格式转储到终端或文件中 get 获取参数 ros2 param get /talker /user_sim_time list 输出可用参数的列表 ros2 param list set 设置参数 ros2 param set /talker /user_sim_time false ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:14","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"pkg 创建ros2包或者输出 包相关的信息 动词 create 创建新的ros2包 executables 输出指定包的可执行文件列表 ros2 pkg executables demo_nodes_cpp list 输出可用包的列表 ros2 pkg list prefix 输出包的前缀路径 ros2 pkg prefix std_msgs xml 输出包xml清单列表里的信息 ros2 pkg xml -t version ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:15","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"run 在任意包中允许运行可执行文件，而不用cd ros2 run \u003cpackage\u003e \u003cexecutable\u003e ros2 run demo_node_cpp talker ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:16","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"security 安全相关动词 动词 create_key 创建key ros2 security create_key demo keys /talker create_permission 创建keystore ros2 security create_permission demo keys /talker policies/sample policy.xml generate_artifacts 创建权限 ros2 security_generate artifacts list_keys 分配key create_keystore 从身份和策略文件中生成key和权限文件 ros2 security create keystore demo keys distribute_key 从ros 图数据生成 XML策略文件 generate_policy 列出key ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:17","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"service 允许手动调用服务，并显示有关服务的调试信息 动词 call 调用服务 ros2 service call /add two ints example interfaces/AddTwoInts ”a: 1, b: 2” find 输出 给定类型的服务列表 ros2 service find rcl interfaces/srv/ListParameters list 输出 服务名称列表 ros2 service list type 输出服务类型 ros2 service type /talker/describe parameters ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:18","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"srv(弃用) 服务相关动词 动词 list 输出可用服务类型 package 输出包中的可用服务类型 packages 输出包含服务的包 show 输出服务定义 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:19","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"test 运行ros2启动测试 ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:20","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["Code"],"content":"topic 用于显示有关ROS主题的调试信息的工具，包括发布者、订阅者、发布速率和消息。 动词 bw 展示 topic的带宽 ros2 topic bw /chatter delay 从header的时间戳展示topic的延迟 echo 输出给定topic的消息到屏幕 ros2 topic echo /chatter find 查找给定类型的topic类型 ros2 topic find rcl interfaces/msg/Log hz 展示topic的发布率 ros2 topic hz /chatter info 输出给定topic的信息 ros2 topic info /chatter list 输出 活动的topic列表 ros2 topic list pub 发布数据到topic ros2 topic pub /chatter std msgs/msg/String ’data: Hello ROS 2 world’ type 输出topic的类型 ros2 topic type /rosout ","date":"2023-08-09","objectID":"/ros2-cheat-sheet/:2:21","tags":["ros"],"title":"ROS2 cli 清单","uri":"/ros2-cheat-sheet/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年08月06日16:12:00 北京市东城区东直门外新中街甲1号 2 不用排队 280 ","date":"2023-08-06","objectID":"/sanyangcai/:1:0","tags":["重庆菜","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":["生活"],"content":"一句话 萌生了去山城走一遭的想法 ","date":"2023-08-06","objectID":"/sanyangcai/:2:0","tags":["重庆菜","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":["生活"],"content":"点餐 ","date":"2023-08-06","objectID":"/sanyangcai/:3:0","tags":["重庆菜","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":["生活"],"content":"凉菜 素三样：豇豆，藕，苦瓜？其实还不错 蒜蓉白肉：蒜还比较辣，真不错，一点没觉得腻 ","date":"2023-08-06","objectID":"/sanyangcai/:3:1","tags":["重庆菜","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":["生活"],"content":"热菜 辣子鸡：酥脆麻辣，还有花生米，鸡肉有骨头，感觉肉有点少？ 酸菜鱼：嗯，还是好吃的 ","date":"2023-08-06","objectID":"/sanyangcai/:3:2","tags":["重庆菜","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":["生活"],"content":"主食 葱花饼：第一口吃下去，真不错 ","date":"2023-08-06","objectID":"/sanyangcai/:3:3","tags":["重庆菜","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":["生活"],"content":"总结 个人感觉偏重庆那边的风格，够麻够辣，有机会还是要多出去看看。吃完了还是有个疑问，三样菜到底是那三样？ ","date":"2023-08-06","objectID":"/sanyangcai/:4:0","tags":["重庆菜","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-08-06","objectID":"/sanyangcai/:5:0","tags":["重庆菜","川菜"],"title":"三样菜","uri":"/sanyangcai/"},{"categories":["Code"],"content":"SQL验证完成后，需要将SQL语法树转换成Operation的集合 public List\u003cOperation\u003e parse(String statement) { ... return Collections.singletonList( SqlNodeToOperationConversion.convert(planner, catalogManager, parsed.get(0)) .orElseThrow(() -\u003e new TableException(\"Unsupported query: \" + statement))); } 在这里将已经验证好的SQL 转换成Oper的过程，就会使用到SqlNodeToOperationConversion.convert ","date":"2023-07-15","objectID":"/operation1/:0:0","tags":["flink","sql"],"title":"FlinkSQL - Operation","uri":"/operation1/"},{"categories":["Code"],"content":"SqlNodeToOperationConversion.convert public static Optional\u003cOperation\u003e convert( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode sqlNode) { // validate the query final SqlNode validated = flinkPlanner.validate(sqlNode); return convertValidatedSqlNode(flinkPlanner, catalogManager, validated); } convertValidatedSqlNode 就是转换Operation的入口，一共有三个参数 flinkPlanner: FlinkPlannerImpl 优化器，将SQL语法树生成flink的执行计划 catalogManager: CatalogManager 当前作业的库表等信息的管理 validated: SqlNode 已经检验好的SQL语法树 ","date":"2023-07-15","objectID":"/operation1/:1:0","tags":["flink","sql"],"title":"FlinkSQL - Operation","uri":"/operation1/"},{"categories":["Code"],"content":"convert 参数 FlinkPlannerImpl 在实际运行中, flinkPlanner 是通过 FlinkPlannerImpl 实例实现的，具体是在 PlannerContext进行实现的 package org.apache.flink.table.planner.delegation; public class PlannerContext { ... public FlinkPlannerImpl createFlinkPlanner() { return new FlinkPlannerImpl( createFrameworkConfig(), this::createCatalogReader, typeFactory, cluster); } ... } 一套flink 使用的schema，类型系统以及SQL优化器等组件的framework配置 一个读取flink的catalog的入口实例 生成flink类型的工厂实例 专门为Flink服务的 calcite 关系优化cluster planner 的作用就是用于将 FlinkSQL 转换成Flink 可以识别的DAG作业，运行到Flink 环境中 CatalogManager CatalogManager 隐藏的很深，在最初创建的时候他就已经生成了。如果没有特殊要求，catalog 回创建一个名为 “default_catalog” 的catalog，在此之下，还会创建一个 名为 “default_database”的数据库 通常情况下，这里的catalog是不需要我们修改的，在flink UI 界面里我们也经常可以看到，所有的表都是放在这个“default_catlog.default_database\"下的 流式任务的创建的代码是在 org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl 下的 package org.apache.flink.table.api.bridge.scala.internal ... object StreamTableEnvironmentImpl { ... val catalogManager = CatalogManager.newBuilder .classLoader(userClassLoader) .config(tableConfig) .defaultCatalog( settings.getBuiltInCatalogName, new GenericInMemoryCatalog(settings.getBuiltInCatalogName, settings.getBuiltInDatabaseName)) .executionConfig(executionEnvironment.getConfig) .catalogModificationListeners(TableFactoryUtil .findCatalogModificationListenerList(tableConfig.getConfiguration, userClassLoader)) .catalogStoreHolder( CatalogStoreHolder .newBuilder() .catalogStore(catalogStore) .factory(catalogStoreFactory) .config(tableConfig) .classloader(userClassLoader) .build()) .build ... } 这里的所有参数都是默认值 ","date":"2023-07-15","objectID":"/operation1/:1:1","tags":["flink","sql"],"title":"FlinkSQL - Operation","uri":"/operation1/"},{"categories":["Code"],"content":"SQL验证 写入的SQL语句 会先进行验证，通过Flink自己写的Planner，设置自己的规则然后借助Calicte 进行验证 具体验证过程: FlinkSQL - 验证过程 ","date":"2023-07-15","objectID":"/operation1/:2:0","tags":["flink","sql"],"title":"FlinkSQL - Operation","uri":"/operation1/"},{"categories":["Code"],"content":"convertValidatedSqlNode 从这里开始SqlNode进入后就要转换成Operation了 package org.apache.flink.table.planner.operations; public class SqlNodeToOperationConversion { ... private static Optional\u003cOperation\u003e convertValidatedSqlNode( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode validated) { ... } ... } 该方法接收3个参数： flinkPlanner: 就是从上层传进来的执行计划 catalogManager: 用来管理数据的schema validated: 验证后的Sql语句 private static Optional\u003cOperation\u003e convertValidatedSqlNode( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode validated) { beforeConversion(); // delegate conversion to the registered converters first SqlNodeConvertContext context = new SqlNodeConvertContext(flinkPlanner, catalogManager); Optional\u003cOperation\u003e operation = SqlNodeConverters.convertSqlNode(validated, context); if (operation.isPresent()) { return operation; } ... } 首先做一些前置工作 beforeConversion： 清除行级修改上下文，在普通的SQL作业中这一步没什么变化 如果SQL之前注册过，那么就返回之前的SQL，这里因为第一次启动，之前也没有任务，所以也不会直接返回 ","date":"2023-07-15","objectID":"/operation1/:3:0","tags":["flink","sql"],"title":"FlinkSQL - Operation","uri":"/operation1/"},{"categories":["Code"],"content":"Q\u0026A 为什么 parse 方法返回的是 Operation的集合呢？ ","date":"2023-07-15","objectID":"/operation1/:4:0","tags":["flink","sql"],"title":"FlinkSQL - Operation","uri":"/operation1/"},{"categories":["Code"],"content":"简介 Paimon 是一个流数据湖平台 ","date":"2023-07-15","objectID":"/paimon1/:1:0","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"数据入湖 实时集成 数据库整库同步 schema变更同步（表结构变更） 部分列更新 批量覆盖 ","date":"2023-07-15","objectID":"/paimon1/:1:1","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"支持 增量快照 产生changelog lookup join batch/OLAP 查询 ","date":"2023-07-15","objectID":"/paimon1/:1:2","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"核心特性 统一批处理和流处理 批量写入和读取，流式更新，变更日志生成 数据湖能力 低成本、高可靠性、可扩展的元数据 各种合并引擎 按照要求更新记录 变更日志生成 多种表类型 schema 变更 重命名列 一致性保证 使用两阶段提交 每次提交在提交时最多生成2个快照 桶是读写的最小单元 两个writer如果修改的不是同一个桶，则提交是有序的 如果两个writer修改同一个桶，保证快照隔离，不会丢失更改信息，可能会合并两次提交，混合起来 ","date":"2023-07-15","objectID":"/paimon1/:1:3","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"文件布局 快照文件(Snapshot Files) 快照目录存储了所有的快照文件 快照文件是JSON文件 一个快照文件包含 若干使用的schema文件 快照所有更改的manifest lists 清单文件(Manifest Files) 清单目录存储了清单列表(manifest lists)和清单文件(manifest files) 清单列表保存的是清单文件的文件名 清单文件包含 LSM数据文件和 changelog 文件的变化(例如快照中LSM数据的创建或者是文件的删除) 数据文件(Data Files) 数据文件会进行分区和分桶 一个分区下面会有多个分桶(目录) 一个分桶里包含LSM数据 以及 changelog 默认使用orc文件，支持parquet,avro LSM Trees paimon 的数据是由LSM Tree 进行组织的 Sorted Runs LSM Tree 将文件组织成若干个 Sorted Runs 一个 sorted run 包含一个或多个数据文件，每个数据文件只属于一个sorted run 数据文件中的记录按照主键排序 同一个 sorted run 中，数据文件的主键范围不会重叠 不同的 sorted run 中，数据文件的主键范围是有可能重叠的，因此在查询LSM Tree 的时候，需要合并所有的 sorted runs 中相同主键的记录，按照用户指定的合并引擎 和时间戳 LSM Tree 的新纪录会先写入内存，内存缓冲区满了之后会刷新到磁盘，创建新的sorted run Compaction 越来越多的记录写入LSM Tree 后，sorted run 就会增多 查询LSM Tree时需要将所有的 sorted run 都合并起来，过多的sorted run 就会导致查询性能下降，或者OOM 将多个sorted run 合并成一个大的 sorted run 的这个过程被称之为 Compaction 过于频繁的 Compaction也会消耗一定的cpu和磁盘io， paimon 使用与rocksdb类似的通用压缩 Compaction策略 可以指定 专用compaction作业(dedicated compaction job) 进行 compaction ","date":"2023-07-15","objectID":"/paimon1/:1:4","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"catalog 文件系统 paimon catalog 可以持久化元数据 支持两种metastore 文件系统，file HDFS 等 Hive,使用 hive metastore 存放元数据 CREATE CATALOG my_hive WITH ( 'type' = 'paimon', 'metastore' = 'hive', 'uri' = 'thrift://\u003chive-metastore-host-name\u003e:\u003cport\u003e', -- 'hive-conf-dir' = '...', this is recommended in the kerberos environment 'warehouse' = 'hdfs:///path/to/table/store/warehouse' ); ","date":"2023-07-15","objectID":"/paimon1/:2:0","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"上手 目前 paimon 提供了两个的jar包 boudled jar 用于 读写数据，也就是启动flink任务，with 参数等 action jar 用于 各种操作，如手动compaction等， 这个可以在任务之外进行操作 -- if you're trying out Paimon in a distributed environment, -- the warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( 'type'='paimon', 'warehouse'='file:/tmp/paimon' ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); -- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( 'connector' = 'datagen', 'fields.word.length' = '1' ); -- paimon requires checkpoint interval in streaming mode SET 'execution.checkpointing.interval' = '10 s'; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; 使用paimon后，该目录如下 ","date":"2023-07-15","objectID":"/paimon1/:3:0","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"default.db default.db 为默认的数据库，就是catalog下的数据库目录 ","date":"2023-07-15","objectID":"/paimon1/:3:1","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"word_count word_cound 是在数据库下创建的表目录 ","date":"2023-07-15","objectID":"/paimon1/:3:2","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"bucket-0 bucket-x 就是 真正存储数据的地方，数据多了之后还会有 bucket-1 bucket-2 等 是一系列orc 后缀的文件 ","date":"2023-07-15","objectID":"/paimon1/:3:3","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"schema schema 也是json 文件，而且会有多个,主要也是为了保存变更的schema { \"id\" : 0, \"fields\" : [ { \"id\" : 0, \"name\" : \"word\", \"type\" : \"STRING NOT NULL\" }, { \"id\" : 1, \"name\" : \"cnt\", \"type\" : \"BIGINT\" } ], \"highestFieldId\" : 1, \"partitionKeys\" : [ ], \"primaryKeys\" : [ \"word\" ], \"options\" : { } } ","date":"2023-07-15","objectID":"/paimon1/:3:4","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"snapshot 快照目录保存了很多的文件id，通过这些id 可以更快找到 清单文件和schema { \"version\" : 3, \"id\" : 15, \"schemaId\" : 0, \"baseManifestList\" : \"manifest-list-f8c8e502-67ed-4b4f-9d57-bd1ff93943e6-28\", \"deltaManifestList\" : \"manifest-list-f8c8e502-67ed-4b4f-9d57-bd1ff93943e6-29\", \"changelogManifestList\" : null, \"commitUser\" : \"a196f82d-68d9-416b-a941-3b478cb0ff9b\", \"commitIdentifier\" : 13, \"commitKind\" : \"APPEND\", \"timeMillis\" : 1692273479392, \"logOffsets\" : { }, \"totalRecordCount\" : 400, \"deltaRecordCount\" : 16, \"changelogRecordCount\" : 0, \"watermark\" : -9223372036854775808 } ","date":"2023-07-15","objectID":"/paimon1/:3:5","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"manifest 清单目录里有很多清单文件，文件不能完全打开，但是可以看到内部是有数据文件的id等信息，应该是数据变更的信息内容 ","date":"2023-07-15","objectID":"/paimon1/:3:6","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"数据类型 支持了基本上所有的Flink 的数据类型，除了 multiset map类型不支持主键 ","date":"2023-07-15","objectID":"/paimon1/:4:0","tags":["flink","paimon"],"title":"Paimon","uri":"/paimon1/"},{"categories":["Code"],"content":"当 SqlValidator解析完作用域信息之后，紧接着就开始对类型进行验证 @Override public SqlNode validate(SqlNode topNode) { SqlValidatorScope scope = new EmptyScope(this); scope = new CatalogScope(scope, ImmutableList.of(\"CATALOG\")); final SqlNode topNode2 = validateScopedExpression(topNode, scope); final RelDataType type = getValidatedNodeType(topNode2); Util.discard(type); return topNode2; } ","date":"2023-06-15","objectID":"/flink-validate-type/:0:0","tags":["flink","calcite","sql"],"title":"FlinkSQL - 类型验证","uri":"/flink-validate-type/"},{"categories":["Code"],"content":"getValidatedNodeType 每个SqlNode 都会设定一个类型比如， SELECT * FROM table WHERE column1 + column2 = 10 在解析完成后，“*” 会被解开变成user, product, amoutn， 就会被设定成这样一个类型 RecordType(BIGINT user, VARCHAR(2147483647) product, INTEGER amount) @Override public RelDataType getValidatedNodeType(SqlNode node) { RelDataType type = getValidatedNodeTypeIfKnown(node); if (type == null) { if (node.getKind() == SqlKind.IDENTIFIER) { throw newValidationError(node, RESOURCE.unknownIdentifier(node.toString())); } throw Util.needToImplement(node); } else { return type; } } @Override public @Nullable RelDataType getValidatedNodeTypeIfKnown(SqlNode node) { final RelDataType type = nodeToTypeMap.get(node); if (type != null) { return type; } final SqlValidatorNamespace ns = getNamespace(node); if (ns != null) { return ns.getType(); } final SqlNode original = originalExprs.get(node); if (original != null \u0026\u0026 original != node) { return getValidatedNodeType(original); } if (node instanceof SqlIdentifier) { return getCatalogReader().getNamedType((SqlIdentifier) node); } return null; } 这里的 nodeToTypeMap 是在validateScopedExpression 这一步就确定了的，整个过程是在 验证命名空间的过程汇总就写入了，当然也是递归的过程 type 类型推断 是通过 SqlValidatorScope 实现这个接口实现的 每个SqlNode 都对应一个 Scope 在这些Scope中定义规则 /** Derives the type of a node, never null. */ RelDataType deriveTypeImpl(SqlValidatorScope scope, SqlNode operand) { DeriveTypeVisitor v = new DeriveTypeVisitor(scope); final RelDataType type = operand.accept(v); return requireNonNull(scope.nullifyType(operand, type)); } 于是，在 SqlValidatorImpl中可以找到事如何推导出这些类型是如何生成的 private class DeriveTypeVisitor implements SqlVisitor\u003cRelDataType\u003e { private final SqlValidatorScope scope; DeriveTypeVisitor(SqlValidatorScope scope) { this.scope = scope; } @Override public RelDataType visit(SqlLiteral literal) { return literal.createSqlType(typeFactory); } @Override public RelDataType visit(SqlCall call) { final SqlOperator operator = call.getOperator(); return operator.deriveType(SqlValidatorImpl.this, scope, call); } @Override public RelDataType visit(SqlNodeList nodeList) { // Operand is of a type that we can't derive a type for. If the // operand is of a peculiar type, such as a SqlNodeList, then you // should override the operator's validateCall() method so that it // doesn't try to validate that operand as an expression. throw Util.needToImplement(nodeList); } @Override public RelDataType visit(SqlIdentifier id) { // First check for builtin functions which don't have parentheses, // like \"LOCALTIME\". final SqlCall call = makeNullaryCall(id); if (call != null) { return call.getOperator().validateOperands(SqlValidatorImpl.this, scope, call); } RelDataType type = null; if (!(scope instanceof EmptyScope)) { id = scope.fullyQualify(id).identifier; } // Resolve the longest prefix of id that we can int i; for (i = id.names.size() - 1; i \u003e 0; i--) { // REVIEW jvs 9-June-2005: The name resolution rules used // here are supposed to match SQL:2003 Part 2 Section 6.6 // (identifier chain), but we don't currently have enough // information to get everything right. In particular, // routine parameters are currently looked up via resolve; // we could do a better job if they were looked up via // resolveColumn. final SqlNameMatcher nameMatcher = catalogReader.nameMatcher(); final SqlValidatorScope.ResolvedImpl resolved = new SqlValidatorScope.ResolvedImpl(); scope.resolve(id.names.subList(0, i), nameMatcher, false, resolved); if (resolved.count() == 1) { // There's a namespace with the name we seek. final SqlValidatorScope.Resolve resolve = resolved.only(); type = resolve.rowType(); for (SqlValidatorScope.Step p : Util.skip(resolve.path.steps())) { type = type.getFieldList().get(p.i).getType(); } break; } } // Give precedence to namespace found, unless there // are no more identifier components. if (type == null || id.names.size() == 1) { // See if there's a column with the name we seek in // precisely one of the namespaces in this scope. RelDataType colType = scope.resolveColumn(id.names.get(0), id); if (colType !=","date":"2023-06-15","objectID":"/flink-validate-type/:1:0","tags":["flink","calcite","sql"],"title":"FlinkSQL - 类型验证","uri":"/flink-validate-type/"},{"categories":["Code"],"content":" 协议 线数 通信类型 多主 数据率 总线上期间的数量 线缆长度(米) 工作模式 使用距离 $$UART$$ 2 异步 不支持 3Kbps 到 4Mbps 2 1.5 全双工 远距离 $$SPI$$ 3 同步 不支持 1Mbps \u003c 10 \u003c3 全双工 近距离低速 $$I^2C$$ 2 同步 支持 \u003c3.4Mbps \u003c 10 \u003c3 半双工 近距离低速 举例说明： I2C的使用场景：连接传感器、存储器、显示器等设备。例如，连接温度传感器、EEPROM存储器、OLED显示器等。 UART的使用场景：连接串口设备，如调试器、GPS接收器等。例如，连接串口调试器进行程序调试。 SPI的使用场景：连接存储器、显示器、传感器等设备。例如，连接Flash存储器、LCD显示器、加速度传感器等。 选择使用这三者的场景取决于具体的应用需求。 如果需要连接多个设备，可以选择使用I2C或SPI协议； 如果只需要连接单个设备，可以选择使用UART协议。 如果需要高速传输数据，可以选择使用SPI协议。 如果需要低功耗和简单的通信方式，可以选择使用I2C协议。 如果需要长距离传输数据，可以选择使用UART协议。 ","date":"2023-06-13","objectID":"/transmitter/:0:0","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":["Code"],"content":"UART UART（Universal Asynchronous Receiver/Transmitter）：UART是一种异步串行通信协议，用于在计算机和外部设备之间传输数据。它使用两根线（TX和RX）进行通信，其中TX是发送线，RX是接收线。UART协议只支持单主机和单从机的通信，不能连接多个设备。UART通常用于连接串口设备，如调试器、GPS接收器等。 ","date":"2023-06-13","objectID":"/transmitter/:1:0","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":["Code"],"content":"流控制 流控制的方式分别有软件和硬件两种。 软件的流控制方式，在UART通信中，只需RxD、TxD、GND三根即可，数据在传输过程中，依靠代码的判断处理，并通过收发双方进行的数据交互完成控制，在现有通信物理信号线基础上，使用控制字符(ASCII表中的0x00~0x0x1F、0x7F)完成控制指令的交互。一般在私有协议下也会定义一些特殊字符设为控制指令。 硬件的流控制方式，即在原有的RxD、TxD、GND三根信号线的基础上，再增加RTS/CTS和DTR/DSR这两组信号线。第一组线是RTS（Request toSend）和CTS（Clear toSend）。当接收方准备好接收数据，它置高RTS线表示它准备好了，如果发送方也就绪，它置高CTS，表示它即将发送数据。第二组线是DTR（DataTerminal Ready）和DSR（Data SetReady）。这些线主要用于Modem通信。使得串口和Modem通信他们的状态。例如：当Modem已经准备好接收来自PC的数据，它置高DTR线，表示和电话线的连接已经建立。读取DSR线置高，PC机开始发送数据。一个简单的规则是DTR/DSR用于表示系统通信就绪，而RTS/CTS用于单个数据包的传输。 ","date":"2023-06-13","objectID":"/transmitter/:1:1","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":["Code"],"content":"优点 只使用两根电线，不需要时钟信号 有一个奇偶校验位，只要双方设置后，就可以改变数据包的结构 ","date":"2023-06-13","objectID":"/transmitter/:1:2","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":["Code"],"content":"缺点 数据帧的大小限制为最多9位，不支持多个从属或多个主系统 每个UART的波特率必须在10％之内 ","date":"2023-06-13","objectID":"/transmitter/:1:3","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":["Code"],"content":"SPI SPI（Serial Peripheral Interface）：SPI是一种串行通信协议，用于在芯片之间传输数据。它使用四根线（MOSI、MISO、SCK和SS）进行通信，其中MOSI是主设备输出从设备输入的数据线，MISO是主设备输入从设备输出的数据线，SCK是时钟线，SS是片选线。SPI协议支持多主机和多从机的通信，可以连接多个设备。SPI通常用于连接存储器、显示器、传感器等设备。 ","date":"2023-06-13","objectID":"/transmitter/:2:0","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":["Code"],"content":"SPI使用的四根信号线 SCLK: Serial Clock (output from master)：串行时钟，用来同步数据传输，由主机输出； MOSI \\ SIMO: Master Output, Slave Input(output from master)：主机输出从机输入数据线，通常先传输MSB； MISO \\ SOMI: Master Input, Slave Output(output from slave)：主机输入从机输出数据线，通常先传输LSB； SS: Slave Select (active low, output from master)：片选线，低电平有效，由主机输出。 SS\\CS：控制芯片是否被选中的，也就是说只有片选信号为预先规定的使能信号时（一般默认为低电位），对此芯片的操作才有效，这就允许在同一总线上连接多个SPI设备成为可能。也就是说：当有多个从设备的时候，因为每个从设备上都有一个片选引脚接入到主设备机中，当我们的主设备和某个从设备通信时将需要将从设备对应的片选引脚电平拉低。 ","date":"2023-06-13","objectID":"/transmitter/:2:1","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":["Code"],"content":"SPI的四种操作模式 SPI的四种操作模式，它们的区别是定义了在时钟脉冲的哪条边沿转换（toggles）输出信号，哪条边沿采样输入信号，还有时钟脉冲的稳定电平值（就是时钟信号无效时是高还低）。 对于STM32等MCU自带的硬件SPI外设来说，可能没有那么重要，只需要配置一下模式就行了，但是对于使用使用GPIO模拟或者FPGA来实现SPI的时序，这一点是非常非常重要的。 Master 设备会根据将要交换的数据来产生相应的时钟脉冲(Clock Pulse)，时钟脉冲组成了时钟信号(Clock Signal) ，每种模式由时钟信号中的时钟极性（clock polarity）CPOL与时钟周期（clock phase）CPHA来定义。 不同的从设备可能在出厂是就是配置为某种模式，这是不能改变的，但我们的通信双方必须是工作在同一模式下，所以我们可以对我们的主设备的SPI模式进行配置，从而实现主从通讯。 时钟极性CPOL是用来配置SCLK的电平出于哪种状态时是空闲态或者有效态；时钟相位CPHA是用来配置数据采样是在第几个边沿。 CPOL=0，表示当SCLK=0时处于空闲态，所以有效状态就是SCLK处于高电平时； CPOL=1，表示当SCLK=1时处于空闲态，所以有效状态就是SCLK处于低电平时； CPHA=0，表示数据采样是在第1个边沿，数据发送在第2个边沿； CPHA=1，表示数据采样是在第2个边沿，数据发送在第1个边沿。 在高电平有效状态时，第一边沿为上升沿，第二边沿为下降沿；在低电平有效状态时，第一边沿为下降沿，第二边沿为上升沿 具体四种模式如下： CPOL = 0，CPHA = 0：时钟高电平时为有效状态，时钟上升沿（第一个边沿）采样。 CPOL = 0，CPHA = 1：时钟高电平时为有效状态，时钟下降沿（第二个边沿）采样。 CPOL = 1，CPHA = 0：时钟低电平时为有效状态，时钟下降沿（第一个边沿）采样。 CPOL = 1，CPHA = 1：时钟低电平时为有效状态，时钟上升沿（第二个边沿）采样。 ","date":"2023-06-13","objectID":"/transmitter/:2:2","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":["Code"],"content":"I2C I2C（Inter-Integrated Circuit）：I2C是一种串行通信协议，用于在芯片之间传输数据。它使用两根线（SDA和SCL）进行通信，其中SDA是数据线，SCL是时钟线。I2C协议支持多主机和多从机的通信，可以连接多个设备。I2C通常用于连接传感器、存储器、显示器等设备。 IIC 是多主设备的总线，IIC没有物理的芯片选择信号线，没有仲裁逻辑电路，只使用serial data (SDA)数据线 和 serial clock(SCL)时钟线两条信号线，数据线用来传输数据，时钟线用来同步数据收发。两根信号线都是双向传输的，这两条线都是漏极开路或者集电极开路结构，使用时需要外加上拉电阻，可以挂载多个设备。 ","date":"2023-06-13","objectID":"/transmitter/:3:0","tags":["uart","i2c","spi"],"title":"UART, I2C, SPI","uri":"/transmitter/"},{"categories":["Code"],"content":"当Sql语句转换为SqlNode 之后 就会进入下一个阶段：将SqlNode 转换成 Operations @Override public List\u003cOperation\u003e parse(String statement) { CalciteParser parser = calciteParserSupplier.get(); FlinkPlannerImpl planner = validatorSupplier.get(); Optional\u003cOperation\u003e command = EXTENDED_PARSER.parse(statement); if (command.isPresent()) { return Collections.singletonList(command.get()); } // parse the sql query // use parseSqlList here because we need to support statement end with ';' in sql client. SqlNodeList sqlNodeList = parser.parseSqlList(statement); List\u003cSqlNode\u003e parsed = sqlNodeList.getList(); Preconditions.checkArgument(parsed.size() == 1, \"only single statement supported\"); return Collections.singletonList( SqlNodeToOperationConversion.convert(planner, catalogManager, parsed.get(0)) .orElseThrow(() -\u003e new TableException(\"Unsupported query: \" + statement))); } 其中 SqlNodeToOperationConversion.convert方法，就算是将 SqlNode转换成 Operation SqlNod会先进行验证工作，然后再进行转换 public static Optional\u003cOperation\u003e convert( FlinkPlannerImpl flinkPlanner, CatalogManager catalogManager, SqlNode sqlNode) { // validate the query final SqlNode validated = flinkPlanner.validate(sqlNode); return convertValidatedSqlNode(flinkPlanner, catalogManager, validated); } ","date":"2023-06-06","objectID":"/flink-sql-validate/:0:0","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":["Code"],"content":"为什么 SqlNode 需要验证 目的当然是为了确保SQL语句的正确性和合法性，避免在后续的执行过程中出现错误 举例来说，假设有以下的SQL语句： SELECT name, age FROM student WHERE age \u003e 18 AND gender = 'male' 在进行转换成Operation的过程中，需要先进行validate，检查SQL语句是否符合语法规则和语义规则。如果SQL语句中存在语法错误或者语义错误，那么validate过程会抛出异常，提示用户需要修改SQL语句 例如，如果SQL语句中存在以下错误： SELECT name, age FROM student WHERE age \u003e 'two' AND gender = 'male' 其中，age的类型是整数，但是在SQL语句中使用了字符串类型的’two’进行比较，这是一个语义错误。在进行validate的过程中，会检测到这个错误并抛出异常，提示用户需要修改SQL语句中的比较条件 因此，先进行validate的过程可以帮助用户在SQL语句执行之前就发现错误，避免在后续的执行过程中出现问题，提高SQL语句的执行效率和准确性 ","date":"2023-06-06","objectID":"/flink-sql-validate/:1:0","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":["Code"],"content":"validate validate的过程的输入和输出都是SqlNode，除了检查类型之外还会更改SqlNode的节点信息 通过FlinkPlanner 对SqlNode进行检查 def validate(sqlNode: SqlNode): SqlNode = { val validator = getOrCreateSqlValidator() validate(sqlNode, validator) } FlinkCalciteSqlValidator 实现 了Calicte 的SqlValidatorImpl 作为validator，就是用来检查FlinkSql的Sql语法的 /** Extends Calcite's {@link SqlValidator} by Flink-specific behavior. */ @Internal public final class FlinkCalciteSqlValidator extends SqlValidatorImpl { // Enables CallContext#getOutputDataType() when validating SQL expressions. private SqlNode sqlNodeForExpectedOutputType; private RelDataType expectedOutputType; public FlinkCalciteSqlValidator( SqlOperatorTable opTab, SqlValidatorCatalogReader catalogReader, RelDataTypeFactory typeFactory, SqlValidator.Config config) { super(opTab, catalogReader, typeFactory, config); } ","date":"2023-06-06","objectID":"/flink-sql-validate/:2:0","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":["Code"],"content":"创建 validator 第一次写入Sql时，会新建一个validator，后面再创建就会使用一个单例来处理 def getOrCreateSqlValidator(): FlinkCalciteSqlValidator = { if (validator == null) { val catalogReader = catalogReaderSupplier.apply(false) validator = createSqlValidator(catalogReader) } validator } 其中，catalogReaderSupplier是一个函数，目的是读取和解析数据库中的元信息，比如数据库，表，列等信息 函数的输入是一个bool值，用来确定大小写是否敏感；输出是FlinkCalciteCatalogReader FlinkCalciteCatalogReader相比于CalciteCatalogReader，包含了Flink特有的信息，例如：Flink的UDF函数信息和Flink的Table信息 FlinkCalciteSqlValidator 通过数据元信息，就可以创建 SqlValidator private def createSqlValidator(catalogReader: CalciteCatalogReader) = { val validator = new FlinkCalciteSqlValidator( operatorTable, catalogReader, typeFactory, SqlValidator.Config.DEFAULT .withIdentifierExpansion(true) .withDefaultNullCollation(FlinkPlannerImpl.defaultNullCollation) .withTypeCoercionEnabled(false) ) // Disable implicit type coercion for now. validator } 创建SqlValidator 需要以下信息 SqlOperatorTable, 定义了Sql操作符和方法，也可以查找这些操作符和方法 这个SqlOperatorTable是在创建 StreamTableEnvironment的时候就创建了，具体是在 // org.apache.flink.table.planner.delegation.PlannerContext public FrameworkConfig createFrameworkConfig() { return Frameworks.newConfigBuilder() .defaultSchema(rootSchema.plus()) .parserConfig(getSqlParserConfig()) .costFactory(new FlinkCostFactory()) .typeSystem(typeSystem) .convertletTable(FlinkConvertletTable.INSTANCE) .sqlToRelConverterConfig(getSqlToRelConverterConfig()) .operatorTable(getSqlOperatorTable(getCalciteConfig())) // 这里创建 // set the executor to evaluate constant expressions .executor( new ExpressionReducer( context.getTableConfig(), context.getClassLoader(), false)) .context(context) .traitDefs(traitDefs) .build(); } 一般来说 opTab 里包含的都是 Calcite自带的操作符 比如 not like, is not null 这类 SqlValidatorCatalogReader, 就是上面所说的FlinkCalciteCatalogReader，保存的SQL的元数据 RelDataTypeFactory, FlinkSQL的类型，连接 Flink 的 LogicalType 和 Calcite 的 RelDataType。 RelDataType 是 RelNode关系代数中的数据类型，使用Calcite解析SQL的时候内部会转换成这种类型 /** * Flink specific type factory that represents the interface between Flink's [[LogicalType]] and * Calcite's [[RelDataType]]. */ class FlinkTypeFactory( classLoader: ClassLoader, typeSystem: RelDataTypeSystem = FlinkTypeSystem.INSTANCE) extends JavaTypeFactoryImpl(typeSystem) with ExtendedRelTypeFactory { private val seenTypes = mutable.HashMap[LogicalType, RelDataType]() /** * Create a calcite field type in table schema from [[LogicalType]]. It use PEEK_FIELDS_NO_EXPAND * when type is a nested struct type (Flink [[RowType]]). * * @param t * flink logical type. * @return * calcite [[RelDataType]]. */ def createFieldTypeFromLogicalType(t: LogicalType): RelDataType = { def newRelDataType(): RelDataType = t.getTypeRoot match { case LogicalTypeRoot.NULL =\u003e createSqlType(NULL) case LogicalTypeRoot.BOOLEAN =\u003e createSqlType(BOOLEAN) case LogicalTypeRoot.TINYINT =\u003e createSqlType(TINYINT) case LogicalTypeRoot.SMALLINT =\u003e createSqlType(SMALLINT) case LogicalTypeRoot.INTEGER =\u003e createSqlType(INTEGER) case LogicalTypeRoot.BIGINT =\u003e createSqlType(BIGINT) case LogicalTypeRoot.FLOAT =\u003e createSqlType(FLOAT) case LogicalTypeRoot.DOUBLE =\u003e createSqlType(DOUBLE) case LogicalTypeRoot.VARCHAR =\u003e createSqlType(VARCHAR, t.asInstanceOf[VarCharType].getLength) case LogicalTypeRoot.CHAR =\u003e createSqlType(CHAR, t.asInstanceOf[CharType].getLength) // temporal types case LogicalTypeRoot.DATE =\u003e createSqlType(DATE) case LogicalTypeRoot.TIME_WITHOUT_TIME_ZONE =\u003e createSqlType(TIME) // interval types case LogicalTypeRoot.INTERVAL_YEAR_MONTH =\u003e createSqlIntervalType( new SqlIntervalQualifier(TimeUnit.YEAR, TimeUnit.MONTH, SqlParserPos.ZERO)) case LogicalTypeRoot.INTERVAL_DAY_TIME =\u003e createSqlIntervalType( new SqlIntervalQualifier(TimeUnit.DAY, TimeUnit.SECOND, SqlParserPos.ZERO)) case LogicalTypeRoot.BINARY =\u003e createSqlType(BINARY, t.asInstanceOf[BinaryType].getLength) case LogicalTypeRoot.VARBINARY =\u003e createSqlType(VARBINARY, t.asInstanceOf[VarBinaryType].getLength) case LogicalTypeRoot.DECIMAL =\u003e t match { case decimalType: DecimalType =\u003e createSqlType(DECIMAL, de","date":"2023-06-06","objectID":"/flink-sql-validate/:2:1","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":["Code"],"content":"开始验证 创建validator 后 根据validate 来验证 sqlNode 重写 为什么要重写？ 重写相当于是在SqlNode进行一些预处理主要作用是检查查询语句是否符合Flink Table的语法规范，并对查询语句进行一些必要的转换和优化，以便更好地支持Flink的执行引擎 Flink中重写是依靠PreValidateReWriter来实现的 sqlNode.accept(new PreValidateReWriter(validator, typeFactory)) 如果是简单的SQL其实不需要重写 override def visit(call: SqlCall): Unit = { call match { case e: SqlRichExplain =\u003e e.getStatement match { case r: RichSqlInsert =\u003e rewriteInsert(r) case _ =\u003e // do nothing } case r: RichSqlInsert =\u003e rewriteInsert(r) case _ =\u003e // do nothing } } validator验证SqlNode 如果不是特殊的SQL，应该都会使用 validator来验证SqlNode case _ =\u003e validator.validate(sqlNode) // org.apache.calcite.sql.validate.SqlValidatorImpl; @Override public SqlNode validate(SqlNode topNode) { SqlValidatorScope scope = new EmptyScope(this); scope = new CatalogScope(scope, ImmutableList.of(\"CATALOG\")); final SqlNode topNode2 = validateScopedExpression(topNode, scope); final RelDataType type = getValidatedNodeType(topNode2); Util.discard(type); return topNode2; } SqlValidatorScope SqlValidatorScope是Calcite中的一个接口，用于表示SQL语句中的作用域。它包含了当前作用域中可见的所有表、列、函数等信息。 具体来说，SqlValidatorScope中包含了以下信息： 当前作用域中可见的所有表，包括别名和表的元数据信息。 当前作用域中可见的所有列，包括别名和列的元数据信息。 当前作用域中可见的所有函数，包括函数的元数据信息和参数信息。 当前作用域中可见的所有变量，包括变量的类型和值。 举例说明，假设有以下SQL语句： SELECT a.name, b.salary FROM employee a JOIN salary b ON a.id = b.id WHERE b.salary \u003e 5000; 在这个SQL语句中，作用域可以分为以下几个部分： SELECT子句中的作用域，包括a.name和b.salary两个列。 FROM子句中的作用域，包括employee和salary两个表。 JOIN子句中的作用域，包括a和b两个表的别名。 WHERE子句中的作用域，包括b.salary列和5000常量。 在每个作用域中，SqlValidatorScope都会包含相应的表、列、函数等信息，以便进行语法和语义的验证。 SqlValidatorScope 本身也是一个树形结构，根节点是一个空scope validateScopedExpression 验证过程的第一步就是重写SqlNode，将SqlNode中的不确定的地方都进行更新 然后如果是Select这种简单的SQL，会将查询进行注册 private SqlNode validateScopedExpression(SqlNode topNode, SqlValidatorScope scope) { SqlNode outermostNode = performUnconditionalRewrites(topNode, false); cursorSet.add(outermostNode); top = outermostNode; TRACER.trace(\"After unconditional rewrite: {}\", outermostNode); if (outermostNode.isA(SqlKind.TOP_LEVEL)) { registerQuery(scope, null, outermostNode, outermostNode, null, false); } outermostNode.validate(this, scope); if (!outermostNode.isA(SqlKind.TOP_LEVEL)) { // force type derivation so that we can provide it to the // caller later without needing the scope deriveType(scope, outermostNode); } TRACER.trace(\"After validation: {}\", outermostNode); return outermostNode; } performUnconditionalRewrites 这里大部分都是递归调用，能够实际重写的大部分都是函数调用 if (call.getOperator() instanceof SqlUnresolvedFunction) { assert call instanceof SqlBasicCall; final SqlUnresolvedFunction function = (SqlUnresolvedFunction) call.getOperator(); // This function hasn't been resolved yet. Perform // a half-hearted resolution now in case it's a // builtin function requiring special casing. If it's // not, we'll handle it later during overload resolution. final List\u003cSqlOperator\u003e overloads = new ArrayList\u003c\u003e(); opTab.lookupOperatorOverloads( function.getNameAsId(), function.getFunctionType(), SqlSyntax.FUNCTION, overloads, catalogReader.nameMatcher()); if (overloads.size() == 1) { ((SqlBasicCall) call).setOperator(overloads.get(0)); } } SqlNode的lookupOperatorOverloads方法主要是用于查找可用的操作符重载。具体来说，它会根据传入的操作符名称和参数类型，查找符合条件的操作符重载方法。 举个例子，假设有以下的SqlNode： SELECT * FROM table WHERE column1 + column2 = 10 在这个SqlNode中，有一个加法操作符“+”，它的左右两边分别是column1和column2这两个列。因此，lookupOperatorOverloads方法会首先根据“+”操作符名称查找可用的操作符重载方法。如果找到了多个重载方法，它会根据参数类型进一步筛选出符合条件的重载方法。 假设我们定义了以下的操作符重载方法： public static int operator +(int a, int b) {...} public static double operator +(double a, double b) {...} public static string operator +(string a, string b) {...} 在这种情况下，lookupOperatorOverloads方法会根据column1和column2的数据类型来选择合适的重载方法。 如果column1和column2都是int类型，那么会选择第一个重载方法； 如果column1和column2都是double类型，那么会选择第二个重载方法； 如果column1和column2都是string类型，那么会选择第三个重载方法。 如果找不到符合条件的操作符重载方法，lookupOperatorOverloads方法会抛出异常。 registerQuery private void registerQuery( SqlValidatorScope parentScope, @Nullable SqlValidatorScope usingScope, SqlNode node, SqlNode enclosingNode, @Nullable String alias, boolean forceNullabl","date":"2023-06-06","objectID":"/flink-sql-validate/:2:2","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":["Code"],"content":"举例 以 select * from tableA where amount \u003e 2 为例， ","date":"2023-06-06","objectID":"/flink-sql-validate/:3:0","tags":["flink","sql"],"title":"FlinkSQL - 验证过程","uri":"/flink-sql-validate/"},{"categories":["Code"],"content":"parseSqlList Sql语句从ParseImpl中调用CalciteParser的方法parseSqlList，将SQL语句转换成SqlNode // org.apache.flink.table.planner.parse.CalciteParser public SqlNodeList parseSqlList(String sql) { try { SqlParser parser = SqlParser.create(sql, config); return parser.parseStmtList(); } catch (SqlParseException e) { if (e.getMessage().contains(\"Encountered \\\"\u003cEOF\u003e\\\"\")) { throw new SqlParserEOFException(e.getMessage(), e); } throw new SqlParserException(\"SQL parse failed. \" + e.getMessage(), e); } } 其中 parser 的创建 使用到了 calcite 的 SqlParser实例 生成 SqlParser 需要两个参数 sql String 类型，就是传入的Sql字符串 config SqlParser.Config类型 ","date":"2023-05-27","objectID":"/flink-parse-sql/:1:0","tags":["flink","calcite","sql","javacc"],"title":"FlinkSQL - SQL语句到SqlNode","uri":"/flink-parse-sql/"},{"categories":["Code"],"content":"SqlParser.Config Flink 在解析sqlQuery 或者 createTemporaryView 这类操作SQL的处理中，就会创建CalciteParser,在创建CalciteParser的时候会调用 getSqlParserConfig() 的方法获取SqlParser.Config // org.apache.flink.table.planner.delegation.PlannerContext /** * Returns the SQL parser config for this environment including a custom Calcite configuration. */ private SqlParser.Config getSqlParserConfig() { return JavaScalaConversionUtil.\u003cSqlParser.Config\u003etoJava( getCalciteConfig().getSqlParserConfig()) .orElseGet( // we use Java lex because back ticks are easier than double quotes in // programming and cases are preserved () -\u003e { SqlConformance conformance = getSqlConformance(); return SqlParser.config() .withParserFactory(FlinkSqlParserFactories.create(conformance)) .withConformance(conformance) .withLex(Lex.JAVA) .withIdentifierMaxLength(256); }); } 如果设置了Calicte的一些配置，那么就会在这里被读取出来自定义的CalicteParser， 否则，走orElseGet信息 SqlParser在这里设置了解析SQL的语法、词法、方言以及限定词的最大长度 SqlConformance SqlConformance 是用来设置 SQL的方言(或标准)的 在FlinkSQL中一共有两种方言， Calcite 默认方言 Hive 方言 FlinkSqlParserFactories /** A util method to create SqlParserImplFactory according to SqlConformance. */ public class FlinkSqlParserFactories { private FlinkSqlParserFactories() {} public static SqlParserImplFactory create(SqlConformance conformance) { if (conformance == FlinkSqlConformance.DEFAULT) { return FlinkSqlParserImpl.FACTORY; } else { throw new TableException(\"Unsupported SqlConformance: \" + conformance); } } } 这里就很有意思，FlinkSqlParser只接受Calcite默认方言 然后再看FlinkSqlParserImpl // package org.apache.flink.sql.parser.impl; /** * SQL parser, generated from Parser.jj by JavaCC. * * \u003cp\u003eThe public wrapper for this parser is {@link SqlParser}. */ public class FlinkSqlParserImpl extends SqlAbstractParserImpl implements FlinkSqlParserImplConstants { private static final Logger LOGGER = CalciteTrace.getParserTracer(); 可以看出，FlinkSqlParserImpl 是通过JavaCC生成的 生成此类的 JavaCC 文件 在 flink-table/flink-sql-parser/target/generated-sources/javacc/Parser.jj 而这个应该是根据 FreeMarker的模板fmpp来生成的,打开 flink-sql-parser 下面 codegen里面的config.fmpp，我们可以发现一下一段话，答题意思就是说FMPP继承了CalciteParser，然后解析指定的Sql # This file is an FMPP (http://fmpp.sourceforge.net/) configuration file to # allow clients to extend Calcite's SQL parser to support application specific # SQL statements, literals or data types. # # Calcite's parser grammar file (Parser.jj) is written in javacc # (https://javacc.org/) with Freemarker (http://freemarker.org/) variables # to allow clients to: # 1. have custom parser implementation class and package name. # 2. insert new parser method implementations written in javacc to parse # custom: # a) SQL statements. # b) literals. # c) data types. # 3. add new keywords to support custom SQL constructs added as part of (2). # 4. add import statements needed by inserted custom parser implementations. # # Parser template file (Parser.jj) along with this file are packaged as # part of the calcite-core-\u003cversion\u003e.jar under \"codegen\" directory. 简单看一下这个jj文件，可以发现，就是在定义SQL如何声明，以及如何处理SQL语句的 SqlLiteral AllOrDistinct() : { } { \u003cDISTINCT\u003e { return SqlSelectKeyword.DISTINCT.symbol(getPos()); } | \u003cALL\u003e { return SqlSelectKeyword.ALL.symbol(getPos()); } } 在 flink-sql-parser 中还可以看到生成这些FlinkSQL的逻辑，比如这种 Flink的建表语句如何判断的逻辑，都在这里可以找到 // org.apache.flink.sql.parser.ddl public class SqlCreateTable extends SqlCreate implements ExtendedSqlNode { public static final SqlSpecialOperator OPERATOR = new SqlSpecialOperator(\"CREATE TABLE\", SqlKind.CREATE_TABLE); ... writer.keyword(\"CREATE\"); if (isTemporary()) { writer.keyword(\"TEMPORARY\"); } writer.keyword(\"TABLE\"); if (isIfNotExists()) { writer.keyword(\"IF NOT EXISTS\"); } tableName.unparse(writer, leftPrec, rightPrec); if (columnList.size() \u003e 0 || tableConstraints.size() \u003e 0 || watermark != null) { SqlUnparseUtils.unparseTableSchema( writer, leftPrec, rightPrec, columnList, tableConstraints, watermark); } if (comment != null) { writer.newlineAndIndent(); writer.keyword(\"COMMENT\"); comment.unparse(writer, leftPrec, rightPrec); } ... 返回jj文件中，我们可以看出一条SQL语句是如何","date":"2023-05-27","objectID":"/flink-parse-sql/:1:1","tags":["flink","calcite","sql","javacc"],"title":"FlinkSQL - SQL语句到SqlNode","uri":"/flink-parse-sql/"},{"categories":["Code"],"content":"SqlParser 了解了 SqlParser.Config的构成，SqlParser就更简单了创建过程就是将Config中的的数据拿出来 //~ Constructors ----------------------------------------------------------- private SqlParser(SqlAbstractParserImpl parser, Config config) { this.parser = parser; parser.setTabSize(1); parser.setQuotedCasing(config.quotedCasing()); parser.setUnquotedCasing(config.unquotedCasing()); parser.setIdentifierMaxLength(config.identifierMaxLength()); parser.setConformance(config.conformance()); parser.switchTo(SqlAbstractParserImpl.LexicalState.forConfig(config)); } 其中parser.switchTo 用于切换解析器的状态 具体来说，switchTo方法接受一个参数，表示要切换到的状态。在解析SQL语句的过程中，不同的状态对应着不同的语法规则和解析方式。通过切换状态，解析器可以根据当前的语法规则和上下文信息，正确地解析SQL语句。 举个例子，假设我们有一个SQL语句： SELECT name, age FROM users WHERE age \u003e 18; 在解析这个SQL语句时，解析器需要根据不同的关键字和符号，判断当前的语法状态。比如，在解析SELECT关键字时，解析器需要切换到SELECT状态；在解析FROM关键字时，解析器需要切换到FROM状态；在解析WHERE关键字时，解析器需要切换到WHERE状态。 具体的实现可以参考SqlAbstractParserImpl类的源代码。 所以一条字符串类型的Sql语句 转换成 SqlNode 的流程应该就是 ","date":"2023-05-27","objectID":"/flink-parse-sql/:1:2","tags":["flink","calcite","sql","javacc"],"title":"FlinkSQL - SQL语句到SqlNode","uri":"/flink-parse-sql/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年05月21日12:45:00 北京市西城区西四北大街24号 2 5分钟 170 ","date":"2023-05-21","objectID":"/xingyuancanting/:1:0","tags":["小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":["生活"],"content":"一句话 点多了，下次尝尝过油肉 ","date":"2023-05-21","objectID":"/xingyuancanting/:2:0","tags":["小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":["生活"],"content":"点餐 ","date":"2023-05-21","objectID":"/xingyuancanting/:3:0","tags":["小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":["生活"],"content":"主食 炖肉面：吸溜…吸溜… 肥肉也挺香的肥而不腻 双拼捞面：一半过油肉，一半虾仁 ","date":"2023-05-21","objectID":"/xingyuancanting/:3:1","tags":["小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":["生活"],"content":"热菜 糖醋里脊：酸甜口，酸酸甜甜 ","date":"2023-05-21","objectID":"/xingyuancanting/:3:2","tags":["小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":["生活"],"content":"小吃 干炸丸子：本人还是比较喜欢吃丸子的，各种丸子都很好吃 豆皮：豆皮配面条，一样的爽口 ","date":"2023-05-21","objectID":"/xingyuancanting/:3:3","tags":["小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":["生活"],"content":"总结 一家家常小店，很实惠的，据说还是一家国营店，人一直很多，我想哪一天如果路过了，还是会再进去的 ","date":"2023-05-21","objectID":"/xingyuancanting/:4:0","tags":["小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-05-21","objectID":"/xingyuancanting/:5:0","tags":["小吃","面条"],"title":"杏园餐厅","uri":"/xingyuancanting/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年05月19日13:30:00 北京市东城区东直门内大街233号 3 不用排队 574 ","date":"2023-05-19","objectID":"/hudafanguan/:1:0","tags":["小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":["生活"],"content":"一句话 看着别人吃还真挺香的~~ ","date":"2023-05-19","objectID":"/hudafanguan/:2:0","tags":["小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":["生活"],"content":"点餐 ","date":"2023-05-19","objectID":"/hudafanguan/:3:0","tags":["小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":["生活"],"content":"热菜 馋嘴蛙仔：很嫩，辣味十足，点了大份，量还是蛮大的，唯一不足的就是太咸了 香辣美容蹄：猪蹄软烂，辣味咸味刚刚好，点的菜里唯一一道没有那么辣 辣炒花蛤：花蛤很干净，有点辣还有一点类似香油的那种淡淡的香味，还是很不错的 ","date":"2023-05-19","objectID":"/hudafanguan/:3:1","tags":["小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":["生活"],"content":"烧烤 烤羊肉串：烤羊肉串还不错，后来又点了一份 烤生蚝：肉很厚，蒜香味没有浸入到里面 ","date":"2023-05-19","objectID":"/hudafanguan/:3:2","tags":["小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":["生活"],"content":"小龙虾 麻辣小龙虾：麻辣味很足，肉比较紧实，虾面往里面拌一下，吃着蛮爽的 ","date":"2023-05-19","objectID":"/hudafanguan/:3:3","tags":["小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":["生活"],"content":"总结 这家店名气很大，来了就是吃小龙虾的 ","date":"2023-05-19","objectID":"/hudafanguan/:4:0","tags":["小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-05-19","objectID":"/hudafanguan/:5:0","tags":["小吃","小龙虾"],"title":"胡大饭馆","uri":"/hudafanguan/"},{"categories":["生活"],"content":"记录 时间 地址 人数 排队等待 花销 2023年05月14日12:30:00 海淀区清河中街66号院1号楼6层L650B 2 30分钟 340 ","date":"2023-05-14","objectID":"/putiancanting/:1:0","tags":["闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":["生活"],"content":"一句话 下次一个人跑去尝尝肉燕 ","date":"2023-05-14","objectID":"/putiancanting/:2:0","tags":["闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":["生活"],"content":"点餐 ","date":"2023-05-14","objectID":"/putiancanting/:3:0","tags":["闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":["生活"],"content":"凉菜 土笋冻：吃起来嘎吱嘎吱的，QQ弹弹的，啥都别想放嘴里就行啦 水晶猪蹄冻：浓稠，猪蹄冻的酱汁没吃过，很新奇 ","date":"2023-05-14","objectID":"/putiancanting/:3:1","tags":["闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":["生活"],"content":"热菜 铁板盐焗蛏：肉很肥美，蛏子刚取出来的时候还有汁水 家乡焖笋干：笋干就是笋干的味道，让我想起了我不曾去过的鱼米水乡？ 九层塔海蛏煲：蛏子裹得面太厚了，鲜味被油炸和面团盖住了，尝不出肉质感 莆田荔枝肉：荔枝很好吃 马来风光：咸，油很大，菜上面的配料还挺鲜的 ","date":"2023-05-14","objectID":"/putiancanting/:3:2","tags":["闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":["生活"],"content":"甜品 建莲雪耳汤：甜，有点甜 ","date":"2023-05-14","objectID":"/putiancanting/:3:3","tags":["闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":["生活"],"content":"总结 作为一个北方人，对福建菜有一种好奇，吃到的东西都会觉得很新奇，有意思，可以再去尝尝别的菜 ","date":"2023-05-14","objectID":"/putiancanting/:4:0","tags":["闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":["生活"],"content":"附录 北京吃喝篇 ","date":"2023-05-14","objectID":"/putiancanting/:5:0","tags":["闽菜"],"title":"莆田餐厅","uri":"/putiancanting/"},{"categories":["Code"],"content":"简介 Apache Calcite是一个动态数据管理框架。 它包含构成典型数据库管理系统的许多部分，但省略了一些关键功能:数据存储、处理数据的算法和存储元数据的存储库。 我们主要用 Calcite 来解析SQL。很多项目也是直接使用了Calcite来解析SQL，优化SQL等，比如 FlinkSQL calicte的基本架构如下 ","date":"2023-05-11","objectID":"/calcite-prefix/:1:0","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":["Code"],"content":"关系代数 关系模型源于数学。关系是由元组构成的集合，可以通过关系的运算来表达查询要求，而关系代数恰恰是关系操作语言的一种传统的表示方式，它是一种抽象的查询语言。 关系代数的运算对象是关系，运算结果也是关系。与一般的运算一样，运算对象、运算符和运算结果是关系代数的三大要素。 关系代数的运算可分为两大类： 传统的集合运算。这类运算完全把关系看成元组的集合。传统的集合运算包括集合的广义笛卡儿积运算、并运算、交运算和差运算。 专门的关系运算。这类运算除了把关系看成元组的集合外，还通过运算表达了查询的要求。专门的关系运算包括选择、投影、连接和除运算。 关系代数中的运算符可以分为四类：传统的集合运算符、专门的关系运算符、比较运算符和逻辑运算符。 关系运算 和 SQL的关系如下 运算符 SQL关键字 含义 分类 $$\\cap$$ 交 集合运算 $$\\cup$$ 并 集合运算 $$-$$ 差 集合运算 $$\\times$$ from A,B 广义笛卡尔积 集合运算 $$\\sigma$$ where 选择 关系运算 $$\\Pi$$ select distinct 投影 关系运算 $$\\bowtie$$ join 连接 关系运算 $$\\div$$ 除 关系运算 $$\u003e$$ 大于 比较运算 $$\u003c$$ 小于 比较运算 $$=$$ 等于 比较运算 $$\\neq$$ 不等 比较运算 $$\\leqslant$$ 小于等于 比较运算 $$\\geqslant$$ 大于等于 比较运算 $$\\neg$$ 非 逻辑运算 $$\\land$$ 与 逻辑运算 $$\\lor$$ 或 逻辑运算 SQL语句会先翻译成为关系代数后再被执行的 在执行explain 一条SQL的时候 就可以看到翻译后关系代数的命名 == Abstract Syntax Tree == LogicalProject(user=[$0], product=[$1], amount=[$2]) +- LogicalFilter(condition=[\u003e($2, 2)]) +- LogicalTableScan(table=[[*anonymous_datastream_source$1*]]) ","date":"2023-05-11","objectID":"/calcite-prefix/:2:0","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":["Code"],"content":"Calcite 解析流程 Parser 将SQL语句(字符串) 解析成 AST(抽象语法树) 语法树的节点在代码中为SqlNode Validate 校验SqlNode节点，替换节点中的部分属性，设置单调性等信息 Convert 将SqlNode 抽象语法树 转换成RelNode，执行逻辑执行计划 Optimize 成本优化 Execute 生成动态代码，并执行物理执行计划 其中， SqlNode 是语法树上的节点，其本质是把SQL语句进行拆解 RelNode 是关系节点，代表的事关系代数中的关系操作，RelNode 更倾向数学的概念，就可以进行下一步的优化了 RexNode 虽然RexNode也属于关系节点，但是这里的RexNode更偏向于去表示表达式，比如一个常数，或者是 简单的a+b的运算，亦或是count(*) 这样的聚合；所以一个RelNode中会有很多RexNode节点 ","date":"2023-05-11","objectID":"/calcite-prefix/:3:0","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":["Code"],"content":"SqlNode SqlNode 是一个抽象类，没有过多的信息，就包括一个位置信息的对象SqlParserPos SqlParserPos 是用来表示SQL语句中的位置信息的类。它包含了行号、列号和字符偏移量等信息，可以用于定位SQL语句中的错误或者生成更加详细的错误信息 public abstract class SqlNode implements Cloneable { //~ Static fields/initializers --------------------------------------------- public static final @Nullable SqlNode[] EMPTY_ARRAY = new SqlNode[0]; //~ Instance fields -------------------------------------------------------- protected final SqlParserPos pos; } public class SqlParserPos implements Serializable { //~ Static fields/initializers --------------------------------------------- /** * SqlParserPos representing line one, character one. Use this if the node * doesn't correspond to a position in piece of SQL text. */ public static final SqlParserPos ZERO = new SqlParserPos(0, 0); /** Same as {@link #ZERO} but always quoted. **/ public static final SqlParserPos QUOTED_ZERO = new QuotedParserPos(0, 0, 0, 0); private static final long serialVersionUID = 1L; //~ Instance fields -------------------------------------------------------- private final int lineNumber; private final int columnNumber; private final int endLineNumber; private final int endColumnNumber; } 从SqlSelect的这个类中就可以看出，一个简单的SQL语句在SqlSelect中都能找到 public class SqlSelect extends SqlCall { //~ Static fields/initializers --------------------------------------------- // constants representing operand positions public static final int FROM_OPERAND = 2; public static final int WHERE_OPERAND = 3; public static final int HAVING_OPERAND = 5; SqlNodeList keywordList; SqlNodeList selectList; @Nullable SqlNode from; @Nullable SqlNode where; @Nullable SqlNodeList groupBy; @Nullable SqlNode having; SqlNodeList windowDecls; @Nullable SqlNodeList orderBy; @Nullable SqlNode offset; @Nullable SqlNode fetch; @Nullable SqlNodeList hints; } 这里的keywordList是保留字，如果有 distinct 这种就会放在这个keywordList中 不过从debug的过程来看 这个keywordList是个空的数据 ","date":"2023-05-11","objectID":"/calcite-prefix/:3:1","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":["Code"],"content":"RelNode RelNode 是个接口 继承了 RelOptNode RelOptNode 目前没有看到除了RelNode以外有其他地方使用和实现 RelNode 接口定义了树结构的父类以及节点数据和类型的方法 真正处理的方法都放在了 RelShuttle 和 RexShuttle中 public interface RelNode extends RelOptNode, Cloneable { /** * Accepts a visit from a shuttle. * * @param shuttle Shuttle * @return A copy of this node incorporating changes made by the shuttle to * this node's children */ RelNode accept(RelShuttle shuttle); /** * Accepts a visit from a shuttle. If the shuttle updates expression, then * a copy of the relation should be created. This new relation might have * a different row-type. * * @param shuttle Shuttle * @return A copy of this node incorporating changes made by the shuttle to * this node's children */ RelNode accept(RexShuttle shuttle); } 可以具体一点，看一下比较常用的RelNode 的实现是如何定义的：LogicalProject /** * Sub-class of Project not targeted at any particular engine or calling convention. */ public final class LogicalProject extends Project { //~ Constructors ----------------------------------------------------------- /** * Creates a LogicalProject. * * \u003cp\u003eUse {@link #create} unless you know what you're doing. * * @param cluster Cluster this relational expression belongs to * @param traitSet Traits of this relational expression * @param hints Hints of this relational expression * @param input Input relational expression * @param projects List of expressions for the input columns * @param rowType Output row type */ public LogicalProject( RelOptCluster cluster, RelTraitSet traitSet, List\u003cRelHint\u003e hints, RelNode input, List\u003c? extends RexNode\u003e projects, RelDataType rowType) { super(cluster, traitSet, hints, input, projects, rowType); assert traitSet.containsIfApplicable(Convention.NONE); } /** * Creates a LogicalProject by parsing serialized output. */ public LogicalProject(RelInput input) { super(input); } //~ Methods ---------------------------------------------------------------- /** Creates a LogicalProject. */ public static LogicalProject create(final RelNode input, List\u003cRelHint\u003e hints, final List\u003c? extends RexNode\u003e projects, @Nullable List\u003c? extends @Nullable String\u003e fieldNames) { final RelOptCluster cluster = input.getCluster(); final RelDataType rowType = RexUtil.createStructType(cluster.getTypeFactory(), projects, fieldNames, SqlValidatorUtil.F_SUGGESTER); return create(input, hints, projects, rowType); } /** Creates a LogicalProject, specifying row type rather than field names. */ public static LogicalProject create(final RelNode input, List\u003cRelHint\u003e hints, final List\u003c? extends RexNode\u003e projects, RelDataType rowType) { final RelOptCluster cluster = input.getCluster(); final RelMetadataQuery mq = cluster.getMetadataQuery(); final RelTraitSet traitSet = cluster.traitSet().replace(Convention.NONE) .replaceIfs(RelCollationTraitDef.INSTANCE, () -\u003e RelMdCollation.project(mq, input, projects)); return new LogicalProject(cluster, traitSet, hints, input, projects, rowType); } } LocalProject 的变量定义都散落在他的父类中 /** * Relational expression that computes a set of 'select expressions' from its input relational expression. * See Also: * org.apache.calcite.rel.logical.LogicalProject */ public abstract class Project extends SingleRel implements Hintable { //~ Instance fields -------------------------------------------------------- protected final ImmutableList\u003cRexNode\u003e exps; protected final ImmutableList\u003cRelHint\u003e hints; } /** * Abstract base class for relational expressions with a single input. * * \u003cp\u003eIt is not required that single-input relational expressions use this * class as a base class. However, default implementations of methods make life * easier. */ public abstract class SingleRel extends AbstractRelNode { //~ Instance fields -------------------------------------------------------- protected RelNode input; } ","date":"2023-05-11","objectID":"/calcite-prefix/:3:2","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":["Code"],"content":"RexNode RexNode 是 行表达式(Row Expression) 是通过SqlNode 转换过来 尤其是 select 的项 where 的条件 等等 与SqlNode不同的是,RexNode的类型都是确定的，SqlNode的类型是在优化前就已经定好了，所以这个类型可能不能用 RexNode 是一个抽象类，也是实现了访问者模式 /** * Row expression. * * \u003cp\u003eEvery row-expression has a type. * (Compare with {@link org.apache.calcite.sql.SqlNode}, which is created before * validation, and therefore types may not be available.) * * \u003cp\u003eSome common row-expressions are: {@link RexLiteral} (constant value), * {@link RexVariable} (variable), {@link RexCall} (call to operator with * operands). Expressions are generally created using a {@link RexBuilder} * factory.\u003c/p\u003e * * \u003cp\u003eAll sub-classes of RexNode are immutable.\u003c/p\u003e */ public abstract class RexNode { //~ Instance fields -------------------------------------------------------- // Effectively final. Set in each sub-class constructor, and never re-set. protected @MonotonicNonNull String digest; //~ Methods ---------------------------------------------------------------- public abstract RelDataType getType(); /** * Returns whether this expression always returns true. (Such as if this * expression is equal to the literal \u003ccode\u003eTRUE\u003c/code\u003e.) */ public boolean isAlwaysTrue() { return false; } /** * Returns whether this expression always returns false. (Such as if this * expression is equal to the literal \u003ccode\u003eFALSE\u003c/code\u003e.) */ public boolean isAlwaysFalse() { return false; } public boolean isA(SqlKind kind) { return getKind() == kind; } public boolean isA(Collection\u003cSqlKind\u003e kinds) { return getKind().belongsTo(kinds); } /** * Returns the kind of node this is. * * @return Node kind, never null */ public SqlKind getKind() { return SqlKind.OTHER; } @Override public String toString() { return requireNonNull(digest, \"digest\"); } /** Returns the number of nodes in this expression. * * \u003cp\u003eLeaf nodes, such as {@link RexInputRef} or {@link RexLiteral}, have * a count of 1. Calls have a count of 1 plus the sum of their operands. * * \u003cp\u003eNode count is a measure of expression complexity that is used by some * planner rules to prevent deeply nested expressions. */ public int nodeCount() { return 1; } /** * Accepts a visitor, dispatching to the right overloaded * {@link RexVisitor#visitInputRef visitXxx} method. * * \u003cp\u003eAlso see {@link RexUtil#apply(RexVisitor, java.util.List, RexNode)}, * which applies a visitor to several expressions simultaneously. */ public abstract \u003cR\u003e R accept(RexVisitor\u003cR\u003e visitor); /** * Accepts a visitor with a payload, dispatching to the right overloaded * {@link RexBiVisitor#visitInputRef(RexInputRef, Object)} visitXxx} method. */ public abstract \u003cR, P\u003e R accept(RexBiVisitor\u003cR, P\u003e visitor, P arg); /** {@inheritDoc} * * \u003cp\u003eEvery node must implement {@link #equals} based on its content */ @Override public abstract boolean equals(@Nullable Object obj); /** {@inheritDoc} * * \u003cp\u003eEvery node must implement {@link #hashCode} consistent with * {@link #equals} */ @Override public abstract int hashCode(); } 还是具体一点看一下 RexCall的定义 RexCall是有操作符的表达式，操作符可以是 一元二元，也可是函数，或者是固定语法 /** * An expression formed by a call to an operator with zero or more expressions * as operands. * * \u003cp\u003eOperators may be binary, unary, functions, special syntactic constructs * like \u003ccode\u003eCASE ... WHEN ... END\u003c/code\u003e, or even internally generated * constructs like implicit type conversions. The syntax of the operator is * really irrelevant, because row-expressions (unlike * {@link org.apache.calcite.sql.SqlNode SQL expressions}) * do not directly represent a piece of source code. * * \u003cp\u003eIt's not often necessary to sub-class this class. The smarts should be in * the operator, rather than the call. Any extra information about the call can * often be encoded as extra arguments. (These don't need to be hidden, because * no one is going to be generating source code from this tree.)\u003c/p\u003e */ public class RexCall extends RexNode { //~ Instance fields -------------------------------------------------------- public final SqlOperator op; public final ImmutableList\u003cRexNode\u003e operands; public","date":"2023-05-11","objectID":"/calcite-prefix/:3:3","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":["Code"],"content":"案例 通过一个简单的例子 -- table(user,product,amount) select * from tableA where amount \u003e 2 该Sql语句经过后生成 LogicalProject(user=[$0], product=[$1], amount=[$2]) +- LogicalFilter(condition=[\u003e($2, 2)]) +- LogicalTableScan(table=[[default_catalog, default_database, tableA]]) 就可以看出： select * 对应的是 LocalProject where amount \u003e 2 对应的是 LogicalFilter from tableA 对应的是 LogicalTableScan LogicalProject 有以下变量 变量 类型 说明 案例中的取值 exprs ImmutableList\u003cRexNode\u003e 表达式，一般就是select 后面跟的表达式，这里的表达式已经转换过了，给不同的表达式进行了命名,user=[$0], product=[$1], amount=[$2] 这里都是RexInputRef [$0, $1, $2] hints ImmutableList\u003cRelHint\u003e 这里是hint 的表达式，是嵌在代码里的一串增强说明 [] input LogicalFilter 关联其他RelNode，是该对象的输入节点 LogicalFilter(condition=[\u003e($2, 2)]) rowType RelRecordType 数据类型 RecordType(BIGINT user, VARCHAR(2147483647) product, INTEGER amount) digest AbstractRelNode.InnerRelDigest 关系信息的摘要，根据此判断两个关系是否相同 - cluster RelOptCluster 默认的Cluster，提供元数据，统计信息，管理查询计划的各种关系运算符，提供优化查询计划的方法等 - id int - - traitSet List\u003cRelTraitSet\u003e 关系的一些特征、特质，比如处理引擎的规范，Flink分布，mini-batch，下ModifyKind， UpdateKind 这些 [Convention, FlinkRelDistribution, MiniBatchIntervalTrait, ModifyKindSetTrait, UpdateKindTrait] 可以看出RelNode 其实更像是一个关系代数，这个关系代数也是有树型关系在里面的 ","date":"2023-05-11","objectID":"/calcite-prefix/:3:4","tags":["calcite","sql"],"title":"Calcite 简介","uri":"/calcite-prefix/"},{"categories":["Code"],"content":"sqlQuery sql 进入sqlQuery后，首先就是获取Parser 解析sql语句 // TableEnvironmentImpl.java @Override public Table sqlQuery(String query) { List\u003cOperation\u003e operations = getParser().parse(query); if (operations.size() != 1) { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query.\"); } Operation operation = operations.get(0); if (operation instanceof QueryOperation \u0026\u0026 !(operation instanceof ModifyOperation)) { return createTable((QueryOperation) operation); } else { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query of type \" + \"SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.\"); } } 这里会获取StreamPlanner的Parser @Override public Parser getParser() { return getPlanner().getParser(); } @VisibleForTesting public Planner getPlanner() { return planner; } StreamTableEnvironment 在创建的过程中会创建 Planner final Planner planner = PlannerFactoryUtil.createPlanner( executor, tableConfig, userClassLoader, moduleManager, catalogManager, functionCatalog); 其中， executor 用于执行Planner的对象 tableConfig table和SQL的配置项，比如 checkpoint，watermark等 userClassLoader 用户动态类加载器 默认的使用org.apache.flink.util.FlinkUserCodeClassLoaders来创建的 moduleManager 模块管理器，会将CoreModule的模块加入到管理器中 module 就是 定义的一系列元数据，包括函数、规则、操作符等 Modules define a set of metadata, including functions, user defined types, operators, rules, etc. Metadata from modules are regarded as built-in or system metadata that users can take advantages of. catalogManager 用于处理 catalog，封装catalog和一些临时表对象 catalog 提供了元数据信息，用来管理元数据信息(如table、view、function 和 type等)，提供了一套api，可以使用Table API和SQL来访问 This interface is responsible for reading and writing metadata such as database/table/views/UDFs from a registered catalog. It connects a registered catalog and Flink’s Table API. This interface only processes permanent metadata objects. In order to process temporary objects, a catalog can also implement the TemporaryOperationListener interface. functionCatalog 函数catalog，保存函数的定义 注册的函数就会放在这个对象中，像UDF 等注册的catalog也会放在这里 FLIP-65 Simple function catalog to store FunctionDefinitions in catalogs. Note: This class can be cleaned up a lot once we drop the methods deprecated as part of FLIP-65. In the long-term, the class should be a part of catalog manager similar to DataTypeFactory. PlannerFactoryUtil.createPlanner 方法会先找到 PlannerFactory（默认是 DefaultPlannerFactory）然后根据 TableConfig 中的execution.runtime-mode 确认启动的任务是流任务还是批任务，进而创建Planner(SteamPlanner) @Override public Planner create(Context context) { final RuntimeExecutionMode runtimeExecutionMode = context.getTableConfig().get(ExecutionOptions.RUNTIME_MODE); switch (runtimeExecutionMode) { case STREAMING: return new StreamPlanner( context.getExecutor(), context.getTableConfig(), context.getModuleManager(), context.getFunctionCatalog(), context.getCatalogManager(), context.getClassLoader()); case BATCH: return new BatchPlanner( context.getExecutor(), context.getTableConfig(), context.getModuleManager(), context.getFunctionCatalog(), context.getCatalogManager(), context.getClassLoader()); default: throw new TableException( String.format( \"Unsupported mode '%s' for '%s'. Only an explicit BATCH or \" + \"STREAMING mode is supported in Table API.\", runtimeExecutionMode, RUNTIME_MODE.key())); } } Planner的Parser, 就是用来解析SQL语句的, Parser 目前分为两种SQL方言 flink 默认的SQL 和 Hive @PublicEvolving public enum SqlDialect { /** Flink's default SQL behavior. */ DEFAULT, /** * SQL dialect that allows some Apache Hive specific grammar. * * \u003cp\u003eNote: We might never support all of the Hive grammar. See the documentation for supported * features. */ HIVE } 默认情况下 我们创建出来的的Parser(ParserImpl) 是使用Calcite进行解析的 /** * 这里的 context 就是根据planner 的信息创建的 * parser = parserFactory.create(new DefaultCalciteContext(catalogManager, plannerContext)) * */ @Override public Parser create(Context context) { DefaultCalciteContext defaultCalciteContext = (DefaultCalciteContext) context; return new ParserImpl( defaultCalciteContext.getCatalogManager(), defaultCalciteContext.getPlannerCon","date":"2023-05-04","objectID":"/flink-sql-query/:1:0","tags":["flink","sql"],"title":"FlinkSQL - SQL解析过程","uri":"/flink-sql-query/"},{"categories":["Code"],"content":"ParserImpl 默认的ParserImpl的类定义 public ParserImpl( CatalogManager catalogManager, Supplier\u003cFlinkPlannerImpl\u003e validatorSupplier, Supplier\u003cCalciteParser\u003e calciteParserSupplier, RexFactory rexFactory) { this.catalogManager = catalogManager; this.validatorSupplier = validatorSupplier; this.calciteParserSupplier = calciteParserSupplier; this.rexFactory = rexFactory; } parse 的过程就是调用 CalciteParser 将 Sql 语句转换成SqlNode的过程，Calcite会调用JavaCC来解析SQL语句 然后经过转换 把 sqlNode转换成Operation 在这里如果有一些特殊的SQL解析 会放到EXTENDED_PARSER 里面进行解析 @Override public List\u003cOperation\u003e parse(String statement) { CalciteParser parser = calciteParserSupplier.get(); FlinkPlannerImpl planner = validatorSupplier.get(); Optional\u003cOperation\u003e command = EXTENDED_PARSER.parse(statement); if (command.isPresent()) { return Collections.singletonList(command.get()); } // parse the sql query // use parseSqlList here because we need to support statement end with ';' in sql client. SqlNodeList sqlNodeList = parser.parseSqlList(statement); List\u003cSqlNode\u003e parsed = sqlNodeList.getList(); Preconditions.checkArgument(parsed.size() == 1, \"only single statement supported\"); return Collections.singletonList( SqlNodeToOperationConversion.convert(planner, catalogManager, parsed.get(0)) .orElseThrow(() -\u003e new TableException(\"Unsupported query: \" + statement))); } 例如， SQL语句 select * from tableA where amount \u003e 2 经过 CalciteParser 就会 生成一个 SqlNode 最后经过SqlNodeToOperationConversion.convert 会转换成 包含逻辑计划的operation 目前流程为 ","date":"2023-05-04","objectID":"/flink-sql-query/:2:0","tags":["flink","sql"],"title":"FlinkSQL - SQL解析过程","uri":"/flink-sql-query/"},{"categories":["Code"],"content":"flink 参数 参数 说明 flink 版本 1.17 java 版本 1.8 ","date":"2023-04-30","objectID":"/flink-prefix/:1:0","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":["Code"],"content":"测试SQL select * from tableA where amount \u003e 2 ","date":"2023-04-30","objectID":"/flink-prefix/:2:0","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":["Code"],"content":"运行环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); final DataStream\u003cOrder\u003e orderA = env.fromCollection( Arrays.asList( new Order(1L, \"beer\", 3), new Order(1L, \"diaper\", 4), new Order(3L, \"rubber\", 2))); final Table tableA = tableEnv.fromDataStream(orderA); final Table result = tableEnv.sqlQuery( \"select * from \" + tableA + \" where amount \u003e 2\" ); tableEnv.toDataStream(result, Row.class).print(); env.execute(); 运行结果为 (true,+I[1, beer, 3, 2, pen, 3]) (true,+I[1, beer, 3, 2, rubber, 3]) ","date":"2023-04-30","objectID":"/flink-prefix/:3:0","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":["Code"],"content":"SQL流程 一条SQL语句 通过Calcite 转换成 物理计划，物理计划通过代码生成计划转换成Flink Transformation 从而最终转换成 Flink 的执行图 从代码来看 tableEnv.sqlQuery() 将sql 语句转换成了 逻辑计划 -\u003e 物理计划 env.execute() 生成StreamGraph 最终执行语句 ","date":"2023-04-30","objectID":"/flink-prefix/:4:0","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":["Code"],"content":"sqlQuery sqlQuery 会把 输入的 SQL 语句转换成Operation Operation 就是对于表的所有操作(DML, DDL, DQL, DCL) Covers all sort of Table operations such as queries(DQL), modifications(DML), definitions(DDL), or control actions(DCL). This is the output of Planner.getParser() and Parser.parse(String). 在这里，Operation就是一个PlannerQueryOperation，里面包含了RelNode的信息。Operation 会包装成Table 对象返回 public Table sqlQuery(String query) { /** * 这里会解析Sql语句转换成关系代数 */ List\u003cOperation\u003e operations = getParser().parse(query); if (operations.size() != 1) { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query.\"); } Operation operation = operations.get(0); if (operation instanceof QueryOperation \u0026\u0026 !(operation instanceof ModifyOperation)) { /** * 这里将转换的Operation转换成Flink Table API可以识别的Table对象 */ return createTable((QueryOperation) operation); } else { throw new ValidationException( \"Unsupported SQL query! sqlQuery() only accepts a single SQL query of type \" + \"SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.\"); } } ","date":"2023-04-30","objectID":"/flink-prefix/:4:1","tags":["flink","sql"],"title":"FlinkSQL - 开始","uri":"/flink-prefix/"},{"categories":["Code"],"content":"LSM 树(Log-Structured-Merge-Tree) 不算是树，其实是一种存储结构 利用顺序追加写来提高写性能 内存-文件读取方式会降低读性能 ","date":"2023-04-28","objectID":"/lsm/:1:0","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":["Code"],"content":"MemTable 放置在内存里 最新的数据 按照Key 组织数据有序(HBase：使用跳表) WAL保证可靠性 ","date":"2023-04-28","objectID":"/lsm/:1:1","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":["Code"],"content":"Immutable MemTable MemTable 达到一定大小后转化成Immutable MemTable 写操作由新的MemTable 处理， 在转存过程中不阻塞数据更新操作 ","date":"2023-04-28","objectID":"/lsm/:1:2","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":["Code"],"content":"SSTable(Sorted String Table) 有序键值对集合 在磁盘的数据结构 为了加快读取SSTable的读取，可以通过建立key索引以及布隆过滤器来加快key的查找 LSM树会将所有的数据插入、修改、删除等操作记录保存在内存中；当此类操作达到一定的数据量后，再批量地顺序写入到磁盘当中 LSM树的数据更新是日志式的，当一条数据更新会直接append一条更新记录完成的，目的就是为了顺序写，将Immutable MemTable flush到持久化存储，而不用修改之前的SSTable中的key 不同的SSTable中，可能存在相同Key的记录，最新的记录是准确的（索引/Bloom来优化查找速度） 为了去除冗余的key需要进行compactcao操作 ","date":"2023-04-28","objectID":"/lsm/:1:3","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":["Code"],"content":"Compact策略 ","date":"2023-04-28","objectID":"/lsm/:2:0","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":["Code"],"content":"顺序冗余存储可能带来的问题 读放大 读取数据时实际读取的数据量大于真正的数据量 Eg: 先在 MemTable 查看当前Key 是否存在，不存在继续从SSTable中查找 写放大 写入数据时实际写入的数据量大于真正的数据量 Eg: 写入时可能触发Compact操作，导致实际写入的数据量远大于该key的数据量 空间放大 数据实际占用的磁盘空间比数据的真正大小更多 Eg: 对于一个key来说，只有最新的那条记录是有效的，而之前的记录都是可以被清理回收的。 ","date":"2023-04-28","objectID":"/lsm/:2:1","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":["Code"],"content":"Compact策略 size-tiered 策略 保证每层内部的SSTable的大小相近 同时限制每一层SSTable的数量 每层限制SSTable为N，当每层SSTable达到N后，则触发Compact操作合并这些SSTable，并将合并后的结果写入到下一层成为一个更大的sstable。 当层数达到一定数量时，最底层的单个SSTable的大小会变得非常大。并且size-tiered策略会导致空间放大比较严重。即使对于同一层的SSTable，每个key的记录是可能存在多份的，只有当该层的SSTable执行compact操作才会消除这些key的冗余记录。 leveled策略 leveled策略也是采用分层的思想，每一层限制总文件的大小 将每一层切分成多个大小相近的SSTable 这一层的SSTable是全局有序的，意味着一个key在每一层至多只有1条记录，不存在冗余记录 合并过程 L1的总大小超过L1本身大小限制： 此时会从L1中选择至少一个文件，然后把它跟L2有交集的部分(非常关键)进行合并。生成的文件会放在L2: 此时L1第二SSTable的key的范围覆盖了L2中前三个SSTable，那么就需要将L1中第二个SSTable与L2中前三个SSTable执行Compact操作。 如果L2合并后的结果仍旧超出L5的阈值大小，需要重复之前的操作 —— 选至少一个文件然后把它合并到下一层 Ps: 多个不相干的合并是可以并发进行的： leveled策略相较于size-tiered策略来说，每层内key是不会重复的，即使是最坏的情况，除开最底层外，其余层都是重复key，按照相邻层大小比例为10来算，冗余占比也很小。因此空间放大问题得到缓解。但是写放大问题会更加突出。举一个最坏场景，如果LevelN层某个SSTable的key的范围跨度非常大，覆盖了LevelN+1层所有key的范围，那么进行Compact时将涉及LevelN+1层的全部数据 ","date":"2023-04-28","objectID":"/lsm/:2:2","tags":["lsm"],"title":"LSM树","uri":"/lsm/"},{"categories":["Code"],"content":"介绍 AggHandlerCodeGenerator 的代码在 flink planner 下，用来生成聚合函数的代码,是scala 代码 ","date":"2022-09-26","objectID":"/flink-agg-handler-code-generator/:0:0","tags":["flink"],"title":"FlinkSQL - AggHandlerCodeGenerator","uri":"/flink-agg-handler-code-generator/"},{"categories":["Code"],"content":"类定义 package org.apache.flink.table.planner.codegen.agg class AggsHandlerCodeGenerator( ctx: CodeGeneratorContext, // 上下文 relBuilder: RelBuilder, // 用来生成关系表达式 inputFieldTypes: Seq[LogicalType], copyInputField: Boolean // 需要缓存时将此字段设置为true) { private val inputType = RowType.of(inputFieldTypes: _*) /** 常量表达式 */ private var constants: Seq[RexLiteral] = Seq() private var constantExprs: Seq[GeneratedExpression] = Seq() /** 窗口相关参数，窗口聚合才会用到 */ private var namespaceClassName: String = _ private var windowProperties: Seq[PlannerWindowProperty] = Seq() private var hasNamespace: Boolean = false /** 聚合信息 */ private var accTypeInfo: RowType = _ private var aggBufferSize: Int = _ private var mergedAccExternalTypes: Array[DataType] = _ private var mergedAccOffset: Int = 0 private var mergedAccOnHeap: Boolean = false private var ignoreAggValues: Array[Int] = Array() private var isAccumulateNeeded = false private var isRetractNeeded = false private var isMergeNeeded = false var valueType: RowType = _ /** * 生成 [[AggsHandleFunction]] 或者 [[NamespaceAggsHandleFunction]] 会创建 [[aggBufferCodeGens]] and [[aggActionCodeGens]] 两者包含相同的AggCodeGens，aggBufferCodeGens 以列表的扁平形式， aggActionCodeGens是树形结构 在没有distinct 的的情况下，两者相同 */ /** aggBufferCodeGens 用于生成相关累加器(Accumulator)的 方法 */ private var aggBufferCodeGens: Array[AggCodeGen] = _ /** aggActionCodeGens 树形结构，聚合distinct数据的时候，会将相同需要distinct的字段组成树结构 */ private var aggActionCodeGens: Array[AggCodeGen] = _ object aggshandlercodegenerator { /** static terms **/ val acc_term = \"acc\" val merged_acc_term = \"otheracc\" val accumulate_input_term = \"accinput\" val retract_input_term = \"retractinput\" val distinct_key_term = \"distinctkey\" val namespace_term = \"namespace\" val store_term = \"store\" val collector: string = classname[collector[_]] val collector_term = \"out\" val member_collector_term = \"convertcollector\" val convert_collector_type_term = \"convertcollector\" val key_term = \"groupkey\" val input_not_null = false } 如果一个SQL的结构如下 count(*), count(distinct a), count(distinct a) filter d \u003e 5, sum(a), sum(distinct a) +----------+-----------+-----------+---------+---------+----------------+ | count(*) | count(a') | count(a') | sum(a) | sum(a') | distinct(a) a' | +----------+-----------+-----------+---------+---------+----------------+ 那么 aggBufferCodeGens 会这样保存 ┌ │ └ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ * ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ' ─ ─ ) ─ ┬ │ ┴ ─ ─ ─ d ─ ─ i ─ ─ s ─ ─ t ─ ─ i ─ ─ n ─ ─ c ─ ─ t ─ ─ ( ─ ─ a ─ ─ ) ─ ┐ │ ┘ aggActionCodeGens 会这样保存 ┌ │ │ │ │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ c ─ ─ o ─ ─ u ─ ─ n ─ ─ t ─ ─ ( ─ ─ * ─ ─ ) ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┬ │ │ │ │ ┴ ─ ─ ─ s ─ ─ u ─ ─ m ─ ─ ( ─ ─ a ─ ─ ) ─ ┬ │ │ │ │ ┴ ─ ─ ─ ─ ─ d ─ ─ i ─ ─ s ├ ├ └ ─ ─ t ─ ─ ─ ─ ─ i c c s ─ ─ n o o u ─ ─ c u u m ─ ─ t n n ( ─ ─ ( t t a ─ ─ a ( ( ' ─ ─ ) a a ) ─ ─ ' ' ─ ─ a ) ) ─ ─ ' ─ ─ ( ─ ─ f ─ ─ i ─ ─ l ─ ─ t ─ ─ e ─ ─ r ─ ─ ─ ─ d ─ ─ ─ ─ \u003e ─ ─ ─ ─ 5 ─ ─ ) ─ ─ ─ ─ ─ ┐ │ │ │ │ ┘ ","date":"2022-09-26","objectID":"/flink-agg-handler-code-generator/:1:0","tags":["flink"],"title":"FlinkSQL - AggHandlerCodeGenerator","uri":"/flink-agg-handler-code-generator/"},{"categories":["Code"],"content":"CodeGeneratorContext package org.apache.flink.table.planner.codegen /** 生成代码的上下文，维护代码段的状态 */ class CodeGeneratorContext(val tableConfig: TableConfig) { // 保存用于传递生成类的对象列表 val references: mutable.ArrayBuffer[AnyRef] = new mutable.ArrayBuffer[AnyRef]() // 插入有序，只会被添加一次， 成员状态 private val reusableMemberStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 构造状态 private val reusableInitStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中open方法的状态 private val reusableOpenStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， RichFunction 中close方法的状态 private val reusableCloseStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // 插入有序，只会被添加一次， 清理 dataview 的状态 private val reusableCleanupStatements = mutable.LinkedHashSet[String]() // 单个记录的状态， 插入有序，因为本地变量需要被分割，所以本地变量无法访问，只能更新成员变量 private val reusablePerRecordStatements: mutable.LinkedHashSet[String] = mutable.LinkedHashSet[String]() // (inputTerm, index) -\u003e expr // 只会被添加一次， 初始化拆箱表达式Map val reusableInputUnboxingExprs: mutable.Map[(String, Int), GeneratedExpression] = mutable.Map[(String, Int), GeneratedExpression]() // 插入有序，只会被添加一次，构造函数的状态 private val reusableConstructorStatements: mutable.LinkedHashSet[(String, String)] = mutable.LinkedHashSet[(String, String)]() // 插入有序，只会被添加一次，类声明状态 private val reusableInnerClassDefinitionStatements: mutable.Map[String, String] = mutable.Map[String, String]() // string_constant -\u003e reused_term // 常量 private val reusableStringConstants: mutable.Map[String, String] = mutable.Map[String, String]() // LogicalType -\u003e reused_term // 类型序列化 private val reusableTypeSerializers: mutable.Map[LogicalType, String] = mutable.Map[LogicalType, String]() /** * Flag map that indicates whether the generated code for method is split into several methods. */ private val isCodeSplitMap = mutable.Map[String, Boolean]() // method_name -\u003e local_variable_statements private val reusableLocalVariableStatements = mutable.Map[String, mutable.LinkedHashSet[String]]( (currentMethodNameForLocalVariables, mutable.LinkedHashSet[String]())) ","date":"2022-09-26","objectID":"/flink-agg-handler-code-generator/:1:1","tags":["flink"],"title":"FlinkSQL - AggHandlerCodeGenerator","uri":"/flink-agg-handler-code-generator/"},{"categories":null,"content":"Toxi Alisa ","date":"2022-09-02","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]